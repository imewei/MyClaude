---
description: Sparse matrix operations and sparse Jacobian computation in JAX for memory-efficient scientific computing
category: jax-optimization
argument-hint: "[--operation=jacobian|solve|matmul] [--sparsity-pattern] [--memory-efficient] [--agents=auto|jax|scientific|ai|optimization|all] [--orchestrate] [--intelligent] [--breakthrough] [--optimize] [--sparse]"
allowed-tools: "*"
model: inherit
---

# JAX Sparse Ops

Sparse matrix operations and sparse Jacobian computation in JAX for memory-efficient scientific computing.

```bash
/jax-sparse-ops [--operation=jacobian|solve|matmul] [--sparsity-pattern] [--memory-efficient] [--agents=auto|jax|scientific|ai|optimization|all] [--orchestrate] [--intelligent] [--breakthrough] [--optimize] [--sparse]
```

## Options

- `--operation=<op>`: Type of sparse operation (jacobian, solve, matmul)
- `--sparsity-pattern`: Enable automatic sparsity pattern detection
- `--memory-efficient`: Use memory-optimized sparse algorithms
- `--agents=<agents>`: Agent selection (auto, jax, scientific, ai, optimization, all)
- `--orchestrate`: Enable advanced 23-agent orchestration with sparse intelligence
- `--intelligent`: Enable intelligent agent selection based on sparse analysis
- `--breakthrough`: Enable breakthrough sparse optimization and methodology
- `--optimize`: Apply performance optimization to sparse operations
- `--sparse`: Advanced sparse methodology with research-grade efficiency

## What it does

1. **Sparse Matrices**: Efficient sparse data structures and operations
2. **Sparse Jacobians**: Memory-efficient Jacobian computation with sparsity patterns
3. **Linear Algebra**: Sparse matrix multiplication and linear system solving
4. **Pattern Detection**: Automatic sparsity pattern identification and analysis
5. **Performance Optimization**: Memory and computational efficiency for large problems
6. **23-Agent Sparse Intelligence**: Multi-agent collaboration for optimal sparse computing strategies
7. **Advanced Optimization**: Agent-driven sparse algorithm optimization and selection
8. **Research-Grade Performance**: Agent-coordinated sparse methods for scientific computing

## 23-Agent Intelligent Sparse Operations System

### Intelligent Agent Selection (`--intelligent`)
**Auto-Selection Algorithm**: Analyzes sparse operation requirements, problem structure, and performance constraints to automatically choose optimal agent combinations from the 23-agent library.

```bash
# Sparse Problem Type Detection → Agent Selection
- Large-Scale Linear Systems → scientific-computing-master + systems-architect + jax-pro
- Finite Element Methods → scientific-computing-master + research-intelligence-master + correlation-function-expert
- Optimization Jacobians → ai-systems-architect + scientific-computing-master + jax-pro
- High-Performance Computing → systems-architect + jax-pro + multi-agent-orchestrator
- Research Applications → research-intelligence-master + scientific-computing-master + domain experts
```

### Core Sparse Computing Agents

#### **`scientific-computing-master`** - Sparse Scientific Computing Expert
- **Numerical Linear Algebra**: Advanced sparse linear algebra and numerical methods
- **Scientific Applications**: Sparse methods for computational science and engineering
- **Algorithm Development**: Cutting-edge sparse algorithms for scientific problems
- **Performance Engineering**: High-performance sparse computing optimization
- **Domain Integration**: Sparse methods for specific scientific disciplines

#### **`jax-pro`** - JAX Sparse Optimization Specialist
- **JAX Sparse Methods**: Deep expertise in JAX sparse operations and transformations
- **Memory Optimization**: JAX-specific sparse memory management and efficiency
- **GPU Acceleration**: Hardware-accelerated sparse computation with JAX
- **Performance Tuning**: JAX sparse operation performance optimization
- **Advanced Transformations**: JIT, vmap, pmap optimization for sparse operations

#### **`systems-architect`** - Sparse System Performance & Infrastructure
- **System Optimization**: System-level sparse computation optimization
- **Memory Architecture**: Large-scale sparse matrix memory management
- **Computational Efficiency**: Resource allocation for sparse operations
- **Scalability Engineering**: Sparse system design for large-scale problems
- **Performance Monitoring**: Real-time sparse computation performance tracking

#### **`ai-systems-architect`** - AI Sparse Systems & Optimization
- **ML Sparse Methods**: Sparse methods for machine learning and optimization
- **Distributed Sparse Computing**: Multi-device sparse computation architecture
- **Production Optimization**: Sparse systems for production AI applications
- **Resource Management**: Computational resource optimization for sparse ML
- **System Integration**: Sparse computation integration with AI infrastructure

### Specialized Sparse Agents

#### **`research-intelligence-master`** - Sparse Research & Innovation
- **Sparse Research**: Cutting-edge sparse methods research and development
- **Algorithm Innovation**: Novel sparse algorithms and breakthrough techniques
- **Academic Standards**: Research-grade sparse methods for publication
- **Methodology Development**: Advanced sparse methodologies and frameworks
- **Cross-Domain Innovation**: Sparse method innovation across multiple domains

#### **`correlation-function-expert`** - Statistical Sparse Methods
- **Sparse Statistics**: Statistical methods for sparse data and matrices
- **Pattern Analysis**: Statistical analysis of sparsity patterns and structures
- **Experimental Design**: Statistical design for sparse computation experiments
- **Data Analysis**: Statistical analysis of sparse computational results
- **Model Validation**: Statistical validation of sparse methods and algorithms

#### **`neural-networks-master`** - Sparse Deep Learning
- **Sparse Neural Networks**: Sparse architectures and pruning techniques
- **Optimization**: Sparse optimization methods for deep learning
- **Model Compression**: Neural network sparsification and compression
- **Training Efficiency**: Sparse training methods and acceleration
- **Inference Optimization**: Sparse inference and deployment optimization

### Advanced Agent Selection Strategies

#### **`auto`** - Intelligent Agent Selection for Sparse Operations
Automatically analyzes sparse requirements and selects optimal agent combinations:
- **Problem Analysis**: Detects sparsity patterns, problem size, computational requirements
- **Performance Assessment**: Evaluates computational constraints and optimization opportunities
- **Agent Matching**: Maps sparse challenges to relevant agent expertise
- **Method Optimization**: Balances comprehensive sparse methods with computational efficiency

#### **`jax`** - JAX-Specialized Sparse Team
- `jax-pro` (JAX ecosystem lead)
- `scientific-computing-master` (numerical methods)
- `systems-architect` (system optimization)
- `ai-systems-architect` (ML integration)

#### **`scientific`** - Scientific Computing Sparse Team
- `scientific-computing-master` (lead)
- `research-intelligence-master` (research methodology)
- `jax-pro` (JAX implementation)
- `correlation-function-expert` (statistical methods)
- Domain-specific experts based on scientific application

#### **`ai`** - AI/ML Sparse Operations Team
- `ai-systems-architect` (lead)
- `neural-networks-master` (sparse ML)
- `scientific-computing-master` (numerical methods)
- `jax-pro` (JAX optimization)
- `systems-architect` (infrastructure)

#### **`optimization`** - Sparse Optimization Team
- `scientific-computing-master` (lead)
- `ai-systems-architect` (optimization systems)
- `systems-architect` (performance)
- `jax-pro` (JAX optimization)
- `research-intelligence-master` (advanced methods)

#### **`all`** - Complete 23-Agent Sparse Ecosystem
Activates all relevant agents with intelligent orchestration for breakthrough sparse computing.

### 23-Agent Sparse Operations Orchestration (`--orchestrate`)

#### **Multi-Agent Sparse Pipeline**
1. **Sparsity Analysis Phase**: Multiple agents analyze sparsity patterns simultaneously
2. **Algorithm Selection**: Collaborative selection of optimal sparse methods
3. **Performance Optimization**: Agent-coordinated optimization of sparse operations
4. **Implementation Coordination**: Multi-agent sparse algorithm implementation
5. **Validation Framework**: Comprehensive sparse method validation and testing

#### **Breakthrough Sparse Discovery (`--breakthrough`)**
- **Cross-Domain Innovation**: Sparse techniques from multiple computational domains
- **Emergent Methods**: Novel sparse algorithms through agent collaboration
- **Research-Grade Performance**: Academic and industry-leading sparse standards
- **Adaptive Algorithms**: Dynamic sparse method selection based on problem characteristics

### Advanced 23-Agent Sparse Examples

```bash
# Intelligent auto-selection for sparse operations
/jax-sparse-ops --agents=auto --intelligent --operation=jacobian --optimize

# Scientific computing sparse methods with specialized agents
/jax-sparse-ops --agents=scientific --breakthrough --orchestrate --sparse

# AI/ML sparse optimization with performance focus
/jax-sparse-ops --agents=ai --operation=solve --optimize --breakthrough

# Research-grade sparse development
/jax-sparse-ops --agents=all --breakthrough --orchestrate --sparse

# JAX-specialized sparse optimization
/jax-sparse-ops --agents=jax --operation=matmul --memory-efficient --optimize

# Complete 23-agent sparse ecosystem
/jax-sparse-ops --agents=all --orchestrate --breakthrough --intelligent

# Finite element sparse systems
/jax-sparse-ops finite_element.py --agents=scientific --intelligent --sparse

# Large-scale optimization sparse Jacobians
/jax-sparse-ops optimization.py --agents=optimization --breakthrough --orchestrate

# Sparse neural network optimization
/jax-sparse-ops sparse_ml.py --agents=ai --optimize --intelligent

# High-performance sparse linear algebra
/jax-sparse-ops hpc_sparse.py --agents=all --breakthrough --orchestrate

# Research sparse methods development
/jax-sparse-ops research_sparse.py --agents=all --intelligent --breakthrough

# Production sparse systems
/jax-sparse-ops production_sparse.py --agents=ai --optimize --sparse
```

### Intelligent Agent Selection Examples

```bash
# Sparse Problem Type Detection → Intelligent Agent Selection

# Large finite element system
/jax-sparse-ops fem_system.py --agents=auto --intelligent
# → Selects: scientific-computing-master + systems-architect + correlation-function-expert

# Sparse optimization Jacobians
/jax-sparse-ops optimization_jacobian.py --agents=auto --intelligent
# → Selects: ai-systems-architect + scientific-computing-master + jax-pro

# High-performance sparse linear algebra
/jax-sparse-ops hpc_linalg.py --agents=auto --intelligent
# → Selects: systems-architect + jax-pro + scientific-computing-master

# Sparse neural network research
/jax-sparse-ops sparse_nn_research.py --agents=auto --intelligent
# → Selects: neural-networks-master + research-intelligence-master + ai-systems-architect

# Complex multi-physics sparse system
/jax-sparse-ops multiphysics.py --agents=auto --intelligent
# → Selects: scientific-computing-master + multi-agent-orchestrator + research-intelligence-master

# Comprehensive sparse computing platform
/jax-sparse-ops sparse_platform.py --agents=all --orchestrate --breakthrough
# → Activates: All 23 agents with intelligent coordination for maximum sparse computing effectiveness
```

## Example output

```python
import jax
import jax.numpy as jnp
import jax.random as random
from jax import jit, jacfwd, jacrev, vmap
import jax.scipy.sparse as jsp_sparse
import jax.scipy.sparse.linalg as jsp_linalg
import numpy as np
import scipy.sparse as sp_sparse
from typing import Callable, Tuple, Dict, Any, Optional, List, NamedTuple
import functools
import time

# Initialize PRNG key
key = random.PRNGKey(0)

# ============================================================================
# 1. SPARSE MATRIX DATA STRUCTURES
# ============================================================================

class SparseMatrix(NamedTuple):
    """Simple sparse matrix representation"""
    data: jnp.ndarray      # Non-zero values
    indices: jnp.ndarray   # Column indices
    indptr: jnp.ndarray    # Row pointers (CSR format)
    shape: Tuple[int, int] # Matrix dimensions

def create_sparse_matrix(dense_matrix, threshold=1e-12):
    """Convert dense matrix to sparse CSR format"""

    # Convert to scipy sparse for convenient conversion
    scipy_sparse = sp_sparse.csr_matrix(np.array(dense_matrix))
    scipy_sparse.eliminate_zeros()

    # Extract sparse components
    data = jnp.array(scipy_sparse.data)
    indices = jnp.array(scipy_sparse.indices)
    indptr = jnp.array(scipy_sparse.indptr)
    shape = scipy_sparse.shape

    return SparseMatrix(data=data, indices=indices, indptr=indptr, shape=shape)

def sparse_to_dense(sparse_matrix):
    """Convert sparse matrix back to dense format"""

    dense = jnp.zeros(sparse_matrix.shape)

    for i in range(sparse_matrix.shape[0]):
        start = sparse_matrix.indptr[i]
        end = sparse_matrix.indptr[i + 1]

        for j in range(start, end):
            col = sparse_matrix.indices[j]
            val = sparse_matrix.data[j]
            dense = dense.at[i, col].set(val)

    return dense

def analyze_sparsity(matrix):
    """Analyze sparsity pattern of a matrix"""

    if isinstance(matrix, SparseMatrix):
        total_elements = matrix.shape[0] * matrix.shape[1]
        nonzero_elements = len(matrix.data)
    else:
        total_elements = matrix.size
        nonzero_elements = jnp.count_nonzero(matrix)

    sparsity_ratio = 1.0 - (nonzero_elements / total_elements)

    return {
        'total_elements': total_elements,
        'nonzero_elements': int(nonzero_elements),
        'sparsity_ratio': float(sparsity_ratio),
        'density': float(1.0 - sparsity_ratio)
    }

# ============================================================================
# 2. SPARSE MATRIX OPERATIONS
# ============================================================================

@jit
def sparse_matvec(sparse_matrix, vector):
    """Sparse matrix-vector multiplication"""

    result = jnp.zeros(sparse_matrix.shape[0])

    for i in range(sparse_matrix.shape[0]):
        start = sparse_matrix.indptr[i]
        end = sparse_matrix.indptr[i + 1]

        row_sum = 0.0
        for j in range(start, end):
            col = sparse_matrix.indices[j]
            val = sparse_matrix.data[j]
            row_sum += val * vector[col]

        result = result.at[i].set(row_sum)

    return result

@jit
def sparse_matmul(sparse_a, sparse_b):
    """Sparse matrix-matrix multiplication (simplified implementation)"""

    # This is a simplified version - in practice, use optimized libraries
    dense_a = sparse_to_dense(sparse_a)
    dense_b = sparse_to_dense(sparse_b)
    dense_result = jnp.dot(dense_a, dense_b)

    return create_sparse_matrix(dense_result)

def sparse_linear_solve(sparse_matrix, rhs, method='cg'):
    """Solve sparse linear system Ax = b"""

    # Convert to dense for solving (in practice, use specialized sparse solvers)
    dense_matrix = sparse_to_dense(sparse_matrix)

    if method == 'direct':
        # Direct solve using LU decomposition
        solution = jnp.linalg.solve(dense_matrix, rhs)
    elif method == 'cg':
        # Conjugate gradient method
        solution = conjugate_gradient(sparse_matrix, rhs)
    else:
        raise ValueError(f"Unknown method: {method}")

    return solution

@jit
def conjugate_gradient(sparse_matrix, b, x0=None, max_iter=1000, tol=1e-6):
    """Conjugate gradient solver for sparse systems"""

    n = sparse_matrix.shape[0]

    if x0 is None:
        x = jnp.zeros(n)
    else:
        x = x0

    r = b - sparse_matvec(sparse_matrix, x)
    p = r
    rsold = jnp.dot(r, r)

    for i in range(max_iter):
        Ap = sparse_matvec(sparse_matrix, p)
        alpha = rsold / jnp.dot(p, Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        rsnew = jnp.dot(r, r)

        if jnp.sqrt(rsnew) < tol:
            break

        beta = rsnew / rsold
        p = r + beta * p
        rsold = rsnew

    return x

# ============================================================================
# 3. SPARSE JACOBIAN COMPUTATION
# ============================================================================

def detect_jacobian_sparsity(func, x_sample, perturbation=1e-8):
    """Detect sparsity pattern of Jacobian matrix"""

    # Compute full Jacobian for sparsity pattern
    jacobian_full = jacfwd(func)(x_sample)

    # Identify non-zero entries
    sparsity_pattern = jnp.abs(jacobian_full) > perturbation

    # Analyze sparsity
    sparsity_info = analyze_sparsity(jacobian_full)

    return sparsity_pattern, sparsity_info, jacobian_full

def compute_sparse_jacobian(func, x, sparsity_pattern=None, method='forward'):
    """Compute Jacobian exploiting sparsity pattern"""

    if sparsity_pattern is None:
        # Compute full Jacobian
        if method == 'forward':
            jacobian = jacfwd(func)(x)
        elif method == 'reverse':
            jacobian = jacrev(func)(x)
        else:
            raise ValueError(f"Unknown method: {method}")
    else:
        # Use sparsity pattern to optimize computation
        jacobian = compute_structured_jacobian(func, x, sparsity_pattern, method)

    return jacobian

def compute_structured_jacobian(func, x, sparsity_pattern, method='forward'):
    """Compute Jacobian using sparsity structure"""

    n_vars = len(x)
    n_funcs = sparsity_pattern.shape[0]

    # Group columns that can be computed together (graph coloring approximation)
    column_groups = simple_column_coloring(sparsity_pattern)

    jacobian = jnp.zeros((n_funcs, n_vars))

    for group in column_groups:
        # Compute multiple columns simultaneously
        directions = jnp.zeros((n_vars, len(group)))
        for i, col in enumerate(group):
            directions = directions.at[col, i].set(1.0)

        if method == 'forward':
            # Forward mode AD
            jvp_func = lambda x_val: jax.jvp(func, (x_val,), (directions,))[1]
            group_jacobian = jvp_func(x)
        else:
            # Reverse mode AD
            vjp_func = jax.vjp(func, x)[1]
            group_jacobian = jnp.stack([vjp_func(directions[:, i])[0] for i in range(len(group))]).T

        # Place results in correct columns
        for i, col in enumerate(group):
            jacobian = jacobian.at[:, col].set(group_jacobian[:, i])

    return jacobian

def simple_column_coloring(sparsity_pattern):
    """Simple greedy column coloring for Jacobian computation"""

    n_cols = sparsity_pattern.shape[1]
    colors = jnp.full(n_cols, -1)
    column_groups = []

    for col in range(n_cols):
        # Find columns that don't conflict with this one
        if colors[col] == -1:  # Uncolored
            group = [col]
            colors = colors.at[col].set(len(column_groups))

            for other_col in range(col + 1, n_cols):
                if colors[other_col] == -1:  # Uncolored
                    # Check if columns conflict (share a nonzero row)
                    conflict = jnp.any(sparsity_pattern[:, col] & sparsity_pattern[:, other_col])
                    if not conflict:
                        group.append(other_col)
                        colors = colors.at[other_col].set(len(column_groups))

            column_groups.append(group)

    return column_groups

# ============================================================================
# 4. MEMORY-EFFICIENT OPERATIONS
# ============================================================================

def memory_efficient_jacobian_vector_product(func, x, v):
    """Compute Jacobian-vector product without storing full Jacobian"""

    # Forward mode AD for Jv
    _, jvp_result = jax.jvp(func, (x,), (v,))
    return jvp_result

def memory_efficient_vector_jacobian_product(func, x, v):
    """Compute vector-Jacobian product without storing full Jacobian"""

    # Reverse mode AD for vJ
    _, vjp_func = jax.vjp(func, x)
    vjp_result = vjp_func(v)[0]
    return vjp_result

def chunked_jacobian_computation(func, x, chunk_size=100):
    """Compute Jacobian in chunks to manage memory"""

    n_vars = len(x)
    n_chunks = (n_vars + chunk_size - 1) // chunk_size

    # Get output dimension
    y_sample = func(x)
    n_outputs = len(y_sample) if y_sample.ndim == 1 else y_sample.shape[0]

    # Initialize Jacobian
    jacobian = jnp.zeros((n_outputs, n_vars))

    for chunk_idx in range(n_chunks):
        start_col = chunk_idx * chunk_size
        end_col = min((chunk_idx + 1) * chunk_size, n_vars)

        # Create directions for this chunk
        chunk_directions = jnp.zeros((n_vars, end_col - start_col))
        for i, col in enumerate(range(start_col, end_col)):
            chunk_directions = chunk_directions.at[col, i].set(1.0)

        # Compute JVP for this chunk
        _, jvp_result = jax.jvp(func, (x,), (chunk_directions,))

        # Store results
        jacobian = jacobian.at[:, start_col:end_col].set(jvp_result.reshape(n_outputs, -1))

    return jacobian

# ============================================================================
# 5. PERFORMANCE BENCHMARKING
# ============================================================================

def benchmark_sparse_vs_dense(matrix_size=1000, sparsity_ratio=0.9, n_runs=10):
    """Benchmark sparse vs dense operations"""

    # Generate test matrix
    key_matrix = random.PRNGKey(42)
    dense_matrix = random.normal(key_matrix, (matrix_size, matrix_size))

    # Make it sparse
    mask = random.uniform(key_matrix, dense_matrix.shape) > sparsity_ratio
    sparse_dense = jnp.where(mask, dense_matrix, 0.0)
    sparse_matrix = create_sparse_matrix(sparse_dense)

    # Generate test vector
    key_vector = random.PRNGKey(123)
    test_vector = random.normal(key_vector, (matrix_size,))

    print(f"=== Sparse vs Dense Benchmark ===")
    print(f"Matrix size: {matrix_size}x{matrix_size}")
    print(f"Sparsity ratio: {sparsity_ratio:.1%}")
    print(f"Non-zero elements: {sparse_matrix.data.shape[0]:,}")

    # Benchmark dense matvec
    dense_times = []
    for _ in range(n_runs):
        start_time = time.time()
        dense_result = jnp.dot(sparse_dense, test_vector)
        dense_result.block_until_ready()
        dense_times.append(time.time() - start_time)

    # Benchmark sparse matvec
    sparse_times = []
    for _ in range(n_runs):
        start_time = time.time()
        sparse_result = sparse_matvec(sparse_matrix, test_vector)
        sparse_result.block_until_ready()
        sparse_times.append(time.time() - start_time)

    # Results
    avg_dense_time = np.mean(dense_times)
    avg_sparse_time = np.mean(sparse_times)
    speedup = avg_dense_time / avg_sparse_time

    print(f"Dense matvec:  {avg_dense_time:.4f}s ± {np.std(dense_times):.4f}s")
    print(f"Sparse matvec: {avg_sparse_time:.4f}s ± {np.std(sparse_times):.4f}s")
    print(f"Speedup: {speedup:.2f}x")

    # Memory usage estimation
    dense_memory = matrix_size * matrix_size * 8  # 8 bytes per float64
    sparse_memory = len(sparse_matrix.data) * 8 + len(sparse_matrix.indices) * 4 + len(sparse_matrix.indptr) * 4
    memory_savings = (dense_memory - sparse_memory) / dense_memory

    print(f"Dense memory:  {dense_memory / 1024**2:.1f} MB")
    print(f"Sparse memory: {sparse_memory / 1024**2:.1f} MB")
    print(f"Memory savings: {memory_savings:.1%}")

    return {
        'dense_time': avg_dense_time,
        'sparse_time': avg_sparse_time,
        'speedup': speedup,
        'memory_savings': memory_savings
    }

def benchmark_jacobian_computation(func, x, sparsity_ratio=0.5, n_runs=5):
    """Benchmark different Jacobian computation methods"""

    print(f"=== Jacobian Computation Benchmark ===")
    print(f"Function input dimension: {len(x)}")

    # Detect sparsity pattern
    sparsity_pattern, sparsity_info, full_jacobian = detect_jacobian_sparsity(func, x)

    print(f"Jacobian shape: {full_jacobian.shape}")
    print(f"Sparsity ratio: {sparsity_info['sparsity_ratio']:.1%}")

    methods = {
        'Forward AD (full)': lambda: jacfwd(func)(x),
        'Reverse AD (full)': lambda: jacrev(func)(x),
        'Structured Forward': lambda: compute_structured_jacobian(func, x, sparsity_pattern, 'forward'),
        'Chunked computation': lambda: chunked_jacobian_computation(func, x, chunk_size=50)
    }

    results = {}

    for method_name, method_func in methods.items():
        times = []

        # Warm up
        _ = method_func()

        # Benchmark
        for _ in range(n_runs):
            start_time = time.time()
            result = method_func()
            if hasattr(result, 'block_until_ready'):
                result.block_until_ready()
            times.append(time.time() - start_time)

        avg_time = np.mean(times)
        std_time = np.std(times)

        results[method_name] = {'time': avg_time, 'std': std_time}
        print(f"{method_name}: {avg_time:.4f}s ± {std_time:.4f}s")

    return results

# ============================================================================
# 6. REAL-WORLD APPLICATIONS
# ============================================================================

def sparse_finite_element_example():
    """Example of sparse matrices in finite element methods"""

    # Simple 1D finite element stiffness matrix
    def create_1d_stiffness_matrix(n_elements, element_length=1.0):
        n_nodes = n_elements + 1
        K = jnp.zeros((n_nodes, n_nodes))

        # Element stiffness
        k_elem = 1.0 / element_length

        for e in range(n_elements):
            # Local to global mapping
            i, j = e, e + 1

            # Add element contribution
            K = K.at[i, i].add(k_elem)
            K = K.at[i, j].add(-k_elem)
            K = K.at[j, i].add(-k_elem)
            K = K.at[j, j].add(k_elem)

        return K

    # Create stiffness matrix
    n_elements = 100
    K_dense = create_1d_stiffness_matrix(n_elements)
    K_sparse = create_sparse_matrix(K_dense)

    # Analyze sparsity
    sparsity_info = analyze_sparsity(K_sparse)

    print("=== Finite Element Example ===")
    print(f"Elements: {n_elements}")
    print(f"Nodes: {n_elements + 1}")
    print(f"Matrix size: {K_dense.shape}")
    print(f"Sparsity ratio: {sparsity_info['sparsity_ratio']:.1%}")

    # Solve system
    f = jnp.ones(n_elements + 1)  # Unit load

    # Apply boundary conditions (fix first node)
    K_bc = K_dense[1:, 1:]
    f_bc = f[1:]

    # Solve
    u_bc = jnp.linalg.solve(K_bc, f_bc)
    u = jnp.concatenate([jnp.array([0.0]), u_bc])  # Add boundary condition

    print(f"Maximum displacement: {jnp.max(u):.6f}")

    return K_sparse, u

def sparse_optimization_example():
    """Example of sparse Jacobians in optimization"""

    def rosenbrock_system(x):
        """Modified Rosenbrock function system"""
        n = len(x)
        f = jnp.zeros(n)

        # First equation
        f = f.at[0].set(10 * (x[1] - x[0]**2))

        # Middle equations
        for i in range(1, n-1):
            f = f.at[i].set(10 * (x[i+1] - x[i]**2) + (1 - x[i]))

        # Last equation
        f = f.at[n-1].set(1 - x[n-1])

        return f

    # Test problem
    n_vars = 50
    x0 = jnp.ones(n_vars) * 0.5

    # Analyze Jacobian sparsity
    sparsity_pattern, sparsity_info, jacobian = detect_jacobian_sparsity(rosenbrock_system, x0)

    print("=== Optimization Example ===")
    print(f"Problem size: {n_vars} variables")
    print(f"Jacobian sparsity: {sparsity_info['sparsity_ratio']:.1%}")

    # Compare Jacobian computation methods
    benchmark_results = benchmark_jacobian_computation(rosenbrock_system, x0)

    return sparsity_pattern, jacobian

# ============================================================================
# 7. COMPREHENSIVE EXAMPLES
# ============================================================================

def run_sparse_operations_examples():
    """Run comprehensive sparse operations examples"""

    print("=== JAX Sparse Operations Examples ===")

    # Example 1: Basic sparse matrix operations
    print("\n1. Basic Sparse Matrix Operations:")

    # Create test matrix
    key, subkey = random.split(key)
    dense_test = random.normal(subkey, (100, 100))
    mask = random.uniform(subkey, dense_test.shape) > 0.8
    sparse_test_dense = jnp.where(mask, dense_test, 0.0)
    sparse_test = create_sparse_matrix(sparse_test_dense)

    sparsity_analysis = analyze_sparsity(sparse_test)
    print(f"  Matrix size: {sparse_test.shape}")
    print(f"  Sparsity: {sparsity_analysis['sparsity_ratio']:.1%}")
    print(f"  Non-zeros: {sparsity_analysis['nonzero_elements']:,}")

    # Test operations
    test_vector = random.normal(key, (100,))
    dense_result = jnp.dot(sparse_test_dense, test_vector)
    sparse_result = sparse_matvec(sparse_test, test_vector)

    error = jnp.max(jnp.abs(dense_result - sparse_result))
    print(f"  Matvec error: {error:.2e}")

    # Example 2: Sparse Jacobian computation
    print("\n2. Sparse Jacobian Computation:")

    def test_function(x):
        # Sparse function (tridiagonal-like structure)
        n = len(x)
        y = jnp.zeros(n)

        y = y.at[0].set(2*x[0] - x[1])
        for i in range(1, n-1):
            y = y.at[i].set(-x[i-1] + 2*x[i] - x[i+1])
        y = y.at[n-1].set(-x[n-2] + 2*x[n-1])

        return y

    x_test = jnp.ones(20) * 0.5

    # Analyze sparsity
    pattern, info, jac = detect_jacobian_sparsity(test_function, x_test)
    print(f"  Jacobian shape: {jac.shape}")
    print(f"  Sparsity: {info['sparsity_ratio']:.1%}")

    # Compare computation methods
    full_jac = jacfwd(test_function)(x_test)
    structured_jac = compute_structured_jacobian(test_function, x_test, pattern)

    jac_error = jnp.max(jnp.abs(full_jac - structured_jac))
    print(f"  Structured Jacobian error: {jac_error:.2e}")

    # Example 3: Performance comparison
    print("\n3. Performance Comparison:")
    benchmark_results = benchmark_sparse_vs_dense(matrix_size=500, sparsity_ratio=0.95)

    # Example 4: Real-world application
    print("\n4. Finite Element Application:")
    K_sparse, displacement = sparse_finite_element_example()

    # Example 5: Optimization application
    print("\n5. Optimization Application:")
    opt_pattern, opt_jacobian = sparse_optimization_example()

# Run examples
run_sparse_operations_examples()
```

## Sparse Operations Best Practices

### When to Use Sparse Operations
- **High sparsity**: When >80% of matrix elements are zero
- **Large problems**: Memory becomes a limiting factor
- **Structured sparsity**: Regular patterns (tridiagonal, block sparse)
- **Iterative methods**: Conjugate gradient, GMRES solvers

### Sparsity Pattern Detection
- **Automatic detection**: Use finite differences or AD to find patterns
- **A priori knowledge**: Exploit problem structure when known
- **Graph coloring**: Optimize Jacobian computation grouping
- **Dynamic patterns**: Handle changing sparsity during computation

### Memory Optimization Strategies
- **Compressed storage**: CSR, CSC, COO formats
- **Block operations**: Process data in chunks
- **Streaming**: Avoid loading full matrices into memory
- **Pattern reuse**: Cache sparsity patterns across computations

### Performance Considerations
- **Method selection**: Forward vs reverse AD based on problem dimensions
- **Chunked computation**: Balance memory and computational efficiency
- **JIT compilation**: Optimize sparse operations with JAX transformations
- **Vectorization**: Use JAX's vmap for batch sparse operations

## Common Sparse Matrix Formats

### Coordinate (COO) Format
- **Best for**: Matrix construction and format conversion
- **Storage**: (row, col, data) triplets
- **Operations**: Addition, format conversion
- **Memory**: 3 arrays of nnz elements

### Compressed Sparse Row (CSR)
- **Best for**: Matrix-vector products, arithmetic operations
- **Storage**: (data, indices, indptr) arrays
- **Operations**: Efficient row access, matvec
- **Memory**: Optimal for most operations

### Compressed Sparse Column (CSC)
- **Best for**: Column operations, transpose operations
- **Storage**: (data, indices, indptr) arrays transposed
- **Operations**: Efficient column access
- **Memory**: Same as CSR but column-oriented

## Sparse Linear Algebra Applications

### Finite Element Methods
- **Stiffness matrices**: Highly sparse due to local element connectivity
- **Assembly process**: Natural sparse structure from element contributions
- **Boundary conditions**: Modify sparse systems efficiently
- **Solvers**: Iterative methods for large sparse systems

### Optimization Problems
- **Jacobian sparsity**: Many variables affect few constraints
- **Hessian approximation**: Quasi-Newton methods with sparse updates
- **Network optimization**: Graph-based sparse structures
- **Machine learning**: Sparse regularization (L1, elastic net)

### Scientific Computing
- **PDEs**: Discretization leads to sparse matrices
- **Graph algorithms**: Adjacency matrices are naturally sparse
- **Signal processing**: Sparse transforms and compressed sensing
- **Chemical kinetics**: Reaction networks with sparse Jacobians

## Agent-Enhanced Sparse Operations Integration Patterns

### Complete Sparse Computing Workflow
```bash
# Intelligent sparse analysis and optimization pipeline
/jax-sparse-ops --agents=auto --intelligent --operation=jacobian --optimize
/jax-performance --agents=jax --technique=memory --optimization
/jax-nlsq --agents=scientific --sparse --optimization
```

### Scientific Computing Sparse Pipeline
```bash
# High-performance scientific sparse computing
/jax-sparse-ops --agents=scientific --breakthrough --orchestrate
/jax-essentials --agents=scientific --operation=grad --sparse
/run-all-tests --agents=scientific --performance --sparse
```

### Production Sparse System Infrastructure
```bash
# Large-scale production sparse systems
/jax-sparse-ops --agents=ai --optimize --breakthrough --sparse
/jax-performance --agents=ai --technique=memory --gpu-accel
/ci-setup --agents=ai --performance-monitoring --sparse
```

## Related Commands

**Prerequisites**: Commands to run before sparse operations
- `/jax-essentials --agents=auto` - Core JAX operations with sparse considerations
- `/jax-init --agents=scientific` - JAX project setup with sparse configuration

**Core Workflow**: Sparse development with agent intelligence
- `/jax-performance --agents=jax` - Sparse performance optimization
- `/jax-nlsq --agents=scientific` - Nonlinear least squares with sparse Jacobians
- `/jax-debug --agents=auto` - Debug sparse computation issues

**Advanced Integration**: Specialized sparse development
- `/jax-models --agents=scientific` - Sparse neural network architectures
- `/jax-training --agents=ai` - Sparse training optimization
- `/jax-numpyro-prob --agents=scientific` - Sparse Bayesian methods

**Quality Assurance**: Sparse validation and testing
- `/generate-tests --agents=auto --type=sparse` - Generate sparse operation tests
- `/run-all-tests --agents=scientific --performance` - Comprehensive sparse testing
- `/check-code-quality --agents=auto --sparse` - Sparse code quality optimization

**Research & Documentation**: Advanced sparse workflows
- `/update-docs --agents=research --type=sparse` - Research-grade sparse documentation
- `/reflection --agents=research --type=sparse` - Sparse methodology analysis
- `/multi-agent-optimize --agents=all --focus=sparse` - Comprehensive sparse optimization

ARGUMENTS: [--operation=jacobian|solve|matmul] [--sparsity-pattern] [--memory-efficient] [--agents=auto|jax|scientific|ai|optimization|all] [--orchestrate] [--intelligent] [--breakthrough] [--optimize] [--sparse]