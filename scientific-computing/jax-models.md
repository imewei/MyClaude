title: "JAX models"
---
description: Neural network model definition with Flax Linen and Equinox - comprehensive guide to JAX deep learning frameworks
subcategory: jax-ecosystem
complexity: intermediate
category: jax-ml
argument-hint: "[--framework=flax|equinox|both] [--architecture=mlp|cnn|transformer] [--functional] [--agents=auto|jax|scientific|ai|research|all] [--orchestrate] [--intelligent] [--breakthrough] [--optimize] [--distributed]"
allowed-tools: "*"
model: inherit
---

# JAX Models

Neural network model definition with Flax Linen and Equinox frameworks for JAX deep learning.

```bash
/jax-models [--framework=flax|equinox|both] [--architecture=mlp|cnn|transformer] [--functional] [--agents=auto|jax|scientific|ai|research|all] [--orchestrate] [--intelligent] [--breakthrough] [--optimize] [--distributed]
```

## Options

- `--framework=<framework>`: Deep learning framework (flax, equinox, both)
- `--architecture=<arch>`: Model architecture (mlp, cnn, transformer)
- `--functional`: Use functional programming style
- `--agents=<agents>`: Agent selection (auto, jax, scientific, ai, research, all)
- `--orchestrate`: Enable advanced 23-agent orchestration with model architecture intelligence
- `--intelligent`: Enable intelligent agent selection based on model analysis
- `--breakthrough`: Enable breakthrough model architecture discovery
- `--optimize`: Apply performance optimization to model definitions
- `--distributed`: Enable distributed model architecture across multiple agents

## What it does

1. **Flax Models**: Define models using Flax Linen with state management
2. **Equinox Models**: Define models using Equinox functional approach
3. **Common Architectures**: MLPs, CNNs, Transformers with best practices
4. **Parameter Management**: Initialize, serialize, and manage model parameters
5. **Training Integration**: Models ready for JAX training workflows
6. **23-Agent Architecture Intelligence**: Multi-agent collaboration for optimal model design
7. **Framework Selection**: Intelligent choice between Flax and Equinox based on requirements
8. **Performance Optimization**: Agent-driven model architecture optimization

## 23-Agent Intelligent Model Architecture System

### Intelligent Agent Selection (`--intelligent`)
**Auto-Selection Algorithm**: Analyzes model requirements, architecture complexity, and performance constraints to automatically choose optimal agent combinations from the 23-agent library.

```bash
# Model Type Detection → Agent Selection
- Deep Learning Research → neural-networks-master + ai-systems-architect + research-intelligence-master
- Scientific ML → jax-pro + scientific-computing-master + neural-networks-master
- Production Models → ai-systems-architect + jax-pro + systems-architect
- Experimental Architectures → research-intelligence-master + neural-networks-master + multi-agent-orchestrator
- Cross-Framework Development → jax-pro + neural-networks-master + fullstack-developer
```

### Core JAX Model Architecture Agents

#### **`jax-pro`** - JAX Ecosystem Optimization Expert
- **Framework Mastery**: Deep expertise in Flax Linen and Equinox architectures
- **JAX Transformations**: Optimal use of JIT, grad, vmap, pmap for model efficiency
- **Memory Optimization**: Model architecture for GPU/TPU memory efficiency
- **Performance Engineering**: XLA optimization and device placement strategies
- **Scientific ML Integration**: JAX ecosystem best practices for research and production

#### **`neural-networks-master`** - Deep Learning Architecture Specialist
- **Architecture Design**: Expert knowledge of modern neural network architectures
- **Model Optimization**: Architecture search and performance optimization strategies
- **Training Dynamics**: Model design for stable and efficient training
- **Research Integration**: Cutting-edge architecture patterns and innovations
- **Multi-Modal Models**: Architecture design for complex multi-modal learning

#### **`ai-systems-architect`** - AI System Design & Scalability
- **System Integration**: Model architecture within larger AI system design
- **Scalability Engineering**: Model design for distributed and production environments
- **Infrastructure Optimization**: Architecture choices for deployment and serving
- **Performance Engineering**: System-level optimization and resource management
- **MLOps Integration**: Model architecture for continuous deployment and monitoring

#### **`research-intelligence-master`** - Research & Innovation Expert
- **Cutting-Edge Research**: Latest developments in neural architecture research
- **Experimental Design**: Novel architecture patterns and research methodologies
- **Innovation Synthesis**: Cross-domain architecture innovation and breakthrough discovery
- **Academic Standards**: Research-grade model design and reproducibility
- **Publication Quality**: Architecture design for academic and research publication

### Specialized Model Architecture Agents

#### **`scientific-computing-master`** - Scientific ML Architectures
- **Physics-Informed Networks**: Architecture design for scientific computing applications
- **Numerical Optimization**: Model architectures for computational science
- **Multi-Scale Modeling**: Architecture patterns for multi-scale scientific problems
- **Domain Integration**: Model design for specific scientific domains
- **Computational Efficiency**: Scientific computing performance optimization

#### **`systems-architect`** - Production & Integration Architecture
- **Production Readiness**: Model architecture for production deployment
- **System Integration**: Architecture design for enterprise systems
- **Performance Monitoring**: Architecture patterns for system observability
- **Resource Management**: Efficient architecture for computational resources
- **Deployment Optimization**: Architecture choices for various deployment scenarios

#### **`data-professional`** - Data-Centric Architecture Design
- **Data Pipeline Integration**: Model architecture for data processing workflows
- **Feature Engineering**: Architecture design for feature processing and selection
- **Data Efficiency**: Model architectures for limited or streaming data
- **Multi-Source Integration**: Architecture for heterogeneous data sources
- **Data Quality Management**: Architecture patterns for robust data handling

### Advanced Agent Selection Strategies

#### **`auto`** - Intelligent Agent Selection for Model Architecture
Automatically analyzes model requirements and selects optimal agent combinations:
- **Architecture Analysis**: Detects model complexity, performance requirements, framework preferences
- **Requirements Matching**: Maps architectural needs to relevant agent expertise
- **Performance Optimization**: Balances comprehensive design with development efficiency
- **Framework Selection**: Intelligent choice between Flax and Equinox based on use case

#### **`jax`** - JAX-Specialized Model Architecture Team
- `jax-pro` (JAX ecosystem lead)
- `neural-networks-master` (architecture design)
- `ai-systems-architect` (system integration)
- `scientific-computing-master` (scientific applications)

#### **`scientific`** - Scientific Computing Model Architecture
- `scientific-computing-master` (lead)
- `jax-pro` (JAX implementation)
- `neural-networks-master` (architecture design)
- `research-intelligence-master` (research methodology)
- Domain-specific experts based on scientific application

#### **`ai`** - AI/ML Production Architecture Team
- `ai-systems-architect` (lead)
- `neural-networks-master` (architecture design)
- `jax-pro` (JAX implementation)
- `data-professional` (data integration)
- `systems-architect` (production readiness)

#### **`research`** - Research-Grade Architecture Development
- `research-intelligence-master` (lead)
- `neural-networks-master` (architecture innovation)
- `jax-pro` (JAX research tools)
- `scientific-computing-master` (computational research)
- `ai-systems-architect` (scalable research infrastructure)

#### **`all`** - Complete 23-Agent Architecture Ecosystem
Activates all relevant agents with intelligent orchestration for breakthrough model architecture discovery.

### 23-Agent Model Architecture Orchestration (`--orchestrate`)

#### **Multi-Agent Architecture Pipeline**
1. **Requirements Analysis Phase**: Multiple agents analyze model requirements simultaneously
2. **Framework Selection**: Intelligent choice between Flax and Equinox based on criteria
3. **Architecture Design**: Collaborative architecture development across agent expertise
4. **Performance Optimization**: Multi-agent optimization of model efficiency
5. **Integration Validation**: Cross-agent validation of model integration patterns

#### **Breakthrough Architecture Discovery (`--breakthrough`)**
- **Cross-Domain Innovation**: Architecture patterns from multiple domains and research areas
- **Emergent Design**: Novel architectures discovered through agent collaboration
- **Research-Grade Innovation**: Academic and industry-leading architecture standards
- **Multi-Framework Integration**: Optimal patterns combining Flax and Equinox strengths

### Advanced 23-Agent Model Examples

```bash
# Intelligent auto-selection for model architecture
/jax-models --agents=auto --intelligent --framework=both

# Scientific computing models with specialized agents
/jax-models --agents=scientific --architecture=transformer --optimize --orchestrate

# AI/ML production models with scalability focus
/jax-models --agents=ai --framework=flax --optimize --distributed

# Research-grade architecture development
/jax-models --agents=research --breakthrough --framework=both --orchestrate

# JAX-specialized model architecture
/jax-models --agents=jax --architecture=cnn --optimize --intelligent

# Complete 23-agent architecture ecosystem
/jax-models --agents=all --orchestrate --breakthrough --distributed

# Physics-informed neural networks
/jax-models physics_model.py --agents=scientific --intelligent --optimize

# Large-scale transformer architecture
/jax-models transformer_large.py --agents=ai --distributed --breakthrough

# Multi-modal research model
/jax-models multimodal.py --agents=research --orchestrate --intelligent

# Production-ready CNN architecture
/jax-models production_cnn.py --agents=ai --optimize --framework=flax

# Experimental architecture development
/jax-models experimental.py --agents=all --breakthrough --orchestrate

# Cross-framework architecture comparison
/jax-models comparison.py --agents=jax --framework=both --intelligent
```

### Intelligent Agent Selection Examples

```bash
# Model Type Detection → Intelligent Agent Selection

# Deep learning research project
/jax-models research_model.py --agents=auto --intelligent
# → Selects: research-intelligence-master + neural-networks-master + jax-pro

# Scientific ML application
/jax-models physics_simulation.py --agents=auto --intelligent
# → Selects: scientific-computing-master + jax-pro + neural-networks-master

# Production transformer model
/jax-models transformer_prod.py --agents=auto --intelligent
# → Selects: ai-systems-architect + neural-networks-master + jax-pro

# Computer vision CNN
/jax-models vision_model.py --agents=auto --intelligent
# → Selects: neural-networks-master + ai-systems-architect + jax-pro

# Multi-modal research architecture
/jax-models multimodal_research.py --agents=auto --intelligent
# → Selects: research-intelligence-master + neural-networks-master + multi-agent-orchestrator

# Cross-framework optimization study
/jax-models framework_study.py --agents=all --orchestrate --breakthrough
# → Activates: All 23 agents with intelligent coordination for comprehensive analysis
```

## Example output

```python
import jax
import jax.numpy as jnp
import jax.random as random
from jax import jit, grad, vmap
import flax.linen as nn
import equinox as eqx
from typing import Sequence, Optional, Callable

# Initialize PRNG key
key = random.PRNGKey(0)

# ============================================================================
# 1. FLAX LINEN MODELS
# ============================================================================

class FlaxMLP(nn.Module):
    """Multi-layer perceptron with Flax Linen"""
    hidden_dims: Sequence[int]
    output_dim: int
    activation: Callable = nn.relu
    use_bias: bool = True

    @nn.compact
    def __call__(self, x, training: bool = True):
        # Hidden layers
        for hidden_dim in self.hidden_dims:
            x = nn.Dense(hidden_dim, use_bias=self.use_bias)(x)
            x = self.activation(x)

            # Optional dropout during training
            if training:
                x = nn.Dropout(rate=0.1)(x, deterministic=not training)

        # Output layer
        x = nn.Dense(self.output_dim, use_bias=self.use_bias)(x)
        return x

class FlaxCNN(nn.Module):
    """Convolutional neural network with Flax"""
    num_classes: int

    @nn.compact
    def __call__(self, x, training: bool = True):
        # Convolutional layers
        x = nn.Conv(features=32, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))

        x = nn.Conv(features=64, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))

        # Flatten and fully connected
        x = x.reshape((x.shape[0], -1))  # Flatten
        x = nn.Dense(features=256)(x)
        x = nn.relu(x)

        if training:
            x = nn.Dropout(rate=0.5)(x, deterministic=not training)

        x = nn.Dense(features=self.num_classes)(x)
        return x

class FlaxTransformerBlock(nn.Module):
    """Transformer block with Flax"""
    embed_dim: int
    num_heads: int
    mlp_dim: int
    dropout_rate: float = 0.1

    @nn.compact
    def __call__(self, x, training: bool = True):
        # Multi-head self-attention
        attn_output = nn.MultiHeadDotProductAttention(
            num_heads=self.num_heads,
            dropout_rate=self.dropout_rate
        )(x, x, deterministic=not training)

        # Residual connection and layer norm
        x = nn.LayerNorm()(x + attn_output)

        # MLP block
        mlp_output = nn.Dense(self.mlp_dim)(x)
        mlp_output = nn.gelu(mlp_output)
        mlp_output = nn.Dropout(rate=self.dropout_rate)(mlp_output, deterministic=not training)
        mlp_output = nn.Dense(self.embed_dim)(mlp_output)

        # Residual connection and layer norm
        x = nn.LayerNorm()(x + mlp_output)

        return x

class FlaxTransformer(nn.Module):
    """Simple transformer model with Flax"""
    vocab_size: int
    embed_dim: int
    num_heads: int
    num_layers: int
    mlp_dim: int
    max_length: int
    num_classes: int

    @nn.compact
    def __call__(self, x, training: bool = True):
        # Token and position embeddings
        token_embed = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_dim)(x)
        pos_embed = nn.Embed(num_embeddings=self.max_length, features=self.embed_dim)(
            jnp.arange(x.shape[1])
        )
        x = token_embed + pos_embed

        # Transformer blocks
        for _ in range(self.num_layers):
            x = FlaxTransformerBlock(
                embed_dim=self.embed_dim,
                num_heads=self.num_heads,
                mlp_dim=self.mlp_dim
            )(x, training=training)

        # Global average pooling and classification
        x = jnp.mean(x, axis=1)  # Pool over sequence dimension
        x = nn.Dense(self.num_classes)(x)

        return x

# ============================================================================
# 2. EQUINOX MODELS
# ============================================================================

class EquinoxMLP(eqx.Module):
    """Multi-layer perceptron with Equinox"""
    layers: list
    activation: Callable

    def __init__(self, input_dim: int, hidden_dims: Sequence[int],
                 output_dim: int, activation: Callable = jax.nn.relu, key=None):
        if key is None:
            key = random.PRNGKey(0)

        self.activation = activation

        # Create layers
        dims = [input_dim] + list(hidden_dims) + [output_dim]
        keys = random.split(key, len(dims) - 1)

        self.layers = []
        for i in range(len(dims) - 1):
            layer = eqx.nn.Linear(dims[i], dims[i + 1], key=keys[i])
            self.layers.append(layer)

    def __call__(self, x):
        for i, layer in enumerate(self.layers[:-1]):
            x = layer(x)
            x = self.activation(x)

        # Final layer without activation
        x = self.layers[-1](x)
        return x

class EquinoxCNN(eqx.Module):
    """Convolutional neural network with Equinox"""
    conv1: eqx.nn.Conv2d
    conv2: eqx.nn.Conv2d
    linear1: eqx.nn.Linear
    linear2: eqx.nn.Linear

    def __init__(self, num_classes: int, key):
        keys = random.split(key, 4)

        self.conv1 = eqx.nn.Conv2d(1, 32, kernel_size=3, key=keys[0])
        self.conv2 = eqx.nn.Conv2d(32, 64, kernel_size=3, key=keys[1])

        # Note: These dimensions would need to be calculated based on input size
        self.linear1 = eqx.nn.Linear(64 * 5 * 5, 256, key=keys[2])  # Adjust as needed
        self.linear2 = eqx.nn.Linear(256, num_classes, key=keys[3])

    def __call__(self, x):
        # Convolutional layers
        x = jax.nn.relu(self.conv1(x))
        x = jax.nn.max_pool2d(x, window_shape=(2, 2), strides=(2, 2))

        x = jax.nn.relu(self.conv2(x))
        x = jax.nn.max_pool2d(x, window_shape=(2, 2), strides=(2, 2))

        # Flatten and fully connected
        x = x.reshape(x.shape[0], -1)
        x = jax.nn.relu(self.linear1(x))
        x = self.linear2(x)

        return x

class EquinoxTransformerBlock(eqx.Module):
    """Transformer block with Equinox"""
    attention: eqx.nn.MultiheadAttention
    norm1: eqx.nn.LayerNorm
    norm2: eqx.nn.LayerNorm
    mlp: EquinoxMLP

    def __init__(self, embed_dim: int, num_heads: int, mlp_dim: int, key):
        keys = random.split(key, 2)

        self.attention = eqx.nn.MultiheadAttention(
            num_heads=num_heads,
            query_size=embed_dim,
            key=keys[0]
        )

        self.norm1 = eqx.nn.LayerNorm(embed_dim)
        self.norm2 = eqx.nn.LayerNorm(embed_dim)

        self.mlp = EquinoxMLP(
            input_dim=embed_dim,
            hidden_dims=[mlp_dim],
            output_dim=embed_dim,
            key=keys[1]
        )

    def __call__(self, x):
        # Multi-head self-attention with residual connection
        attn_output = self.attention(x, x, x)
        x = self.norm1(x + attn_output)

        # MLP with residual connection
        mlp_output = self.mlp(x)
        x = self.norm2(x + mlp_output)

        return x

# ============================================================================
# 3. MODEL INITIALIZATION AND USAGE
# ============================================================================

def initialize_flax_model():
    """Initialize Flax models with proper parameter handling"""

    # Create model
    model = FlaxMLP(hidden_dims=[64, 32], output_dim=10)

    # Initialize parameters
    key = random.PRNGKey(0)
    dummy_input = jnp.ones((1, 20))  # Batch size 1, 20 features

    params = model.init(key, dummy_input, training=False)

    # Model application
    def apply_model(params, x, training=True):
        return model.apply(params, x, training=training)

    return model, params, apply_model

def initialize_equinox_model():
    """Initialize Equinox models"""

    key = random.PRNGKey(0)
    model = EquinoxMLP(
        input_dim=20,
        hidden_dims=[64, 32],
        output_dim=10,
        key=key
    )

    # Equinox models are callable directly
    def apply_model(model, x):
        return model(x)

    return model, apply_model

def compare_frameworks():
    """Compare Flax and Equinox frameworks"""

    print("=== Framework Comparison ===")

    # Sample data
    key = random.PRNGKey(42)
    x = random.normal(key, (5, 20))  # 5 samples, 20 features

    # Flax model
    flax_model, flax_params, flax_apply = initialize_flax_model()
    flax_output = flax_apply(flax_params, x, training=False)
    print(f"Flax output shape: {flax_output.shape}")

    # Equinox model
    eqx_model, eqx_apply = initialize_equinox_model()
    eqx_output = eqx_apply(eqx_model, x)
    print(f"Equinox output shape: {eqx_output.shape}")

# ============================================================================
# 4. ADVANCED MODEL PATTERNS
# ============================================================================

class ResidualBlock(nn.Module):
    """Residual block for deeper networks"""
    features: int

    @nn.compact
    def __call__(self, x, training: bool = True):
        residual = x

        x = nn.Dense(self.features)(x)
        x = nn.relu(x)
        x = nn.Dense(self.features)(x)

        # Skip connection
        if residual.shape[-1] != self.features:
            residual = nn.Dense(self.features)(residual)

        return nn.relu(x + residual)

class ResNet(nn.Module):
    """Simple ResNet-style model"""
    num_blocks: int
    features: int
    num_classes: int

    @nn.compact
    def __call__(self, x, training: bool = True):
        # Initial projection
        x = nn.Dense(self.features)(x)

        # Residual blocks
        for _ in range(self.num_blocks):
            x = ResidualBlock(self.features)(x, training=training)

        # Final classification
        x = nn.Dense(self.num_classes)(x)
        return x

# ============================================================================
# 5. MODEL UTILITIES
# ============================================================================

def count_parameters(params):
    """Count total number of parameters"""
    return sum(p.size for p in jax.tree_leaves(params))

def model_summary(model, input_shape, key):
    """Print model summary similar to Keras"""
    dummy_input = jnp.ones(input_shape)

    if hasattr(model, 'init'):  # Flax model
        params = model.init(key, dummy_input, training=False)
        param_count = count_parameters(params)

        print(f"Model: {model.__class__.__name__}")
        print(f"Input shape: {input_shape}")
        print(f"Total parameters: {param_count:,}")

        # Test forward pass
        output = model.apply(params, dummy_input, training=False)
        print(f"Output shape: {output.shape}")

    else:  # Equinox model
        param_count = count_parameters(model)

        print(f"Model: {model.__class__.__name__}")
        print(f"Input shape: {input_shape}")
        print(f"Total parameters: {param_count:,}")

        # Test forward pass
        output = model(dummy_input)
        print(f"Output shape: {output.shape}")

def save_model_params(params, filename):
    """Save model parameters to file"""
    import pickle
    with open(filename, 'wb') as f:
        pickle.dump(params, f)

def load_model_params(filename):
    """Load model parameters from file"""
    import pickle
    with open(filename, 'rb') as f:
        return pickle.load(f)

# ============================================================================
# 6. EXAMPLE USAGE
# ============================================================================

def run_model_examples():
    """Run various model examples"""

    print("=== JAX Model Examples ===")

    key = random.PRNGKey(0)

    # Example 1: Flax MLP
    print("\n1. Flax MLP Model:")
    flax_mlp = FlaxMLP(hidden_dims=[128, 64], output_dim=10)
    model_summary(flax_mlp, (1, 50), key)

    # Example 2: Equinox MLP
    print("\n2. Equinox MLP Model:")
    eqx_mlp = EquinoxMLP(input_dim=50, hidden_dims=[128, 64], output_dim=10, key=key)
    model_summary(eqx_mlp, (1, 50), key)

    # Example 3: Flax CNN
    print("\n3. Flax CNN Model:")
    flax_cnn = FlaxCNN(num_classes=10)
    model_summary(flax_cnn, (1, 28, 28, 1), key)  # MNIST-like

    # Example 4: ResNet
    print("\n4. ResNet Model:")
    resnet = ResNet(num_blocks=3, features=64, num_classes=10)
    model_summary(resnet, (1, 50), key)

    # Framework comparison
    print("\n5. Framework Comparison:")
    compare_frameworks()

# Run examples
run_model_examples()
```

## Framework Comparison

### Flax Linen
**Pros:**
- Explicit parameter management with init/apply pattern
- Mature ecosystem with many examples
- Clear separation of model definition and parameter handling
- Good integration with JAX transformations

**Cons:**
- More verbose parameter handling
- Requires understanding of init/apply pattern
- Can be complex for functional programming enthusiasts

**Best for:** Traditional neural network architectures, researchers familiar with PyTorch/TensorFlow patterns

### Equinox
**Pros:**
- Purely functional approach
- Models are just PyTrees (can be directly used with JAX transformations)
- More Pythonic and intuitive
- Excellent for functional programming

**Cons:**
- Smaller ecosystem compared to Flax
- Less documentation and examples
- Newer framework with evolving APIs

**Best for:** Functional programming enthusiasts, research requiring custom architectures

## Model Design Best Practices

### Architecture Design
- Start with simple architectures and add complexity gradually
- Use residual connections for deeper networks
- Apply layer normalization for training stability
- Consider dropout for regularization

### Parameter Initialization
- Use appropriate initialization schemes (Xavier, He, etc.)
- Initialize biases to zero or small values
- Use different seeds for different layers
- Consider pre-trained initialization when available

### Integration with Training
- Design models to work with your training loop
- Handle training vs. inference modes properly
- Consider gradient checkpointing for large models
- Ensure compatibility with your optimizer

## Agent-Enhanced Integration Patterns

### Model Architecture Development Workflow
```bash
# Intelligent architecture design and optimization
/jax-models --agents=auto --intelligent --framework=both --optimize
/jax-training --agents=auto --intelligent --optimizer=adam
/jax-performance --agents=jax --technique=caching --gpu-accel
```

### Scientific Computing Model Pipeline
```bash
# Physics-informed neural network development
/jax-models --agents=scientific --breakthrough --orchestrate
/jax-essentials --agents=scientific --operation=grad --higher-order
/jax-training --agents=scientific --optimizer=adam --epochs=1000
```

### Research Architecture Innovation
```bash
# Novel architecture research and development
/jax-models --agents=research --breakthrough --orchestrate --distributed
/jax-debug --agents=research --check-tracers --print-values
/run-all-tests --agents=research --scientific --reproducible
```

## Related Commands

**Prerequisites**: Commands to run before model architecture development
- `/jax-essentials --agents=auto` - Core JAX operations with agent optimization
- `/jax-init --agents=auto` - JAX project setup with intelligent agent selection

**Core Workflow**: Model development with agent intelligence
- `/jax-training --agents=auto` - Complete training workflows with agent optimization
- `/jax-performance --agents=jax` - Model performance optimization with specialized agents
- `/jax-debug --agents=auto` - Debug model compilation with intelligent assistance

**Advanced Integration**: Specialized model development
- `/jax-numpyro-prob --agents=scientific` - Probabilistic models with scientific agents
- `/jax-sparse-ops --agents=scientific` - Sparse architectures with optimization agents
- `/jax-orbax-checkpoint --agents=ai` - Model checkpointing with production agents

**Quality Assurance**: Validation and optimization
- `/generate-tests --agents=auto --type=jax` - Generate model tests with agent intelligence
- `/run-all-tests --agents=ai --scientific` - Comprehensive testing with specialized agents
- `/optimize --agents=jax --language=jax` - Performance optimization with JAX agents

**Research & Documentation**: Advanced workflows
- `/update-docs --agents=research --type=api` - Research-grade documentation
- `/reflection --agents=research --type=scientific` - Model architecture analysis
- `/multi-agent-optimize --agents=all --mode=optimize` - Comprehensive optimization

ARGUMENTS: [--framework=flax|equinox|both] [--architecture=mlp|cnn|transformer] [--functional] [--agents=auto|jax|scientific|ai|research|all] [--orchestrate] [--intelligent] [--breakthrough] [--optimize] [--distributed]