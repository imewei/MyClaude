---
description: JAX performance optimization - JIT caching, mixed precision, memory management, and computational efficiency
category: jax-performance
argument-hint: "[--technique=caching|mixed-precision|memory|profiling] [--gpu-accel] [--optimization] [--agents=auto|jax|scientific|ai|performance|all] [--orchestrate] [--intelligent] [--breakthrough] [--distributed] [--monitor]"
allowed-tools: "*"
model: inherit
---

# JAX Performance

JAX performance optimization techniques including JIT caching, mixed precision, memory management, and computational efficiency.

```bash
/jax-performance [--technique=caching|mixed-precision|memory|profiling] [--gpu-accel] [--optimization] [--agents=auto|jax|scientific|ai|performance|all] [--orchestrate] [--intelligent] [--breakthrough] [--distributed] [--monitor]
```

## Options

- `--technique=<technique>`: Focus on specific optimization (caching, mixed-precision, memory, profiling)
- `--gpu-accel`: Include GPU acceleration optimizations
- `--optimization`: Include advanced optimization techniques
- `--agents=<agents>`: Agent selection (auto, jax, scientific, ai, performance, all)
- `--orchestrate`: Enable advanced 23-agent orchestration with performance intelligence
- `--intelligent`: Enable intelligent agent selection based on performance analysis
- `--breakthrough`: Enable breakthrough performance optimization discovery
- `--distributed`: Enable distributed performance optimization across multiple agents
- `--monitor`: Enable real-time performance monitoring with agent intelligence

## What it does

1. **JIT Caching**: Optimize compilation caching for repeated function calls
2. **Mixed Precision**: Enable bfloat16/float32/float64 with automatic fallbacks
3. **Memory Management**: Efficient memory usage and allocation strategies
4. **Profiling**: Performance analysis and bottleneck identification
5. **GPU Optimization**: Device placement and utilization strategies
6. **23-Agent Performance Intelligence**: Multi-agent collaboration for breakthrough performance optimization
7. **Distributed Optimization**: Agent-coordinated performance optimization across multiple domains
8. **Real-Time Monitoring**: Agent-driven performance monitoring and adaptive optimization

## 23-Agent Intelligent Performance System

### Intelligent Agent Selection (`--intelligent`)
**Auto-Selection Algorithm**: Analyzes performance requirements, computational complexity, and resource constraints to automatically choose optimal agent combinations from the 23-agent library.

```bash
# Performance Type Detection → Agent Selection
- GPU/TPU Optimization → jax-pro + ai-systems-architect + systems-architect
- Memory Optimization → systems-architect + jax-pro + scientific-computing-master
- Training Performance → neural-networks-master + jax-pro + ai-systems-architect
- Scientific Computing Performance → scientific-computing-master + jax-pro + research-intelligence-master
- Production Performance → ai-systems-architect + systems-architect + jax-pro
```

### Core JAX Performance Optimization Agents

#### **`jax-pro`** - JAX Performance Optimization Expert
- **JIT Optimization**: Advanced JIT compilation strategies and caching optimization
- **Memory Engineering**: JAX-specific memory management and allocation strategies
- **GPU/TPU Mastery**: Device placement optimization and hardware acceleration
- **XLA Optimization**: Low-level XLA optimization and computational graph optimization
- **Performance Profiling**: JAX ecosystem performance analysis and bottleneck identification

#### **`systems-architect`** - System-Level Performance Engineering
- **Resource Management**: System-level resource allocation and computational optimization
- **Infrastructure Performance**: Hardware utilization and system bottleneck optimization
- **Scalability Engineering**: Performance optimization for distributed and large-scale systems
- **Memory Architecture**: System memory hierarchy optimization and cache management
- **Performance Monitoring**: Real-time system performance tracking and adaptive optimization

#### **`ai-systems-architect`** - AI Performance & Scalability Specialist
- **ML Performance**: Machine learning workload optimization and inference acceleration
- **Distributed Computing**: Multi-device and multi-node performance optimization
- **Production Optimization**: Performance engineering for production AI systems
- **Resource Allocation**: Optimal resource distribution for AI workloads
- **Performance Intelligence**: AI-driven performance optimization and predictive tuning

#### **`neural-networks-master`** - Deep Learning Performance Expert
- **Training Optimization**: Neural network training performance and convergence optimization
- **Architecture Performance**: Model architecture optimization for computational efficiency
- **Gradient Optimization**: Efficient gradient computation and memory management
- **Large Model Optimization**: Performance strategies for large-scale neural networks
- **Multi-Modal Performance**: Performance optimization for complex multi-modal models

### Specialized Performance Agents

#### **`scientific-computing-master`** - Scientific Performance Engineering
- **Numerical Performance**: High-performance numerical computing optimization
- **Scientific Workloads**: Performance optimization for computational science applications
- **Multi-Scale Performance**: Performance strategies for multi-scale scientific problems
- **Domain Optimization**: Performance tuning for specific scientific domains
- **Research Performance**: Performance engineering for research-grade computational standards

#### **`research-intelligence-master`** - Research Performance Innovation
- **Performance Research**: Cutting-edge performance optimization research and methodologies
- **Experimental Optimization**: Novel performance tuning approaches and breakthrough techniques
- **Academic Performance**: Research-grade performance standards and reproducibility
- **Innovation Synthesis**: Cross-domain performance optimization innovation
- **Benchmark Innovation**: Advanced benchmarking and performance evaluation methodologies

#### **`data-professional`** - Data Performance Optimization
- **Data Pipeline Performance**: High-performance data processing and ETL optimization
- **I/O Optimization**: Data loading and streaming performance optimization
- **Storage Performance**: Efficient data storage and retrieval strategies
- **Memory-Efficient Data**: Data structure optimization for memory-constrained environments
- **Real-Time Performance**: Performance optimization for streaming and real-time data processing

### Advanced Agent Selection Strategies

#### **`auto`** - Intelligent Agent Selection for Performance
Automatically analyzes performance requirements and selects optimal agent combinations:
- **Performance Analysis**: Detects bottlenecks, resource constraints, computational patterns
- **Resource Assessment**: Evaluates hardware capabilities and optimization opportunities
- **Agent Matching**: Maps performance challenges to relevant agent expertise
- **Optimization Focus**: Balances comprehensive optimization with practical implementation

#### **`jax`** - JAX-Specialized Performance Team
- `jax-pro` (JAX ecosystem lead)
- `systems-architect` (system optimization)
- `ai-systems-architect` (AI performance)
- `neural-networks-master` (ML optimization)

#### **`scientific`** - Scientific Computing Performance Team
- `scientific-computing-master` (lead)
- `jax-pro` (JAX implementation)
- `systems-architect` (system performance)
- `research-intelligence-master` (research methodology)
- Domain-specific experts based on scientific application

#### **`ai`** - AI/ML Performance Optimization Team
- `ai-systems-architect` (lead)
- `neural-networks-master` (ML performance)
- `jax-pro` (JAX optimization)
- `systems-architect` (infrastructure)
- `data-professional` (data performance)

#### **`performance`** - Dedicated Performance Engineering Team
- `systems-architect` (lead)
- `jax-pro` (JAX performance)
- `ai-systems-architect` (AI performance)
- `research-intelligence-master` (performance innovation)
- `data-professional` (data performance)

#### **`all`** - Complete 23-Agent Performance Ecosystem
Activates all relevant agents with intelligent orchestration for breakthrough performance optimization.

### 23-Agent Performance Orchestration (`--orchestrate`)

#### **Multi-Agent Performance Pipeline**
1. **Performance Analysis Phase**: Multiple agents analyze different performance aspects simultaneously
2. **Bottleneck Identification**: Collaborative identification of performance bottlenecks across domains
3. **Optimization Strategy**: Multi-agent development of comprehensive optimization strategies
4. **Implementation Coordination**: Agent-coordinated implementation of performance optimizations
5. **Real-Time Monitoring**: Continuous multi-agent performance monitoring and adaptive optimization

#### **Breakthrough Performance Discovery (`--breakthrough`)**
- **Cross-Domain Innovation**: Performance techniques from multiple domains and research areas
- **Emergent Optimization**: Novel performance strategies discovered through agent collaboration
- **Research-Grade Performance**: Academic and industry-leading performance standards
- **Adaptive Strategies**: Dynamic performance optimization based on real-time analysis

### Advanced 23-Agent Performance Examples

```bash
# Intelligent auto-selection for performance optimization
/jax-performance --agents=auto --intelligent --technique=caching --gpu-accel

# Scientific computing performance with specialized agents
/jax-performance --agents=scientific --optimization --orchestrate --monitor

# AI/ML performance optimization with scalability focus
/jax-performance --agents=ai --gpu-accel --distributed --breakthrough

# Research-grade performance development
/jax-performance --agents=all --breakthrough --orchestrate --distributed

# JAX-specialized performance optimization
/jax-performance --agents=jax --technique=mixed-precision --optimization

# Complete 23-agent performance ecosystem
/jax-performance --agents=all --orchestrate --breakthrough --monitor

# GPU acceleration optimization
/jax-performance gpu_workload.py --agents=jax --gpu-accel --intelligent

# Memory optimization for large models
/jax-performance large_model.py --agents=ai --technique=memory --distributed

# Training performance optimization
/jax-performance training_loop.py --agents=ai --optimization --breakthrough

# Scientific simulation performance
/jax-performance simulation.py --agents=scientific --orchestrate --monitor

# Production performance tuning
/jax-performance production_code.py --agents=performance --optimization --intelligent

# Cross-platform performance analysis
/jax-performance multiplatform.py --agents=all --distributed --breakthrough
```

### Intelligent Agent Selection Examples

```bash
# Performance Type Detection → Intelligent Agent Selection

# GPU-intensive deep learning workload
/jax-performance gpu_training.py --agents=auto --intelligent
# → Selects: jax-pro + ai-systems-architect + neural-networks-master

# Memory-constrained scientific computing
/jax-performance memory_limited.py --agents=auto --intelligent
# → Selects: systems-architect + scientific-computing-master + jax-pro

# Production ML inference optimization
/jax-performance inference_service.py --agents=auto --intelligent
# → Selects: ai-systems-architect + systems-architect + jax-pro

# Research performance benchmarking
/jax-performance research_benchmark.py --agents=auto --intelligent
# → Selects: research-intelligence-master + jax-pro + systems-architect

# Large-scale distributed training
/jax-performance distributed_training.py --agents=auto --intelligent
# → Selects: ai-systems-architect + neural-networks-master + multi-agent-orchestrator

# Comprehensive performance audit
/jax-performance complete_system.py --agents=all --orchestrate --breakthrough
# → Activates: All 23 agents with intelligent coordination for maximum performance optimization
```

## Example output

```python
import jax
import jax.numpy as jnp
import jax.random as random
from jax import jit, grad, vmap, pmap
import time
import functools

# Initialize PRNG key
key = random.PRNGKey(0)

# ============================================================================
# 1. JIT COMPILATION OPTIMIZATION
# ============================================================================

# Efficient JIT caching with static arguments
@functools.partial(jit, static_argnums=(1, 2))
def optimized_matmul(x, n_layers, activation_fn):
    """Optimized matrix multiplication with static parameters"""
    result = x
    for _ in range(n_layers):
        result = jnp.dot(result, result.T)
        if activation_fn == 'relu':
            result = jnp.maximum(0, result)
        elif activation_fn == 'tanh':
            result = jnp.tanh(result)
    return result

# Avoid recompilation with proper static arguments
# GOOD: Static arguments prevent recompilation
good_result = optimized_matmul(x, n_layers=3, activation_fn='relu')

# BAD: Dynamic arguments cause recompilation
# bad_result = optimized_matmul(x, n_layers, activation_fn)  # Recompiles each time

# JIT with donation for memory efficiency
@jit(donate_argnums=(0,))  # Donate first argument to avoid copying
def inplace_update(x, y):
    return x.at[0].set(y)

# Pre-allocate and reuse arrays
def efficient_training_loop(n_steps, batch_size, model_dim):
    """Pre-allocate arrays for training loop"""
    # Pre-allocate once
    params = jnp.ones((model_dim, model_dim))
    gradients = jnp.zeros_like(params)

    @jit
    def step(params, batch):
        loss = jnp.sum(jnp.dot(batch, params) ** 2)
        return grad(lambda p: jnp.sum(jnp.dot(batch, p) ** 2))(params)

    for i in range(n_steps):
        batch = random.normal(key, (batch_size, model_dim))
        gradients = step(params, batch)
        params = params - 0.01 * gradients

    return params

# ============================================================================
# 2. MIXED PRECISION OPTIMIZATION
# ============================================================================

# Enable mixed precision globally
jax.config.update('jax_enable_x64', False)  # Use 32-bit by default

def mixed_precision_training(x, y):
    """Training with mixed precision for memory and speed"""

    # Use bfloat16 for forward pass (faster, less memory)
    x_bf16 = x.astype(jnp.bfloat16)

    # Computation in bfloat16
    predictions = jnp.dot(x_bf16, jnp.ones((x.shape[1], 1), dtype=jnp.bfloat16))

    # Cast back to float32 for loss computation (better precision)
    predictions_f32 = predictions.astype(jnp.float32)
    y_f32 = y.astype(jnp.float32)

    loss = jnp.mean((predictions_f32 - y_f32) ** 2)
    return loss

# Automatic mixed precision with error handling
def safe_mixed_precision(fn, *args, **kwargs):
    """Automatically fall back to higher precision if needed"""
    try:
        # Try bfloat16 first
        args_bf16 = [arg.astype(jnp.bfloat16) if hasattr(arg, 'astype') else arg for arg in args]
        return fn(*args_bf16, **kwargs)
    except (ValueError, RuntimeError):
        # Fall back to float32
        args_f32 = [arg.astype(jnp.float32) if hasattr(arg, 'astype') else arg for arg in args]
        return fn(*args_f32, **kwargs)

# ============================================================================
# 3. MEMORY OPTIMIZATION
# ============================================================================

# Memory-efficient gradient computation
def gradient_checkpointing(fn, x):
    """Gradient checkpointing to reduce memory usage"""
    def checkpointed_fn(x):
        # Recompute forward pass during backward pass
        return jax.remat(fn)(x)
    return grad(checkpointed_fn)(x)

# Batch processing with memory limits
def memory_efficient_batch_processing(data, batch_size, process_fn):
    """Process large datasets in memory-efficient batches"""
    n_samples = data.shape[0]
    results = []

    for i in range(0, n_samples, batch_size):
        batch_end = min(i + batch_size, n_samples)
        batch = data[i:batch_end]

        # Process batch and immediately collect result
        batch_result = process_fn(batch)
        results.append(batch_result)

        # Optional: Force garbage collection
        # del batch, batch_result
        # gc.collect()

    return jnp.concatenate(results, axis=0)

# Device memory monitoring
def check_memory_usage():
    """Monitor device memory usage"""
    for i, device in enumerate(jax.devices()):
        try:
            stats = device.memory_stats()
            total_memory = stats.get('bytes_limit', 'Unknown')
            used_memory = stats.get('bytes_in_use', 'Unknown')
            print(f"Device {i}: {used_memory}/{total_memory} bytes")
        except AttributeError:
            print(f"Device {i}: Memory stats not available")

# Memory-efficient array operations
def efficient_array_ops():
    """Memory-efficient array operations"""

    # Use in-place operations when possible
    def inplace_update(arr, indices, values):
        return arr.at[indices].set(values)

    # Avoid intermediate arrays
    def efficient_computation(x):
        # GOOD: Single expression, no intermediate arrays
        return jnp.sum(jnp.sin(x) ** 2 + jnp.cos(x) ** 2)

    # BAD: Creates intermediate arrays
    def inefficient_computation(x):
        sin_x = jnp.sin(x)
        cos_x = jnp.cos(x)
        sin_squared = sin_x ** 2
        cos_squared = cos_x ** 2
        return jnp.sum(sin_squared + cos_squared)

# ============================================================================
# 4. PERFORMANCE PROFILING
# ============================================================================

def profile_jax_function(fn, *args, **kwargs):
    """Profile JAX function performance"""

    # Warm up (trigger compilation)
    _ = fn(*args, **kwargs)

    # Time compilation + execution (first call after changes)
    start_time = time.time()
    result = fn(*args, **kwargs)
    result.block_until_ready()  # Wait for computation
    total_time = time.time() - start_time

    # Time execution only (subsequent calls)
    start_time = time.time()
    result = fn(*args, **kwargs)
    result.block_until_ready()
    execution_time = time.time() - start_time

    print(f"Total time (compilation + execution): {total_time:.4f}s")
    print(f"Execution time: {execution_time:.4f}s")
    print(f"Compilation overhead: {total_time - execution_time:.4f}s")

    return result

# Memory profiling
def profile_memory_usage(fn, *args, **kwargs):
    """Profile memory usage of JAX function"""

    # Get initial memory stats
    initial_stats = {}
    for i, device in enumerate(jax.devices()):
        try:
            initial_stats[i] = device.memory_stats()['bytes_in_use']
        except (AttributeError, KeyError):
            initial_stats[i] = 0

    # Execute function
    result = fn(*args, **kwargs)
    result.block_until_ready()

    # Get final memory stats
    for i, device in enumerate(jax.devices()):
        try:
            final_memory = device.memory_stats()['bytes_in_use']
            memory_diff = final_memory - initial_stats[i]
            print(f"Device {i} memory change: {memory_diff:,} bytes")
        except (AttributeError, KeyError):
            print(f"Device {i}: Memory profiling not available")

    return result

# Benchmark comparison
def benchmark_functions(functions, args, n_runs=10):
    """Benchmark multiple function implementations"""
    results = {}

    for name, fn in functions.items():
        times = []

        # Warm up
        _ = fn(*args)

        # Time multiple runs
        for _ in range(n_runs):
            start_time = time.time()
            result = fn(*args)
            result.block_until_ready()
            times.append(time.time() - start_time)

        avg_time = sum(times) / len(times)
        min_time = min(times)
        results[name] = {'avg': avg_time, 'min': min_time}
        print(f"{name}: avg={avg_time:.4f}s, min={min_time:.4f}s")

    return results

# ============================================================================
# 5. GPU OPTIMIZATION
# ============================================================================

# Device placement optimization
def optimize_device_placement():
    """Optimize computation placement across devices"""

    devices = jax.devices()
    print(f"Available devices: {devices}")

    # Explicit device placement
    with jax.default_device(devices[0]):
        gpu_data = jnp.ones((1000, 1000))

    # Check device placement
    print(f"Data device: {gpu_data.device()}")

# Multi-GPU parallel processing
def multi_gpu_computation(data):
    """Distribute computation across multiple GPUs"""

    if jax.device_count() == 1:
        print("Single device - using regular computation")
        return jnp.sum(data ** 2)

    # Shard data across devices
    n_devices = jax.device_count()
    sharded_data = data.reshape(n_devices, -1, *data.shape[1:])

    @pmap
    def parallel_computation(x):
        return jnp.sum(x ** 2)

    # Parallel computation across devices
    results = parallel_computation(sharded_data)
    return jnp.sum(results)

# GPU memory optimization
def gpu_memory_optimization():
    """Optimize GPU memory usage"""

    # Enable memory preallocation (can improve performance)
    jax.config.update('jax_platform_name', 'gpu')

    # Control memory allocation
    import os
    os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
    os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'  # Use 80% of GPU memory

# ============================================================================
# 6. PRACTICAL OPTIMIZATION EXAMPLES
# ============================================================================

# Optimized neural network training step
@jit
def optimized_training_step(params, batch_x, batch_y, learning_rate):
    """Optimized training step with all performance techniques"""

    def loss_fn(p):
        # Forward pass with efficient operations
        predictions = vmap(lambda x: jnp.dot(x, p))(batch_x)
        return jnp.mean((predictions - batch_y) ** 2)

    # Efficient gradient computation
    loss_value, gradients = jax.value_and_grad(loss_fn)(params)

    # Efficient parameter update
    new_params = params - learning_rate * gradients

    return new_params, loss_value

# Performance monitoring for training
def monitor_training_performance(train_step_fn, params, data_loader):
    """Monitor training performance over time"""

    step_times = []
    memory_usage = []

    for batch_idx, (batch_x, batch_y) in enumerate(data_loader):
        start_time = time.time()

        params, loss = train_step_fn(params, batch_x, batch_y, 0.01)
        loss.block_until_ready()

        step_time = time.time() - start_time
        step_times.append(step_time)

        # Monitor memory every 10 steps
        if batch_idx % 10 == 0:
            try:
                memory = jax.devices()[0].memory_stats()['bytes_in_use']
                memory_usage.append(memory)
            except (AttributeError, KeyError):
                pass

        if batch_idx % 100 == 0:
            avg_time = sum(step_times[-100:]) / min(100, len(step_times))
            print(f"Step {batch_idx}: avg_time={avg_time:.4f}s, loss={loss:.4f}")

    return params, step_times, memory_usage

# Example usage
print("=== JAX Performance Optimization Examples ===")

# Profile a simple function
def simple_function(x):
    return jnp.sum(x ** 2)

key, subkey = random.split(key)
test_data = random.normal(subkey, (1000, 1000))

print("\n1. Performance Profiling:")
profiled_result = profile_jax_function(simple_function, test_data)

print("\n2. Memory Usage:")
memory_result = profile_memory_usage(simple_function, test_data)

print("\n3. Device Information:")
optimize_device_placement()

print("\n4. Memory Monitoring:")
check_memory_usage()
```

## Performance Best Practices

### JIT Optimization
- Use `static_argnums` for compile-time constants
- Avoid dynamic shapes that cause recompilation
- Pre-allocate arrays when possible
- Use `donate_argnums` for memory efficiency

### Memory Management
- Use gradient checkpointing for large models
- Process data in batches to control memory usage
- Monitor device memory usage regularly
- Use in-place operations when possible

### Mixed Precision
- Use bfloat16 for forward pass, float32 for gradients
- Implement automatic fallback to higher precision
- Monitor for numerical instability
- Profile memory and speed improvements

### GPU Optimization
- Distribute computation across multiple devices
- Use explicit device placement for large arrays
- Control memory preallocation settings
- Monitor GPU utilization and memory usage

### Profiling and Monitoring
- Always use `.block_until_ready()` for accurate timing
- Profile both compilation and execution time
- Monitor memory usage during training
- Benchmark different implementations

## Agent-Enhanced Performance Integration Patterns

### Complete Performance Optimization Workflow
```bash
# Intelligent performance analysis and optimization pipeline
/jax-performance --agents=auto --intelligent --optimization --monitor
/jax-training --agents=auto --optimizer=adam --optimize
/jax-debug --agents=auto --performance --profiling
```

### Scientific Computing Performance Pipeline
```bash
# High-performance scientific computing optimization
/jax-performance --agents=scientific --breakthrough --orchestrate
/jax-essentials --agents=scientific --operation=vmap --optimization
/run-all-tests --agents=scientific --benchmark --performance
```

### Production Performance Infrastructure
```bash
# Large-scale production performance optimization
/jax-performance --agents=ai --distributed --monitor --breakthrough
/optimize --agents=ai --language=jax --category=performance
/ci-setup --agents=ai --monitoring --performance
```

## Related Commands

**Prerequisites**: Commands to run before performance optimization
- `/jax-essentials --agents=auto` - Core JAX operations with agent optimization
- `/jax-models --agents=auto` - Model architecture optimization for performance
- `/jax-init --agents=auto` - JAX project setup with performance considerations

**Core Workflow**: Performance development with agent intelligence
- `/jax-training --agents=jax` - Training workflows with performance optimization
- `/jax-debug --agents=auto` - Debug performance issues with intelligent assistance
- `/python-debug-prof --agents=auto` - Python-specific profiling with agent intelligence

**Advanced Integration**: Specialized performance development
- `/jax-data-load --agents=ai` - Data loading performance optimization
- `/jax-sparse-ops --agents=scientific` - Sparse operation performance optimization
- `/jax-orbax-checkpoint --agents=ai` - Checkpointing performance optimization

**Quality Assurance**: Performance validation and monitoring
- `/generate-tests --agents=auto --type=performance` - Generate performance tests
- `/run-all-tests --agents=ai --benchmark` - Comprehensive performance testing
- `/optimize --agents=jax --language=jax` - Code optimization with JAX agents

**Research & Documentation**: Advanced performance workflows
- `/update-docs --agents=research --type=api` - Research-grade performance documentation
- `/reflection --agents=research --optimize=performance` - Performance methodology analysis
- `/multi-agent-optimize --agents=all --mode=optimize` - Comprehensive performance optimization