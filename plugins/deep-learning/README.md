# Deep Learning

Comprehensive deep learning expertise with systematic Chain-of-Thought frameworks: neural network theory, architecture design, training diagnostics, research translation, mathematical foundations, and production-ready implementations.

**Version:** 1.0.1 | **Category:** ai-ml | **License:** MIT

[![Maturity: Production Ready](https://img.shields.io/badge/maturity-production%20ready-green)]()
[![Agents: 2](https://img.shields.io/badge/agents-2-blue)]()
[![Skills: 6](https://img.shields.io/badge/skills-6-blue)]()

## 🎯 What's New in v1.0.1

- **Enhanced agents with systematic frameworks** - Both agents now include Chain-of-Thought reasoning and Constitutional AI principles
- **Comprehensive examples** - 4 detailed examples showing framework application with before/after metrics
- **Improved maturity** - neural-network-master: 78%→87%, neural-architecture-engineer: 75%→86%
- **Better skill discoverability** - All 6 skills enhanced with 3x more specific use cases

[See CHANGELOG.md →](./CHANGELOG.md)

---

## 🤖 Agents (2)

### neural-network-master (v1.0.1)

**Status:** Active | **Maturity:** 87% (target)

Deep learning theory expert combining mathematical rigor with pedagogical clarity. Specializes in theoretical foundations, optimization analysis, training diagnostics, and research translation with systematic reasoning frameworks.

**Key Features:**
- **4-step diagnostic framework** (24 questions):
  - Step 1: Symptom Analysis & Characterization
  - Step 2: Theoretical Hypothesis Generation
  - Step 3: Deep Mathematical Analysis & Explanation
  - Step 4: Theoretically-Grounded Solution Design
- **3 Constitutional AI principles** (24 self-checks):
  - Theoretical Rigor & Mathematical Accuracy (90% target)
  - Pedagogical Clarity & Intuition Building (85% target)
  - Practical Actionability & Implementation Guidance (80% target)
- **2 comprehensive examples**:
  - Vanishing Gradients Diagnosis (90% maturity)
  - Transformer Self-Attention Explanation (95% maturity)

**When to use:**
- Diagnosing training issues (vanishing/exploding gradients, dead neurons, saturation)
- Understanding deep learning theory (optimization, generalization, information theory)
- Explaining complex concepts (backpropagation, attention mechanisms, loss landscapes)
- Analyzing mathematical foundations of neural networks
- Translating research papers into theoretical understanding
- Providing pedagogical explanations for learning

**Example:**
```
User: "My 20-layer network isn't learning. Loss stuck at 2.28, gradients in layer 1 are 9×10^-13"

Agent applies 4-step framework:

Step 1 - Symptom Analysis:
- Loss barely decreases (2.3 → 2.28 over 100 epochs)
- Extremely small gradients in early layers (<10^-12)
- Network behaving like shallow network

Step 2 - Hypothesis Generation:
- Vanishing gradients (90% likelihood)
- Sigmoid activation causing exponential decay: (0.25)^20 ≈ 9×10^-13
- Chain rule multiplication through 20 layers

Step 3 - Mathematical Analysis:
∂L/∂W₁ = ∂L/∂h_L · ∏(∂h_i/∂h_{i-1}) · ∂h₁/∂W₁
With sigmoid: σ'(x) ≤ 0.25
Over 20 layers: (0.25)^20 ≈ 9×10^-13 ✓ matches observation

Step 4 - Solution Design:
A. ReLU activation (gradient = 1 for x > 0, prevents decay)
B. ResNet skip connections (gradient highways)
C. Batch normalization (stabilizes activations)

Self-assessment: 90% maturity
```

[Full neural-network-master documentation →](./agents/neural-network-master.md)

---

### neural-architecture-engineer (v1.0.1)

**Status:** Active | **Maturity:** 86% (target)

Neural architecture specialist for systematic design, training optimization, and multi-framework implementation. Combines architecture selection, implementation best practices, and production-ready deployment with rigorous quality gates.

**Key Features:**
- **5-step architecture design framework** (30 questions):
  - Step 1: Requirements Analysis & Problem Understanding
  - Step 2: Architecture Selection & Design Rationale
  - Step 3: Implementation Design & Best Practices
  - Step 4: Training Strategy & Optimization
  - Step 5: Validation, Iteration & Deployment
- **4 Constitutional AI principles** (32 self-checks):
  - Framework Best Practices & Code Quality (88% target)
  - Architecture Appropriateness & Design Rationale (85% target)
  - Training Robustness & Convergence (82% target)
  - Production Readiness & Deployment (80% target)
- **2 comprehensive examples**:
  - Vision Transformer Implementation in Flax (96.9% maturity)
  - Custom Architecture Debugging Workflow (93.8% maturity)

**Multi-Framework Expertise:**
- **Flax**: Production JAX, Linen API, ecosystem maturity
- **Equinox**: Functional PyTorch-like, research flexibility
- **Haiku**: DeepMind patterns, pure functional
- **PyTorch**: Industry standard, extensive ecosystem
- **Keras**: High-level API, rapid prototyping

**When to use:**
- Designing neural architectures (transformers, CNNs, RNNs, hybrid models)
- Implementing state-of-the-art models (ViT, ResNet, BERT, diffusion models)
- Debugging training convergence issues or architecture problems
- Comparing frameworks (Flax vs Equinox vs Haiku vs PyTorch)
- Optimizing model performance (training strategies, hyperparameters)
- Preparing architectures for production deployment

**Example:**
```
User: "My hybrid CNN-Transformer for medical segmentation won't converge.
      Loss stuck at 0.7, Dice score 0.12 after 50 epochs"

Agent applies 5-step framework:

Step 1 - Requirements Analysis:
- 500 train images (SMALL for transformers)
- 95% background, 5% foreground (severe class imbalance)
- Binary cross-entropy loss (problematic for imbalanced data)

Step 2 - Hypothesis Generation:
1. Class imbalance + BCE loss (90% likelihood) → loss ≈ -log(0.5) = 0.69 ✓
2. Insufficient data for transformer (80%)
3. Learning rate too high (40%)

Step 4 - Solution Implementation:
Solution 1: Dice loss (addresses imbalance)
Solution 2: Aggressive augmentation (addresses data scarcity)
Solution 3: Lower LR (1e-3 → 1e-4)

Results after iteration:
- Dice loss + lower LR: 0.12 → 0.63 (+0.51!)
- + Augmentation: 0.63 → 0.71 (+0.08)
- Simplify to U-Net: 0.71 → 0.75 (matches baseline)

Conclusion: Transformer decoder overfitting small dataset.
U-Net with Dice loss = production-ready (0.75 Dice)

Self-assessment: 93.8% debugging maturity
```

[Full neural-architecture-engineer documentation →](./agents/neural-architecture-engineer.md)

---

## 🎓 Skills (6)

### neural-network-mathematics

**Description:** Mathematical foundations for neural networks including linear algebra, calculus, probability theory, optimization, and information theory for deep understanding and custom implementations.

**Enhanced Use Cases (20 scenarios):**
- Deriving backpropagation for custom layers
- Computing Jacobians, Hessians, vector-Jacobian products
- Implementing automatic differentiation in JAX/PyTorch
- Analyzing gradient flow dynamics (vanishing/exploding)
- Understanding optimization algorithms (SGD, Adam, momentum)
- Working with probabilistic neural networks, Bayesian deep learning
- Computing matrix derivatives, efficient tensor operations (einsum)
- Debugging gradient implementations with numerical checking
- Understanding information theory (entropy, KL divergence, mutual information)
- Translating mathematical notation from papers into code

[Full neural-network-mathematics skill →](./skills/neural-network-mathematics/SKILL.md)

---

### training-diagnostics

**Description:** Systematic diagnosis and resolution of training issues including gradient pathologies, loss curve interpretation, convergence analysis, and performance debugging.

**Enhanced Use Cases (22 scenarios):**
- Diagnosing vanishing gradients (<1e-7, slow convergence)
- Debugging exploding gradients (NaN/Inf, gradient norms >100)
- Analyzing dead ReLU neurons (>40% zero activations)
- Interpreting loss curve anomalies (spikes, plateaus, divergence)
- Diagnosing overfitting (train-val gap) and underfitting (high losses)
- Investigating double descent phenomena
- Setting up gradient clipping thresholds
- Analyzing activation distributions for saturation
- Implementing weight initialization strategies (He, Xavier)
- Debugging training scripts with TensorBoard logging

[Full training-diagnostics skill →](./skills/training-diagnostics/SKILL.md)

---

### research-paper-implementation

**Description:** Translate research papers into production-ready implementations through systematic analysis, architecture extraction, and practical adaptation of SOTA methods.

**Enhanced Use Cases (18 scenarios):**
- Implementing SOTA architectures (Vision Transformers, Diffusion, CLIP, GPT)
- Translating mathematical formulations into PyTorch/JAX code
- Extracting architecture specifications from paper descriptions
- Reproducing experimental results and ablation studies
- Locating implementation details in appendices
- Finding unstated hyperparameters (learning rates, schedules)
- Adapting research to new domains beyond original scope
- Analyzing reference implementations on GitHub
- Debugging reproduction failures

[Full research-paper-implementation skill →](./skills/research-paper-implementation/SKILL.md)

---

### model-optimization-deployment

**Description:** Optimize and deploy neural networks with model compression, framework conversion, and serving infrastructure for production environments.

**Enhanced Use Cases (18 scenarios):**
- Implementing quantization (INT8, FP16, post-training, QAT)
- Applying pruning (structured, unstructured, magnitude-based)
- Using knowledge distillation (teacher-student models)
- Converting models between frameworks (ONNX, TFLite, TorchScript)
- Deploying to edge devices (TensorFlow Lite, Core ML)
- Optimizing for hardware (TensorRT for GPUs, ONNX Runtime for CPUs)
- Setting up serving (TorchServe, TensorFlow Serving, Triton)
- Implementing batch inference, model versioning, A/B testing
- Monitoring production performance and drift detection

[Full model-optimization-deployment skill →](./skills/model-optimization-deployment/SKILL.md)

---

### neural-architecture-patterns

**Description:** Design neural architectures using proven patterns including skip connections, attention mechanisms, normalization, and multi-scale processing across domains.

**Enhanced Use Cases (18 scenarios):**
- Implementing residual connections for deep networks
- Adding attention mechanisms (self-attention, multi-head, cross-attention)
- Selecting architectures for vision (ResNet, EfficientNet, ViT)
- Designing encoder-decoder models (autoencoders, U-Net, seq2seq)
- Building transformers (BERT encoders, GPT decoders)
- Choosing normalization (BatchNorm for CNNs, LayerNorm for transformers)
- Implementing multi-scale feature pyramids
- Understanding inductive biases (CNNs, RNNs, transformers)
- Creating custom PyTorch modules, JAX Flax modules

[Full neural-architecture-patterns skill →](./skills/neural-architecture-patterns/SKILL.md)

---

### deep-learning-experimentation

**Description:** Design systematic experiments with hyperparameter optimization, ablation studies, and reproducible research practices.

**Enhanced Use Cases (15 scenarios):**
- Tuning hyperparameters (learning rate, batch size, architecture)
- Conducting ablation studies
- Implementing experiment tracking (W&B, TensorBoard, MLflow)
- Setting up reproducible workflows (seed management, version control)
- Performing learning rate range tests
- Implementing statistical significance testing
- Optimizing multi-objective trade-offs (accuracy vs latency vs size)
- Managing experiment configurations (YAML/JSON)
- Documenting experimental results

[Full deep-learning-experimentation skill →](./skills/deep-learning-experimentation/SKILL.md)

---

## 🚀 Quick Start

### Installation

1. Ensure Claude Code is installed
2. Clone or copy this plugin to `.claude/plugins/deep-learning/`
3. Plugin will auto-load on next Claude Code session

### Using Agents

Activate agents using `@` mention:

```
@neural-network-master Explain why my gradients are vanishing in this 20-layer network

@neural-architecture-engineer Design a Vision Transformer for ImageNet classification
```

### Using Skills

Skills are automatically invoked when relevant. You can also explicitly reference:

```
Use the training-diagnostics skill to debug this convergence failure

Apply the neural-network-mathematics skill to derive backprop for this custom layer
```

---

## 📖 Examples

### Example 1: Theoretical Diagnosis (neural-network-master)

**Scenario:** Understanding vanishing gradients with mathematical rigor

```
User: "My 20-layer fully-connected network isn't learning.
      Sigmoid activations, loss stuck at 2.28"

Agent Response (4-step framework):

Step 1: Symptom Analysis
- Loss: 2.3 → 2.28 over 100 epochs (minimal progress)
- Training accuracy: ~12% (essentially random for 10 classes)
- Gradients in layer 1: ~9×10^-13 (vanishingly small)

Step 2: Theoretical Hypothesis
Mathematical principle: Chain rule gradient decay
For sigmoid: σ'(x) = σ(x)(1-σ(x)) ≤ 0.25
Over L=20 layers: gradient ≈ (0.25)^20 = 9.09×10^-13 ✓

Step 3: Mathematical Analysis
∂L/∂W₁ = ∂L/∂h_L · ∂h_L/∂h_{L-1} · ... · ∂h₁/∂W₁

Each layer multiplies by ≤0.25 → exponential decay
Visualization: Layer 1: 10^-13, Layer 10: 10^-7, Layer 20: 10^0

Step 4: Theoretically-Grounded Solutions
A. ReLU: ReLU'(x) = 1 for x>0 (no decay) → 78% → 85% accuracy
B. ResNet: Skip connections provide gradient highways
C. BatchNorm: Normalizes activations, prevents saturation

Implementation delegated to neural-architecture-engineer

Outcome: Switching to ReLU achieved 85% validation accuracy
```

---

### Example 2: Production Architecture (neural-architecture-engineer)

**Scenario:** Implementing Vision Transformer for ImageNet

```
User: "Help me implement a Vision Transformer for ImageNet classification.
      Need production-ready code with training workflow"

Agent Response (5-step framework):

Step 1: Requirements Analysis
- Input: 224×224×3 RGB images
- Output: 1000 classes (ImageNet)
- Target: >80% top-1 accuracy (competitive with ResNet-50)
- Hardware: 8× V100 GPUs, 32GB each
- Framework: Flax (production JAX ecosystem)

Step 2: Architecture Selection
Choice: ViT-Base/16 (86M parameters)
Rationale:
- Proven: 84.5% ImageNet accuracy in paper
- Parallelizable: Better GPU utilization than CNNs
- Fits memory: ~12GB for batch 128 per GPU

Step 3: Implementation Design
Components:
1. PatchEmbed: 16×16 patches → 768-dim tokens
2. TransformerEncoder: 12 layers, 12 heads, 3072 FFN dim
3. Classification head: MLP for 1000 classes
4. Training loop: AdamW + cosine schedule

Step 4: Training Strategy
- Optimizer: AdamW (weight decay 0.1)
- Learning rate: 3e-4 with 10K warmup steps
- Regularization: Dropout 0.1, Stochastic depth 0.1, Label smoothing 0.1
- Data augmentation: RandAugment, Mixup α=0.8

Step 5: Validation & Deployment
Results:
- Top-1 accuracy: 84.5% (matches paper ✓)
- Inference latency: 35ms on V100 (batch=32)
- Training time: 72 hours on 8× V100

Production deployment:
Week 1: Shadow mode, Week 2: 5% A/B test, Week 3-4: Full rollout

Self-assessment: 96.9% overall maturity
Files delivered: model.py, config.py, train.py, requirements.txt
```

---

### Example 3: Multi-Framework Comparison

**Scenario:** Choosing between Flax, Equinox, and Haiku for research

```
User: "Should I use Flax, Equinox, or Haiku for my transformer research?"

Agent Response:

Framework Comparison:

Flax (Linen API):
✅ Production-ready, mature ecosystem
✅ Excellent documentation, community support
✅ TrainState for training workflows
✅ Integration with Optax, TFDS
❌ More boilerplate than Equinox

Equinox:
✅ PyTorch-like API (familiar for PyTorch users)
✅ Minimal boilerplate, functional design
✅ Excellent for research prototyping
✅ Clean PyTree-based state management
❌ Smaller ecosystem than Flax

Haiku:
✅ Pure functional (DeepMind style)
✅ transform/apply paradigm
✅ Good for research-grade implementations
❌ Less ecosystem support than Flax

Recommendation for transformer research:
1. Start with Equinox for rapid prototyping
2. Migrate to Flax for production deployment
3. Use Haiku if following DeepMind papers closely

Example Equinox code:
```python
import equinox as eqx
class Transformer(eqx.Module):
    layers: list
    def __init__(self, ...):
        self.layers = [TransformerBlock(...) for _ in range(num_layers)]
```

Clean, minimal, research-friendly ✓
```

---

## 📊 Metrics & Performance

### Agent Maturity

| Agent | Version | Baseline | Target | Improvement |
|-------|---------|----------|--------|-------------|
| neural-network-master | v1.0.1 | 78% | 87% | +12% |
| neural-architecture-engineer | v1.0.1 | 75% | 86% | +15% |

### Expected Performance Improvements

**neural-network-master:**
- Task Success Rate: 78% → 88% (+13%)
- Theoretical Accuracy: 85% → 92% (+8%)
- Pedagogical Clarity: 75% → 88% (+17%)
- User Satisfaction: 7.5/10 → 8.5/10 (+13%)

**neural-architecture-engineer:**
- Task Success Rate: 75% → 87% (+16%)
- Implementation Quality: 80% → 90% (+13%)
- Training Success Rate: 70% → 85% (+21%)
- User Satisfaction: 7/10 → 8.5/10 (+21%)

### Skill Enhancement Impact

All 6 skills enhanced with **3x more specific use cases**:
- neural-network-mathematics: 10 → 20 scenarios
- training-diagnostics: 10 → 22 scenarios
- research-paper-implementation: 5 → 18 scenarios
- model-optimization-deployment: 6 → 18 scenarios
- neural-architecture-patterns: 5 → 18 scenarios
- deep-learning-experimentation: 6 → 15 scenarios

---

## 📚 Documentation

### User Guides
- **[CHANGELOG.md](./CHANGELOG.md)** - Complete version history with detailed improvements
- **[AGENT_ANALYSIS_SUMMARY.md](./AGENT_ANALYSIS_SUMMARY.md)** - Comprehensive agent analysis (450+ lines)

### Agent Documentation
- **[neural-network-master.md](./agents/neural-network-master.md)** - Complete specification (1,675 lines)
- **[neural-architecture-engineer.md](./agents/neural-architecture-engineer.md)** - Complete specification (1,403 lines)

### Skill Documentation
- [neural-network-mathematics/SKILL.md](./skills/neural-network-mathematics/SKILL.md)
- [training-diagnostics/SKILL.md](./skills/training-diagnostics/SKILL.md)
- [research-paper-implementation/SKILL.md](./skills/research-paper-implementation/SKILL.md)
- [model-optimization-deployment/SKILL.md](./skills/model-optimization-deployment/SKILL.md)
- [neural-architecture-patterns/SKILL.md](./skills/neural-architecture-patterns/SKILL.md)
- [deep-learning-experimentation/SKILL.md](./skills/deep-learning-experimentation/SKILL.md)

---

## 🔧 Configuration

### Agent Selection

Claude Code automatically selects the appropriate agent based on context. You can explicitly activate:

```
@neural-network-master      # For theoretical understanding, mathematical analysis
@neural-architecture-engineer # For architecture design, implementation, debugging
```

### Skill Activation

Skills are automatically invoked when relevant. The enhanced descriptions in v1.0.1 improve discoverability with 3x more use case scenarios per skill.

---

## 🗺️ Roadmap

### v1.1.0 (Planned Q1 2026)

**neural-network-master enhancements:**
- Add 3-5 more diverse examples (research translation, advanced diagnostics)
- Implement output format templates for common tasks
- Add self-verification loops in Constitutional AI
- Target maturity: 87% → 92%

**neural-architecture-engineer enhancements:**
- Add architecture decision trees for systematic selection
- Expand to 3-4 comprehensive examples across frameworks
- Add automated testing guidance
- Target maturity: 86% → 92%

### v1.2.0 (Planned Q2 2026)

- Enhanced skill coverage with domain-specific patterns
- Integration with external benchmarking datasets
- Advanced multi-framework interoperability examples
- Expanded production deployment patterns

---

## 🤝 Contributing

Contributions are welcome! Please see the contribution guidelines in the main repository.

### Reporting Issues

- Bug reports: Use GitHub issues with detailed reproduction steps
- Feature requests: Describe use case and expected behavior
- Documentation improvements: Submit pull requests

---

## 📜 License

MIT License - see LICENSE file for details

---

## 👤 Author

**Wei Chen**

---

## 🙏 Acknowledgments

- Agent improvements using **Agent Performance Optimization Workflow**
- Prompt engineering patterns from **Constitutional AI** research
- Deep learning best practices from **Goodfellow, Bengio, Courville "Deep Learning"**
- Architecture patterns from **Vision Transformers, ResNet, BERT** research
- Mathematical foundations from **The Matrix Cookbook**, **Pattern Recognition and Machine Learning**

---

**Built with ❤️ using Claude Code**
