---
name: research-quality-assessment
description: Evaluate scientific research quality across methodology, experimental design, statistical rigor, and publication readiness using systematic assessment frameworks. Use when reviewing research papers, manuscripts, or academic articles for quality and validity. Use when evaluating grant proposals for NSF, NIH, DOE, or other funding agencies. Use when assessing experimental design, sample size justification, or statistical power analysis. Use when reviewing statistical methods, hypothesis testing, p-values, effect sizes, or confidence intervals. Use when checking for research methodology issues like p-hacking, HARKing, or selective reporting. Use when evaluating data quality, missing data handling, or outlier treatment. Use when assessing reproducibility, replication, or open science practices. Use when reviewing manuscripts for journal submission (Nature, Science, Cell, PLOS, eLife). Use when conducting peer review or pre-submission review of research. Use when evaluating clinical trial designs (CONSORT), observational studies (STROBE), or systematic reviews (PRISMA). Use when assessing research ethics, IRB compliance, or informed consent procedures. Use when writing or reviewing methods sections, results sections, or discussion sections of papers.
---

# Research Quality Assessment

Comprehensive evaluation framework for scientific research quality across 6 critical dimensions.

## When to use this skill

- Reviewing research papers, manuscripts, or academic articles for quality
- Evaluating grant proposals for NSF, NIH, DOE, ERC, or other funding agencies
- Assessing experimental design before data collection
- Reviewing sample size calculations and statistical power analysis
- Evaluating statistical methods (t-tests, ANOVA, regression, Bayesian methods)
- Checking hypothesis testing, p-values, effect sizes, and confidence intervals
- Identifying methodology issues (p-hacking, HARKing, selective reporting, circular analysis)
- Assessing data quality, completeness, and integrity
- Evaluating missing data handling strategies (imputation, listwise deletion)
- Reviewing outlier detection and treatment methods
- Assessing reproducibility and replication practices
- Evaluating open science practices (pre-registration, open data, open code)
- Reviewing manuscripts for journal submission
- Conducting peer review or pre-submission quality checks
- Evaluating clinical trial designs using CONSORT guidelines
- Assessing observational studies using STROBE checklist
- Reviewing systematic reviews using PRISMA guidelines
- Evaluating meta-analyses for heterogeneity and publication bias
- Assessing research ethics, IRB/ethics committee compliance
- Reviewing informed consent procedures and data protection
- Writing or reviewing methods, results, or discussion sections

## Core Dimensions

### 1. Methodology Soundness (Weight: 20%)

- Appropriate research design selection
- Clear hypothesis formulation
- Valid measurement instruments
- Proper control conditions
- Reproducible protocols

### 2. Experimental Design Quality (Weight: 20%)

- Statistical power analysis (target: >= 0.80)
- Sample size justification
- Randomization and blinding
- Control group adequacy
- Bias mitigation strategies

### 3. Data Quality & Sufficiency (Weight: 15%)

- Data completeness and integrity
- Missing data handling
- Outlier treatment
- Data provenance documentation
- Quality control procedures

### 4. Statistical Analysis Rigor (Weight: 20%)

- Appropriate statistical tests
- Multiple testing correction
- Effect size reporting
- Confidence intervals
- Sensitivity analysis

### 5. Result Validity & Significance (Weight: 15%)

- Reproducibility of findings
- Statistical significance interpretation
- Practical significance assessment
- Limitation acknowledgment
- Alternative explanation consideration

### 6. Publication Readiness (Weight: 10%)

- Complete methods section
- Reproducibility package
- Data availability statement
- Code availability
- Adherence to reporting guidelines

## Scoring Rubric

| Score | Level | Description |
|-------|-------|-------------|
| 9-10 | Exceptional | Exceeds all standards, exemplary work |
| 7-8 | Strong | Meets all standards, minor improvements possible |
| 5-6 | Adequate | Meets minimum standards, improvements needed |
| 3-4 | Weak | Significant issues, major revisions required |
| 1-2 | Poor | Fundamental flaws, rejection recommended |
| 0 | Unacceptable | Does not meet basic criteria |

## Assessment Workflow

### Step 1: Initial Screening

```markdown
## Initial Screening Checklist

- [ ] Research question clearly stated
- [ ] Hypothesis is testable
- [ ] Methods section present and detailed
- [ ] Results reported with statistics
- [ ] Conclusions supported by data
```

### Step 2: Methodology Assessment

```markdown
## Methodology Evaluation

### Design Appropriateness
- Is the research design suitable for the question?
- Are controls adequate?
- Is the sample representative?

### Measurement Validity
- Are instruments validated?
- Is reliability reported?
- Are measurements objective?
```

### Step 3: Statistical Review

```markdown
## Statistical Analysis Checklist

- [ ] Appropriate tests for data type
- [ ] Assumptions checked and met
- [ ] Multiple comparisons corrected
- [ ] Effect sizes reported
- [ ] Confidence intervals provided
- [ ] Power analysis conducted
- [ ] Missing data addressed
```

### Step 4: Reproducibility Check

```markdown
## Reproducibility Assessment

- [ ] Methods sufficiently detailed
- [ ] Data available or described
- [ ] Code/analysis scripts available
- [ ] Materials accessible
- [ ] Protocol registered (if applicable)
```

## Journal-Specific Standards

### Nature/Science/Cell (Impact Factor > 30)

- Exceptional novelty required
- Multiple independent validations
- Comprehensive supplementary materials
- Data repository requirements
- Extended methods section

### PLOS ONE / eLife (Open Access)

- Sound methodology primary criterion
- Open data requirements
- Registered reports preferred
- Transparent peer review

### Field-Specific Journals

- Adherence to field reporting standards
- CONSORT (clinical trials)
- STROBE (observational studies)
- PRISMA (systematic reviews)

## Common Issues

### Red Flags

1. **Underpowered Studies**: Sample too small for claims
2. **P-hacking**: Multiple testing without correction
3. **HARKing**: Hypothesizing after results known
4. **Selective Reporting**: Cherry-picked outcomes
5. **Circular Analysis**: Same data for discovery and validation

### Improvement Recommendations

```markdown
## Typical Revision Requests

1. "Please provide power analysis justification"
2. "Add effect sizes with confidence intervals"
3. "Address multiple comparison correction"
4. "Include sensitivity analysis"
5. "Provide reproducibility materials"
```

## Best Practices

1. **Pre-registration**: Register hypotheses before data collection
2. **Blinding**: Blind analysts to conditions when possible
3. **Replication**: Include internal replication
4. **Transparency**: Share data, code, and materials
5. **Reporting Standards**: Follow field-specific guidelines
