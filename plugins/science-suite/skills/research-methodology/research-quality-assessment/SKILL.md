---
name: research-quality-assessment
version: "1.0.7"
maturity: "5-Expert"
specialization: Research Quality Evaluation
description: Evaluate scientific research quality across methodology, experimental design, statistical rigor, and publication readiness. Use when reviewing papers, grant proposals, or assessing reproducibility using CONSORT, STROBE, or PRISMA guidelines.
---

# Research Quality Assessment

Systematic evaluation framework for scientific research quality.

---

## Assessment Dimensions

| Dimension | Weight | Key Criteria |
|-----------|--------|--------------|
| Methodology | 20% | Design selection, controls, reproducibility |
| Experimental Design | 20% | Power analysis, randomization, blinding |
| Data Quality | 15% | Completeness, missing data handling |
| Statistical Rigor | 20% | Appropriate tests, effect sizes, corrections |
| Result Validity | 15% | Reproducibility, practical significance |
| Publication Readiness | 10% | Methods detail, data availability |

---

## Scoring Rubric

| Score | Level | Description |
|-------|-------|-------------|
| 9-10 | Exceptional | Exceeds all standards, exemplary |
| 7-8 | Strong | Meets standards, minor improvements |
| 5-6 | Adequate | Minimum standards, improvements needed |
| 3-4 | Weak | Significant issues, major revisions |
| 1-2 | Poor | Fundamental flaws, rejection likely |

---

## Reporting Guidelines

| Study Type | Guideline | Key Requirements |
|------------|-----------|------------------|
| Clinical Trials | CONSORT | Randomization, flow diagram, ITT |
| Observational | STROBE | Selection, confounding, bias |
| Systematic Reviews | PRISMA | Search strategy, selection, synthesis |
| Meta-Analysis | MOOSE | Heterogeneity, publication bias |

---

## Red Flags

| Issue | Detection |
|-------|-----------|
| Underpowered | Sample too small for effect size claimed |
| P-hacking | Multiple tests without correction |
| HARKing | Hypothesis after results known |
| Selective reporting | Cherry-picked outcomes |
| Circular analysis | Same data for discovery and validation |

---

## Statistical Checklist

- [ ] Appropriate test for data type
- [ ] Assumptions checked and met
- [ ] Multiple comparisons corrected
- [ ] Effect sizes with confidence intervals
- [ ] Power analysis conducted
- [ ] Missing data addressed
- [ ] Sensitivity analysis performed

---

## Reproducibility Checklist

- [ ] Methods sufficiently detailed
- [ ] Data available or access described
- [ ] Analysis code/scripts available
- [ ] Materials accessible
- [ ] Protocol pre-registered (if applicable)

---

## Best Practices

| Practice | Implementation |
|----------|----------------|
| Pre-registration | Register hypotheses before data collection |
| Blinding | Blind analysts to conditions |
| Internal replication | Include validation cohort |
| Transparency | Share data, code, materials |
| Reporting standards | Follow field-specific guidelines |

---

## Common Pitfalls

| Pitfall | Solution |
|---------|----------|
| Missing power analysis | Calculate before data collection |
| No effect sizes | Report Cohen's d or equivalent |
| Ignoring assumptions | Check residuals, use robust methods |
| Causal overclaims | Match claims to study design |
| Poor documentation | Detailed methods, supplementary info |

---

## Checklist

- [ ] Research question clearly stated
- [ ] Design appropriate for question
- [ ] Sample size justified
- [ ] Appropriate statistical methods
- [ ] Effect sizes reported
- [ ] Limitations acknowledged
- [ ] Reproducibility materials provided
- [ ] Reporting guidelines followed

---

**Version**: 1.0.5
