---
description: Set up probabilistic models with Numpyro including distributions and MCMC sampling
category: jax-probabilistic
argument-hint: "[--model-type=regression|classification|hierarchical] [--inference=mcmc|svi] [--sampling=nuts|hmc] [--agents=auto|jax|scientific|ai|research|all] [--orchestrate] [--intelligent] [--breakthrough] [--optimize] [--bayesian]"
allowed-tools: "*"
model: inherit
---

# JAX NumPyro Prob

Set up probabilistic models with NumPyro including distributions and MCMC sampling.

```bash
/jax-numpyro-prob [--model-type=regression|classification|hierarchical] [--inference=mcmc|svi] [--sampling=nuts|hmc] [--agents=auto|jax|scientific|ai|research|all] [--orchestrate] [--intelligent] [--breakthrough] [--optimize] [--bayesian]
```

## Options

- `--model-type=<type>`: Type of probabilistic model (regression, classification, hierarchical)
- `--inference=<method>`: Inference method (mcmc, svi)
- `--sampling=<sampler>`: MCMC sampler type (nuts, hmc)
- `--agents=<agents>`: Agent selection (auto, jax, scientific, ai, research, all)
- `--orchestrate`: Enable advanced 23-agent orchestration with probabilistic intelligence
- `--intelligent`: Enable intelligent agent selection based on probabilistic analysis
- `--breakthrough`: Enable breakthrough probabilistic modeling and inference techniques
- `--optimize`: Apply performance optimization to probabilistic workflows
- `--bayesian`: Advanced Bayesian methodology with research-grade standards

## What it does

1. **Probabilistic Models**: Define Bayesian models with NumPyro
2. **Distributions**: Set up priors and likelihood functions
3. **MCMC Sampling**: Implement NUTS and HMC samplers
4. **Variational Inference**: Use automatic guides and SVI
5. **Model Diagnostics**: Analyze convergence and posterior distributions
6. **23-Agent Probabilistic Intelligence**: Multi-agent collaboration for optimal Bayesian modeling
7. **Advanced Inference**: Agent-driven inference optimization and methodology selection
8. **Research-Grade Bayesian Methods**: Agent-coordinated research-quality probabilistic workflows

## 23-Agent Intelligent Probabilistic System

### Intelligent Agent Selection (`--intelligent`)
**Auto-Selection Algorithm**: Analyzes probabilistic modeling requirements, inference complexity, and research goals to automatically choose optimal agent combinations from the 23-agent library.

```bash
# Model Type Detection → Agent Selection
- Bayesian Research → research-intelligence-master + scientific-computing-master + jax-pro
- Scientific Modeling → scientific-computing-master + research-intelligence-master + correlation-function-expert
- Production Inference → ai-systems-architect + jax-pro + systems-architect
- Complex Hierarchical Models → research-intelligence-master + multi-agent-orchestrator + scientific-computing-master
- Domain-Specific Modeling → domain experts + scientific-computing-master + research-intelligence-master
```

### Core Probabilistic Modeling Agents

#### **`research-intelligence-master`** - Bayesian Research & Methodology Expert
- **Bayesian Methodology**: Advanced Bayesian inference methodologies and research standards
- **Model Development**: Cutting-edge probabilistic model design and innovation
- **Academic Standards**: Research-grade probabilistic modeling for publication
- **Innovation Synthesis**: Novel Bayesian approaches and breakthrough techniques
- **Reproducibility**: Research reproducibility frameworks for probabilistic modeling

#### **`scientific-computing-master`** - Scientific Probabilistic Computing
- **Numerical Bayesian Methods**: High-performance numerical Bayesian computation
- **Scientific Applications**: Probabilistic modeling for computational science domains
- **Multi-Scale Modeling**: Bayesian approaches for multi-scale scientific problems
- **Domain Integration**: Probabilistic modeling for specific scientific disciplines
- **Research Computing**: Scientific computing standards for probabilistic workflows

#### **`jax-pro`** - JAX Probabilistic Optimization Expert
- **NumPyro Mastery**: Deep expertise in NumPyro and JAX probabilistic programming
- **Inference Optimization**: MCMC and SVI performance optimization with JAX
- **GPU Acceleration**: Hardware-accelerated probabilistic inference
- **Memory Efficiency**: Large-scale probabilistic model optimization
- **Performance Engineering**: JAX-specific probabilistic computing optimization

#### **`correlation-function-expert`** - Statistical Modeling & Analysis Specialist
- **Statistical Methods**: Advanced statistical modeling and correlation analysis
- **Bayesian Statistics**: Expert statistical inference and model validation
- **Experimental Design**: Statistical experimental design for probabilistic studies
- **Data Analysis**: Statistical data analysis and interpretation methodologies
- **Model Validation**: Statistical model checking and diagnostic techniques

### Specialized Probabilistic Agents

#### **`ai-systems-architect`** - AI Probabilistic Systems & Production
- **Production Bayesian Systems**: Scalable probabilistic systems for production AI
- **Uncertainty Quantification**: Production uncertainty quantification and risk assessment
- **Bayesian ML**: Probabilistic machine learning system architecture
- **Resource Optimization**: Computational resource management for large-scale inference
- **System Integration**: Probabilistic system integration with AI infrastructure

#### **`neural-networks-master`** - Bayesian Deep Learning Expert
- **Bayesian Neural Networks**: Probabilistic neural network architectures
- **Uncertainty in ML**: Uncertainty quantification in deep learning
- **Variational Methods**: Advanced variational inference for neural networks
- **Bayesian Optimization**: Probabilistic optimization for hyperparameter tuning
- **Model Uncertainty**: Neural network uncertainty estimation and calibration

#### **`systems-architect`** - Probabilistic System Infrastructure
- **Computational Infrastructure**: System architecture for probabilistic computing
- **Scalability Engineering**: Large-scale probabilistic system design
- **Resource Management**: Computational resource optimization for Bayesian inference
- **Performance Monitoring**: Real-time probabilistic system performance tracking
- **Fault Tolerance**: Robust probabilistic systems with failure recovery

### Domain-Specific Probabilistic Agents

#### **`neutron-soft-matter-expert`** - Neutron Scattering Bayesian Analysis
- **Bayesian Neutron Analysis**: Probabilistic modeling for neutron scattering data
- **Physical Model Integration**: Bayesian integration of physical neutron models
- **Uncertainty Quantification**: Uncertainty analysis for neutron experiments
- **Parameter Estimation**: Bayesian parameter estimation for neutron studies
- **Model Selection**: Bayesian model comparison for neutron scattering

#### **`xray-soft-matter-expert`** - X-ray Bayesian Analysis Specialist
- **Bayesian X-ray Analysis**: Probabilistic modeling for X-ray scattering data
- **Structure Determination**: Bayesian approaches to structure determination
- **Data Integration**: Bayesian integration of multi-modal X-ray data
- **Model Uncertainty**: Uncertainty quantification in X-ray analysis
- **Experimental Design**: Bayesian experimental design for X-ray studies

#### **`nonequilibrium-stochastic-expert`** - Stochastic Process Modeling
- **Stochastic Modeling**: Advanced stochastic process modeling and analysis
- **Nonequilibrium Systems**: Bayesian modeling of nonequilibrium phenomena
- **Process Inference**: Bayesian inference for stochastic processes
- **Dynamic Systems**: Probabilistic modeling of dynamic systems
- **Time Series Bayesian**: Advanced Bayesian time series analysis

### Advanced Agent Selection Strategies

#### **`auto`** - Intelligent Agent Selection for Probabilistic Modeling
Automatically analyzes modeling requirements and selects optimal agent combinations:
- **Model Analysis**: Detects model complexity, inference requirements, research goals
- **Domain Assessment**: Evaluates scientific domain and application context
- **Agent Matching**: Maps probabilistic challenges to relevant agent expertise
- **Methodology Optimization**: Balances comprehensive modeling with computational efficiency

#### **`jax`** - JAX-Specialized Probabilistic Team
- `jax-pro` (JAX ecosystem lead)
- `research-intelligence-master` (Bayesian methodology)
- `scientific-computing-master` (numerical methods)
- `ai-systems-architect` (system integration)

#### **`scientific`** - Scientific Computing Probabilistic Team
- `scientific-computing-master` (lead)
- `research-intelligence-master` (research methodology)
- `jax-pro` (JAX implementation)
- `correlation-function-expert` (statistical methods)
- Domain-specific experts based on scientific application

#### **`ai`** - AI/ML Probabilistic Team
- `ai-systems-architect` (lead)
- `neural-networks-master` (Bayesian ML)
- `research-intelligence-master` (methodology)
- `jax-pro` (JAX optimization)
- `systems-architect` (infrastructure)

#### **`research`** - Research-Grade Probabilistic Development
- `research-intelligence-master` (lead)
- `scientific-computing-master` (computational methods)
- `correlation-function-expert` (statistical analysis)
- `jax-pro` (implementation)
- Domain-specific scientific experts

#### **`all`** - Complete 23-Agent Probabilistic Ecosystem
Activates all relevant agents with intelligent orchestration for breakthrough probabilistic modeling.

### 23-Agent Probabilistic Orchestration (`--orchestrate`)

#### **Multi-Agent Probabilistic Pipeline**
1. **Model Design Phase**: Multiple agents analyze modeling requirements simultaneously
2. **Inference Strategy**: Collaborative development of optimal inference methodologies
3. **Implementation Optimization**: Agent-coordinated optimization of probabilistic workflows
4. **Validation Framework**: Multi-agent model validation and diagnostic analysis
5. **Research Integration**: Comprehensive integration with research standards and publication

#### **Breakthrough Probabilistic Discovery (`--breakthrough`)**
- **Cross-Domain Innovation**: Probabilistic techniques from multiple scientific domains
- **Emergent Methodology**: Novel probabilistic approaches through agent collaboration
- **Research-Grade Standards**: Academic and industry-leading probabilistic standards
- **Adaptive Inference**: Dynamic inference strategy optimization based on model performance

### Advanced 23-Agent Probabilistic Examples

```bash
# Intelligent auto-selection for probabilistic modeling
/jax-numpyro-prob --agents=auto --intelligent --model-type=hierarchical --bayesian

# Scientific computing probabilistic modeling with specialized agents
/jax-numpyro-prob --agents=scientific --breakthrough --orchestrate --optimize

# AI/ML probabilistic systems with production focus
/jax-numpyro-prob --agents=ai --inference=svi --optimize --breakthrough

# Research-grade Bayesian development
/jax-numpyro-prob --agents=research --breakthrough --orchestrate --bayesian

# JAX-specialized probabilistic optimization
/jax-numpyro-prob --agents=jax --sampling=nuts --optimize --intelligent

# Complete 23-agent probabilistic ecosystem
/jax-numpyro-prob --agents=all --orchestrate --breakthrough --bayesian

# Neutron scattering Bayesian analysis
/jax-numpyro-prob neutron_data.py --agents=scientific --intelligent --breakthrough

# Hierarchical Bayesian modeling for multi-level data
/jax-numpyro-prob hierarchical_study.py --agents=research --orchestrate --bayesian

# Production uncertainty quantification
/jax-numpyro-prob production_model.py --agents=ai --optimize --intelligent

# Cross-domain stochastic modeling
/jax-numpyro-prob stochastic_system.py --agents=all --breakthrough --orchestrate

# Time series Bayesian analysis
/jax-numpyro-prob timeseries.py --agents=scientific --intelligent --optimize

# Advanced mixture model research
/jax-numpyro-prob mixture_research.py --agents=research --breakthrough --bayesian
```

### Intelligent Agent Selection Examples

```bash
# Model Type Detection → Intelligent Agent Selection

# Hierarchical Bayesian research model
/jax-numpyro-prob hierarchical_research.py --agents=auto --intelligent
# → Selects: research-intelligence-master + scientific-computing-master + correlation-function-expert

# Production uncertainty quantification
/jax-numpyro-prob production_uq.py --agents=auto --intelligent
# → Selects: ai-systems-architect + jax-pro + neural-networks-master

# Scientific domain-specific modeling
/jax-numpyro-prob neutron_analysis.py --agents=auto --intelligent
# → Selects: neutron-soft-matter-expert + scientific-computing-master + research-intelligence-master

# Large-scale Bayesian inference
/jax-numpyro-prob large_scale.py --agents=auto --intelligent
# → Selects: ai-systems-architect + jax-pro + systems-architect

# Complex stochastic process modeling
/jax-numpyro-prob stochastic_complex.py --agents=auto --intelligent
# → Selects: nonequilibrium-stochastic-expert + research-intelligence-master + multi-agent-orchestrator

# Comprehensive Bayesian research platform
/jax-numpyro-prob research_platform.py --agents=all --orchestrate --breakthrough
# → Activates: All 23 agents with intelligent coordination for maximum probabilistic modeling effectiveness
```

## Example output

```python
import jax
import jax.numpy as jnp
import jax.random as random
import numpyro
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS, SVI, Trace_ELBO, autoguide
from numpyro.diagnostics import hpdi, effective_sample_size, split_gelman_rubin
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, Any, Tuple, Optional

# Initialize PRNG key
key = random.PRNGKey(0)

# ============================================================================
# 1. BASIC PROBABILISTIC MODEL SETUP
# ============================================================================

def simple_linear_regression(x, y=None):
    """Simple Bayesian linear regression model"""

    # Priors
    alpha = numpyro.sample("alpha", dist.Normal(0.0, 10.0))
    beta = numpyro.sample("beta", dist.Normal(0.0, 10.0))
    sigma = numpyro.sample("sigma", dist.Exponential(1.0))

    # Linear model
    mu = alpha + beta * x

    # Likelihood
    with numpyro.plate("data", len(x)):
        numpyro.sample("obs", dist.Normal(mu, sigma), obs=y)

def generate_regression_data(n_samples=100, true_alpha=2.0, true_beta=1.5, noise_std=0.5):
    """Generate synthetic regression data"""

    key_x, key_noise = random.split(key)

    # Generate x data
    x = random.uniform(key_x, (n_samples,), minval=-2.0, maxval=2.0)

    # Generate y data with noise
    y_true = true_alpha + true_beta * x
    noise = noise_std * random.normal(key_noise, (n_samples,))
    y = y_true + noise

    return x, y, (true_alpha, true_beta, noise_std)

# ============================================================================
# 2. MCMC SAMPLING WITH NUTS
# ============================================================================

def run_mcmc_inference(model, model_args, num_warmup=1000, num_samples=2000,
                       num_chains=4, chain_method='parallel'):
    """Run MCMC inference using NUTS sampler"""

    # Initialize NUTS sampler
    nuts_kernel = NUTS(model)

    # Set up MCMC
    mcmc = MCMC(
        nuts_kernel,
        num_warmup=num_warmup,
        num_samples=num_samples,
        num_chains=num_chains,
        chain_method=chain_method
    )

    # Run MCMC
    mcmc_key = random.PRNGKey(42)
    mcmc.run(mcmc_key, *model_args)

    # Get samples
    samples = mcmc.get_samples()

    # Print summary
    mcmc.print_summary()

    return samples, mcmc

def analyze_mcmc_results(samples, mcmc):
    """Analyze MCMC sampling results"""

    print("\n=== MCMC Analysis ===")

    # Effective sample size
    for param_name, param_samples in samples.items():
        ess = effective_sample_size(param_samples)
        print(f"Effective sample size for {param_name}: {ess:.1f}")

    # R-hat (Gelman-Rubin diagnostic)
    for param_name, param_samples in samples.items():
        rhat = split_gelman_rubin(param_samples)
        print(f"R-hat for {param_name}: {rhat:.4f}")

    # Posterior quantiles
    print("\nPosterior quantiles:")
    for param_name, param_samples in samples.items():
        quantiles = jnp.percentile(param_samples, jnp.array([5, 50, 95]))
        print(f"{param_name}: 5%={quantiles[0]:.3f}, 50%={quantiles[1]:.3f}, 95%={quantiles[2]:.3f}")

    # HPDI (Highest Posterior Density Interval)
    print("\nHighest Posterior Density Intervals (90%):")
    for param_name, param_samples in samples.items():
        hpdi_interval = hpdi(param_samples, 0.9)
        print(f"{param_name}: [{hpdi_interval[0]:.3f}, {hpdi_interval[1]:.3f}]")

# ============================================================================
# 3. VARIATIONAL INFERENCE
# ============================================================================

def run_svi_inference(model, model_args, num_steps=5000, learning_rate=0.01):
    """Run variational inference using SVI"""

    # Set up automatic guide
    guide = autoguide.AutoNormal(model)

    # Set up optimizer
    optimizer = numpyro.optim.Adam(step_size=learning_rate)

    # Set up SVI
    svi = SVI(model, guide, optimizer, loss=Trace_ELBO())

    # Initialize SVI state
    svi_key = random.PRNGKey(42)
    svi_state = svi.init(svi_key, *model_args)

    # Training loop
    losses = []
    for step in range(num_steps):
        svi_state, loss = svi.update(svi_state, *model_args)
        losses.append(loss)

        if step % 500 == 0:
            print(f"Step {step}, Loss: {loss:.4f}")

    # Get posterior samples
    params = svi.get_params(svi_state)
    posterior_samples = guide.sample_posterior(
        random.PRNGKey(123), params, sample_shape=(2000,)
    )

    return posterior_samples, guide, params, losses

def compare_inference_methods(model, model_args):
    """Compare MCMC and SVI inference methods"""

    print("=== Comparing Inference Methods ===")

    # MCMC inference
    print("\nRunning MCMC...")
    mcmc_samples, mcmc_obj = run_mcmc_inference(model, model_args, num_samples=1000)

    # SVI inference
    print("\nRunning SVI...")
    svi_samples, guide, params, losses = run_svi_inference(model, model_args)

    # Compare posterior means
    print("\nPosterior means comparison:")
    for param_name in mcmc_samples.keys():
        mcmc_mean = jnp.mean(mcmc_samples[param_name])
        svi_mean = jnp.mean(svi_samples[param_name])
        print(f"{param_name}: MCMC={mcmc_mean:.3f}, SVI={svi_mean:.3f}")

    return mcmc_samples, svi_samples

# ============================================================================
# 4. BAYESIAN CLASSIFICATION
# ============================================================================

def logistic_regression(x, y=None):
    """Bayesian logistic regression model"""

    n_features = x.shape[1]

    # Priors for weights
    w = numpyro.sample("w", dist.Normal(0.0, 1.0).expand([n_features]))
    b = numpyro.sample("b", dist.Normal(0.0, 1.0))

    # Logistic regression
    logits = jnp.dot(x, w) + b

    # Likelihood
    with numpyro.plate("data", len(x)):
        numpyro.sample("obs", dist.Bernoulli(logits=logits), obs=y)

def generate_classification_data(n_samples=200, n_features=2):
    """Generate synthetic classification data"""

    key_x, key_w, key_noise = random.split(key, 3)

    # Generate features
    x = random.normal(key_x, (n_samples, n_features))

    # True weights
    true_w = random.normal(key_w, (n_features,))
    true_b = 0.5

    # Generate labels with noise
    logits = jnp.dot(x, true_w) + true_b
    probs = jax.nn.sigmoid(logits)
    y = random.bernoulli(key_noise, probs).astype(int)

    return x, y, (true_w, true_b)

def predict_classification(samples, x_new):
    """Make predictions using posterior samples"""

    w_samples = samples["w"]
    b_samples = samples["b"]

    # Compute predictions for each posterior sample
    predictions = []
    for i in range(len(w_samples)):
        logits = jnp.dot(x_new, w_samples[i]) + b_samples[i]
        probs = jax.nn.sigmoid(logits)
        predictions.append(probs)

    predictions = jnp.array(predictions)

    # Compute posterior predictive statistics
    mean_probs = jnp.mean(predictions, axis=0)
    std_probs = jnp.std(predictions, axis=0)

    return mean_probs, std_probs

# ============================================================================
# 5. HIERARCHICAL MODELS
# ============================================================================

def hierarchical_linear_model(group_idx, x, y=None):
    """Hierarchical Bayesian linear regression"""

    n_groups = len(jnp.unique(group_idx))

    # Hyperpriors
    mu_alpha = numpyro.sample("mu_alpha", dist.Normal(0.0, 10.0))
    sigma_alpha = numpyro.sample("sigma_alpha", dist.Exponential(1.0))

    mu_beta = numpyro.sample("mu_beta", dist.Normal(0.0, 10.0))
    sigma_beta = numpyro.sample("sigma_beta", dist.Exponential(1.0))

    # Group-level parameters
    with numpyro.plate("groups", n_groups):
        alpha = numpyro.sample("alpha", dist.Normal(mu_alpha, sigma_alpha))
        beta = numpyro.sample("beta", dist.Normal(mu_beta, sigma_beta))

    # Observation noise
    sigma = numpyro.sample("sigma", dist.Exponential(1.0))

    # Linear model
    mu = alpha[group_idx] + beta[group_idx] * x

    # Likelihood
    with numpyro.plate("data", len(x)):
        numpyro.sample("obs", dist.Normal(mu, sigma), obs=y)

def generate_hierarchical_data(n_groups=5, n_per_group=20):
    """Generate hierarchical regression data"""

    key_alpha, key_beta, key_x, key_noise = random.split(key, 4)

    # Group-level parameters
    mu_alpha, sigma_alpha = 2.0, 0.5
    mu_beta, sigma_beta = 1.0, 0.3

    alpha_true = mu_alpha + sigma_alpha * random.normal(key_alpha, (n_groups,))
    beta_true = mu_beta + sigma_beta * random.normal(key_beta, (n_groups,))

    # Generate data
    x_all = []
    y_all = []
    group_idx_all = []

    for g in range(n_groups):
        x_g = random.uniform(random.split(key_x, n_groups)[g], (n_per_group,), minval=-2, maxval=2)
        y_true_g = alpha_true[g] + beta_true[g] * x_g
        noise_g = 0.2 * random.normal(random.split(key_noise, n_groups)[g], (n_per_group,))
        y_g = y_true_g + noise_g

        x_all.append(x_g)
        y_all.append(y_g)
        group_idx_all.append(jnp.full(n_per_group, g))

    x = jnp.concatenate(x_all)
    y = jnp.concatenate(y_all)
    group_idx = jnp.concatenate(group_idx_all)

    return x, y, group_idx, (alpha_true, beta_true)

# ============================================================================
# 6. MODEL COMPARISON AND SELECTION
# ============================================================================

def compute_waic(model, model_args, samples):
    """Compute WAIC (Widely Applicable Information Criterion)"""

    from numpyro.diagnostics import waic

    # Compute WAIC
    waic_result = waic(
        random.PRNGKey(42),
        model,
        samples,
        *model_args
    )

    print(f"WAIC: {waic_result.waic:.2f}")
    print(f"Effective number of parameters: {waic_result.p_waic:.2f}")

    return waic_result

def posterior_predictive_check(model, samples, x_obs, y_obs):
    """Perform posterior predictive check"""

    from numpyro.infer import Predictive

    # Generate posterior predictions
    predictive = Predictive(model, samples)
    key_pred = random.PRNGKey(42)
    predictions = predictive(key_pred, x_obs)

    # Get predicted observations
    y_pred = predictions["obs"]

    # Compute test statistics
    def test_statistic(y):
        return jnp.mean(y)

    # Observed test statistic
    t_obs = test_statistic(y_obs)

    # Predicted test statistics
    t_pred = jnp.array([test_statistic(y_pred[i]) for i in range(len(y_pred))])

    # Bayesian p-value
    p_value = jnp.mean(t_pred >= t_obs)

    print(f"Posterior predictive check:")
    print(f"  Observed test statistic: {t_obs:.3f}")
    print(f"  Predicted test statistic: {jnp.mean(t_pred):.3f} ± {jnp.std(t_pred):.3f}")
    print(f"  Bayesian p-value: {p_value:.3f}")

    return t_obs, t_pred, p_value

# ============================================================================
# 7. ADVANCED DISTRIBUTIONS AND MODELS
# ============================================================================

def mixture_model(x, y=None, n_components=2):
    """Gaussian mixture model"""

    # Mixture weights
    weights = numpyro.sample("weights", dist.Dirichlet(jnp.ones(n_components)))

    # Component parameters
    with numpyro.plate("components", n_components):
        locs = numpyro.sample("locs", dist.Normal(0.0, 5.0))
        scales = numpyro.sample("scales", dist.Exponential(1.0))

    # Mixture distribution
    mixture = dist.MixtureSameFamily(
        dist.Categorical(weights),
        dist.Normal(locs, scales)
    )

    # Likelihood
    with numpyro.plate("data", len(x)):
        numpyro.sample("obs", mixture, obs=y)

def robust_regression(x, y=None):
    """Robust regression using Student's t-distribution"""

    # Priors
    alpha = numpyro.sample("alpha", dist.Normal(0.0, 10.0))
    beta = numpyro.sample("beta", dist.Normal(0.0, 10.0))
    sigma = numpyro.sample("sigma", dist.Exponential(1.0))
    nu = numpyro.sample("nu", dist.Exponential(1.0/30.0))  # degrees of freedom

    # Linear model
    mu = alpha + beta * x

    # Robust likelihood (Student's t)
    with numpyro.plate("data", len(x)):
        numpyro.sample("obs", dist.StudentT(nu, mu, sigma), obs=y)

def time_series_model(y=None):
    """Simple AR(1) time series model"""

    n_obs = len(y) if y is not None else 100

    # Priors
    phi = numpyro.sample("phi", dist.Uniform(-1.0, 1.0))  # AR coefficient
    sigma = numpyro.sample("sigma", dist.Exponential(1.0))
    y0 = numpyro.sample("y0", dist.Normal(0.0, 1.0))

    # AR(1) process
    def transition_fn(y_prev, _):
        y_curr = numpyro.sample("y", dist.Normal(phi * y_prev, sigma))
        return y_curr, y_curr

    # Scan over time
    _, ys = numpyro.contrib.control_flow.scan(
        transition_fn, y0, jnp.arange(n_obs-1)
    )

    # Combine initial and subsequent observations
    y_pred = jnp.concatenate([jnp.array([y0]), ys])

    # Observation model
    with numpyro.plate("data", n_obs):
        numpyro.sample("obs", dist.Normal(y_pred, 0.1), obs=y)

# ============================================================================
# 8. COMPREHENSIVE EXAMPLES
# ============================================================================

def run_probabilistic_examples():
    """Run comprehensive probabilistic modeling examples"""

    print("=== JAX NumPyro Probabilistic Programming Examples ===")

    # Example 1: Bayesian linear regression
    print("\n1. Bayesian Linear Regression:")
    x_reg, y_reg, true_params_reg = generate_regression_data(n_samples=100)

    # Run MCMC
    samples_reg, mcmc_reg = run_mcmc_inference(
        simple_linear_regression, (x_reg, y_reg), num_samples=1000
    )

    # Analyze results
    analyze_mcmc_results(samples_reg, mcmc_reg)

    print(f"\nTrue parameters: alpha={true_params_reg[0]:.3f}, beta={true_params_reg[1]:.3f}, sigma={true_params_reg[2]:.3f}")

    # Example 2: Bayesian classification
    print("\n2. Bayesian Logistic Regression:")
    x_class, y_class, true_params_class = generate_classification_data(n_samples=200)

    samples_class, mcmc_class = run_mcmc_inference(
        logistic_regression, (x_class, y_class), num_samples=1000
    )

    # Make predictions
    x_new = jnp.array([[0.0, 0.0], [1.0, 1.0], [-1.0, -1.0]])
    mean_probs, std_probs = predict_classification(samples_class, x_new)

    print("Predictions for new data:")
    for i, (mean_p, std_p) in enumerate(zip(mean_probs, std_probs)):
        print(f"  Point {i}: P(y=1) = {mean_p:.3f} ± {std_p:.3f}")

    # Example 3: Hierarchical model
    print("\n3. Hierarchical Linear Regression:")
    x_hier, y_hier, group_idx_hier, true_params_hier = generate_hierarchical_data()

    samples_hier, mcmc_hier = run_mcmc_inference(
        hierarchical_linear_model, (group_idx_hier, x_hier, y_hier), num_samples=1000
    )

    print("Group-level parameters:")
    alpha_post = samples_hier["alpha"]
    beta_post = samples_hier["beta"]

    for g in range(len(true_params_hier[0])):
        alpha_mean = jnp.mean(alpha_post[:, g])
        beta_mean = jnp.mean(beta_post[:, g])
        print(f"  Group {g}: alpha={alpha_mean:.3f} (true: {true_params_hier[0][g]:.3f}), "
              f"beta={beta_mean:.3f} (true: {true_params_hier[1][g]:.3f})")

    # Example 4: Model comparison
    print("\n4. Inference Method Comparison:")
    mcmc_samples, svi_samples = compare_inference_methods(
        simple_linear_regression, (x_reg, y_reg)
    )

    # Example 5: Model diagnostics
    print("\n5. Model Diagnostics:")
    waic_result = compute_waic(simple_linear_regression, (x_reg, y_reg), samples_reg)

    # Posterior predictive check
    t_obs, t_pred, p_value = posterior_predictive_check(
        simple_linear_regression, samples_reg, x_reg, y_reg
    )

# Run examples
run_probabilistic_examples()
```

## Probabilistic Programming Best Practices

### Model Design
- **Clear priors**: Choose informative but not overly restrictive priors
- **Model checking**: Always perform posterior predictive checks
- **Hierarchical structure**: Use hierarchical models for grouped data
- **Robust modeling**: Consider heavy-tailed distributions for outliers

### Inference Strategy
- **MCMC for accuracy**: Use NUTS for high-dimensional problems
- **SVI for speed**: Use variational inference for large datasets
- **Multiple chains**: Run multiple MCMC chains for diagnostics
- **Convergence checking**: Monitor R-hat and effective sample size

### Model Comparison
- **Information criteria**: Use WAIC or LOO for model selection
- **Cross-validation**: Implement proper cross-validation schemes
- **Posterior predictive**: Check model fit with test statistics
- **Domain knowledge**: Incorporate scientific understanding

### Performance Optimization
- **JIT compilation**: Use JAX transformations for speed
- **Vectorization**: Leverage NumPyro's plate primitive
- **Gradient-based sampling**: Prefer NUTS over random walk samplers
- **Automatic guides**: Use AutoNormal and AutoDelta for SVI

## Common Distributions and Use Cases

### Basic Distributions
- **Normal**: Continuous data, linear regression
- **Bernoulli/Categorical**: Classification, discrete choices
- **Exponential**: Positive continuous data, scale parameters
- **Beta**: Probabilities and proportions

### Advanced Distributions
- **StudentT**: Robust modeling with heavy tails
- **Dirichlet**: Multinomial probabilities, mixture weights
- **LKJ**: Correlation matrices in multivariate models
- **GammaPoisson**: Count data with overdispersion

### Specialized Models
- **Mixture models**: Clustering and heterogeneous populations
- **Time series**: AR, VAR, state space models
- **Hierarchical**: Multi-level data structures
- **Survival analysis**: Censored data and hazard functions

## Agent-Enhanced Probabilistic Integration Patterns

### Complete Probabilistic Modeling Workflow
```bash
# Intelligent probabilistic analysis and inference pipeline
/jax-numpyro-prob --agents=auto --intelligent --model-type=hierarchical --bayesian
/jax-performance --agents=jax --technique=profiling --optimization
/jax-training --agents=scientific --uncertainty-quantification
```

### Scientific Computing Probabilistic Pipeline
```bash
# High-performance scientific probabilistic modeling
/jax-numpyro-prob --agents=scientific --breakthrough --orchestrate
/jax-essentials --agents=scientific --operation=grad --probabilistic
/run-all-tests --agents=scientific --reproducible --bayesian
```

### Production Bayesian System Infrastructure
```bash
# Large-scale production probabilistic systems
/jax-numpyro-prob --agents=ai --optimize --inference=svi --breakthrough
/jax-performance --agents=ai --gpu-accel --optimization
/ci-setup --agents=ai --bayesian-monitoring --uncertainty
```

## Related Commands

**Prerequisites**: Commands to run before probabilistic modeling
- `/jax-essentials --agents=auto` - Core JAX operations with probabilistic considerations
- `/jax-init --agents=scientific` - JAX project setup with Bayesian configuration

**Core Workflow**: Probabilistic development with agent intelligence
- `/jax-performance --agents=jax` - Probabilistic performance optimization
- `/jax-training --agents=scientific` - Training with uncertainty quantification
- `/jax-debug --agents=auto` - Debug probabilistic inference issues

**Advanced Integration**: Specialized probabilistic development
- `/jax-models --agents=scientific` - Probabilistic neural network architectures
- `/jax-data-load --agents=scientific` - Bayesian data processing pipelines
- `/jax-sparse-ops --agents=scientific` - Sparse Bayesian methods

**Quality Assurance**: Probabilistic validation and testing
- `/generate-tests --agents=auto --type=bayesian` - Generate probabilistic model tests
- `/run-all-tests --agents=scientific --reproducible` - Comprehensive Bayesian testing
- `/check-code-quality --agents=research --bayesian` - Research-grade code quality

**Research & Documentation**: Advanced probabilistic workflows
- `/update-docs --agents=research --type=bayesian` - Research-grade probabilistic documentation
- `/reflection --agents=research --type=bayesian` - Probabilistic methodology analysis
- `/multi-agent-optimize --agents=all --focus=bayesian` - Comprehensive probabilistic optimization