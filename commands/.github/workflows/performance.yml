name: Performance Monitoring

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install pytest-benchmark memory-profiler py-spy

      - name: Run performance benchmarks
        run: |
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=benchmark.json \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=3 \
            --benchmark-min-rounds=5

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: ${{ github.event_name == 'push' }}
          comment-on-alert: true
          fail-on-alert: true
          alert-threshold: '150%'
          comment-always: false

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark.json
          retention-days: 90

  memory-profile:
    name: Memory Profiling
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install memory-profiler matplotlib

      - name: Run memory profiling
        run: |
          python -m memory_profiler tests/benchmarks/memory_test.py > memory_profile.txt

      - name: Generate memory report
        run: |
          python cicd/monitoring/performance_monitor.py \
            --type memory \
            --input memory_profile.txt \
            --output memory_report.json

      - name: Upload memory profile
        uses: actions/upload-artifact@v3
        with:
          name: memory-profile
          path: |
            memory_profile.txt
            memory_report.json
          retention-days: 30

  cpu-profile:
    name: CPU Profiling
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install py-spy flamegraph

      - name: Run CPU profiling
        run: |
          py-spy record -o profile.svg -- python tests/benchmarks/cpu_test.py || true
          py-spy record -o profile.json --format speedscope -- python tests/benchmarks/cpu_test.py || true

      - name: Upload CPU profile
        uses: actions/upload-artifact@v3
        with:
          name: cpu-profile
          path: |
            profile.svg
            profile.json
          retention-days: 30

  load-test:
    name: Load Testing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install locust

      - name: Run load tests
        run: |
          python tests/benchmarks/load_test.py \
            --users 100 \
            --spawn-rate 10 \
            --run-time 60s \
            --json-report load_test.json

      - name: Analyze load test results
        run: |
          python cicd/monitoring/performance_monitor.py \
            --type load \
            --input load_test.json \
            --output load_report.json

      - name: Upload load test results
        uses: actions/upload-artifact@v3
        with:
          name: load-test-results
          path: |
            load_test.json
            load_report.json
          retention-days: 30

  performance-gate:
    name: Performance Gate
    runs-on: ubuntu-latest
    needs: [benchmark, memory-profile, cpu-profile, load-test]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Download all performance results
        uses: actions/download-artifact@v3

      - name: Run performance gate checks
        run: |
          python cicd/quality/performance_gate.py \
            --benchmark benchmark-results/benchmark.json \
            --memory memory-profile/memory_report.json \
            --load load-test-results/load_report.json \
            --thresholds cicd/quality/performance_thresholds.yaml

      - name: Generate performance report
        run: |
          echo "# Performance Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Results" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmarks: ${{ needs.benchmark.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Memory Profile: ${{ needs.memory-profile.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- CPU Profile: ${{ needs.cpu-profile.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Load Tests: ${{ needs.load-test.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Performance gate: PASSED"

  compare-baseline:
    name: Compare Against Baseline
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: [benchmark]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Download current benchmark
        uses: actions/download-artifact@v3
        with:
          name: benchmark-results
          path: current/

      - name: Checkout main branch
        run: |
          git fetch origin main
          git checkout origin/main

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install pytest-benchmark

      - name: Run baseline benchmarks
        run: |
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=baseline.json

      - name: Compare benchmarks
        run: |
          python cicd/monitoring/performance_monitor.py \
            --type compare \
            --baseline baseline.json \
            --current current/benchmark.json \
            --output comparison.json

      - name: Comment on PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comparison = JSON.parse(fs.readFileSync('comparison.json', 'utf8'));

            let comment = '## Performance Comparison\n\n';
            comment += '| Benchmark | Baseline | Current | Change |\n';
            comment += '|-----------|----------|---------|--------|\n';

            for (const [name, data] of Object.entries(comparison.benchmarks)) {
              const change = ((data.current - data.baseline) / data.baseline * 100).toFixed(2);
              const emoji = change > 5 ? 'ğŸ”´' : change < -5 ? 'ğŸŸ¢' : 'âšª';
              comment += `| ${name} | ${data.baseline.toFixed(3)}s | ${data.current.toFixed(3)}s | ${emoji} ${change}% |\n`;
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });