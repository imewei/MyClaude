# Phase 4: Future Enhancements - Implementation Roadmap

**Status**: üöß In Progress
**Version**: 4.0.0-dev
**Timeline**: 28-40 weeks (estimated)
**Started**: 2025-09-30

---

## Executive Summary

Phase 4 transforms the nonequilibrium physics agent system from a CPU-based research tool (77.6% test pass rate) into a production-grade, GPU-accelerated HPC platform with:

- **50-100x performance improvement** via GPU acceleration
- **Advanced numerical solvers** (Magnus expansion, PMP)
- **ML-enhanced optimal control** (neural network policies)
- **Interactive visualization** dashboards
- **HPC cluster integration** for distributed execution
- **95%+ test coverage** with comprehensive validation

**Current State**: Phase 3 complete (16 agents, 77.6% tests passing)
**Target State**: Production HPC platform with ML intelligence layer

---

## Enhancement Overview

| Enhancement | Priority | Complexity | Impact | Weeks | Dependencies |
|-------------|----------|------------|--------|-------|--------------|
| **1. GPU Acceleration** | P0 | High | Very High | 8-10 | JAX, CuPy, CUDA |
| **2. Advanced Solvers** | P0 | High | High | 6-8 | GPU foundation |
| **3. ML Integration** | P1 | Very High | High | 10-12 | GPU, Advanced solvers |
| **4. Visualization** | P1 | Medium | Medium | 4-6 | None |
| **5. HPC Integration** | P1 | High | High | 8-10 | GPU, Distributed |
| **6. Test Coverage** | P0 | Medium | Critical | 6-8 | None |

**Total Estimated Effort**: 42-54 weeks with parallelization ‚Üí 28-40 weeks calendar time

---

## 1. GPU Acceleration üöÄ

### Objective
Enable GPU-accelerated computation for quantum evolution and molecular dynamics, achieving 50-100x speedup for large-scale simulations.

### Components

#### 1.1 Quantum Evolution GPU Kernels
**Target Performance**:
- n_dim=10: < 1 second (current: ~30 sec) ‚Üí **30x speedup**
- n_dim=20: < 10 seconds (current: intractable) ‚Üí **NEW capability**
- Batch 1000 trajectories: < 5 minutes

**Implementation**:

```python
# gpu_kernels/quantum_evolution.py
import jax
import jax.numpy as jnp
from jax.experimental.ode import odeint
import diffrax

@jax.jit
def lindblad_rhs_jax(rho_vec, t, H, L_ops, gammas, hbar):
    """JAX-compiled Lindblad RHS for GPU execution.

    Args:
        rho_vec: Flattened density matrix (n_dim¬≤,)
        t: Time
        H: Hamiltonian (n_dim, n_dim)
        L_ops: Jump operators [(n_dim, n_dim), ...]
        gammas: Decay rates [float, ...]
        hbar: Reduced Planck constant

    Returns:
        drho_dt: Time derivative (n_dim¬≤,)
    """
    n_dim = int(jnp.sqrt(len(rho_vec)))
    rho = rho_vec.reshape((n_dim, n_dim))

    # Unitary evolution: -i/‚Ñè [H, œÅ]
    drho_dt = -1j / hbar * (H @ rho - rho @ H)

    # Dissipative evolution: Œ£_k Œ≥_k D[L_k]œÅ
    for L, gamma in zip(L_ops, gammas):
        L_dag = jnp.conj(L.T)
        drho_dt += gamma * (
            L @ rho @ L_dag - 0.5 * (L_dag @ L @ rho + rho @ L_dag @ L)
        )

    return drho_dt.flatten()

@jax.jit
def solve_lindblad_jax(rho0, H, L_ops, gammas, t_span, hbar=1.054571817e-34):
    """Solve Lindblad equation on GPU using JAX.

    Uses diffrax for high-performance ODE integration on GPU.
    """
    term = diffrax.ODETerm(
        lambda t, rho, args: lindblad_rhs_jax(rho, t, H, L_ops, gammas, hbar)
    )
    solver = diffrax.Dopri5()
    saveat = diffrax.SaveAt(ts=t_span)

    solution = diffrax.diffeqsolve(
        term,
        solver,
        t0=t_span[0],
        t1=t_span[-1],
        dt0=None,
        y0=rho0.flatten(),
        saveat=saveat
    )

    return solution.ys

# Batched evolution for multiple initial conditions
@jax.jit
@jax.vmap
def batch_lindblad_evolution(rho0_batch, H, L_ops, gammas, t_span, hbar):
    """Vectorized Lindblad evolution for batch of initial states."""
    return solve_lindblad_jax(rho0_batch, H, L_ops, gammas, t_span, hbar)
```

**CUDA Kernels** (for maximum performance):
```cuda
// gpu_kernels/cuda_quantum.cu
__global__ void dissipator_kernel(
    cuDoubleComplex* rho,
    const cuDoubleComplex* L,
    const double gamma,
    const int n_dim
) {
    // Compute D[L]œÅ = L œÅ L‚Ä† - ¬Ω{L‚Ä†L, œÅ}
    // Optimized for GPU memory coalescing
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    // ... CUDA implementation
}
```

#### 1.2 Molecular Dynamics GPU Acceleration
**Integration with HOOMD-blue**:

```python
# simulation/gpu_md.py
import hoomd

class GPUMolecularDynamics:
    """GPU-accelerated MD simulations via HOOMD-blue."""

    def __init__(self, device='GPU'):
        self.device = hoomd.device.GPU() if device == 'GPU' else hoomd.device.CPU()
        self.simulation = hoomd.Simulation(device=self.device)

    def run_nemd_shear(self, system, shear_rate, duration):
        """NEMD simulation with shear flow on GPU.

        Performance target: 10x speedup vs LAMMPS CPU
        """
        # Set up shear flow
        # Run on GPU
        # Return trajectories
```

### Files to Create/Modify

**New Files**:
- `gpu_kernels/__init__.py`
- `gpu_kernels/quantum_evolution.py` (JAX implementation)
- `gpu_kernels/cuda_quantum.cu` (CUDA kernels)
- `gpu_kernels/cuda_quantum.py` (Python wrappers)
- `simulation/gpu_md.py` (HOOMD-blue integration)
- `tests/gpu/test_quantum_gpu.py`
- `tests/gpu/test_md_gpu.py`

**Modified Files**:
- `nonequilibrium_quantum_agent.py`: Add `backend='jax'` or `backend='cuda'` option
- `simulation_agent.py`: Add GPU MD backend
- `requirements.txt`: Add `jax>=0.4.0`, `jaxlib>=0.4.0`, `diffrax>=0.4.0`, `cupy>=12.0.0`

### Success Metrics
- ‚úÖ n_dim=10 Lindblad: < 1 sec (30x speedup achieved)
- ‚úÖ n_dim=20 Lindblad: < 10 sec (new capability)
- ‚úÖ GPU utilization > 80%
- ‚úÖ All quantum tests pass on GPU backend
- ‚úÖ Numerical agreement with CPU: < 1e-10 relative error

---

## 2. Advanced Solvers üßÆ

### Objective
Implement state-of-the-art numerical solvers that outperform standard methods in accuracy and efficiency.

### Components

#### 2.1 Magnus Expansion for Lindblad Equation

**Theory**: The Magnus expansion provides high-order integrators for time-dependent Hamiltonians while preserving complete positivity.

**Formula**:
```
Œ©(t) = Œ©‚ÇÅ(t) + Œ©‚ÇÇ(t) + Œ©‚ÇÉ(t) + ...
U(t) = exp(Œ©(t))

where:
Œ©‚ÇÅ(t) = ‚à´‚ÇÄ·µó A(s) ds
Œ©‚ÇÇ(t) = ¬Ω ‚à´‚ÇÄ·µó ‚à´‚ÇÄÀ¢ [A(s), A(s')] ds' ds
```

**Implementation**:

```python
# solvers/magnus_expansion.py
import numpy as np
from scipy.linalg import expm

class MagnusExpansionSolver:
    """4th order Magnus expansion for Lindblad equation.

    Advantages over RK4:
    - Preserves complete positivity exactly
    - Better energy conservation
    - Faster convergence for driven systems
    """

    def __init__(self, order=4):
        self.order = order

    def solve(self, rho0, H_protocol, L_ops, gammas, t_span, n_steps):
        """Solve Lindblad equation with time-dependent Hamiltonian.

        Args:
            rho0: Initial density matrix
            H_protocol: List of Hamiltonians [H(t‚ÇÄ), H(t‚ÇÅ), ..., H(t‚Çô)]
            L_ops: Jump operators
            gammas: Decay rates
            t_span: Time grid
            n_steps: Number of steps

        Returns:
            rho_evolution: Density matrix at each time point
        """
        rho_evolution = [rho0]
        dt = (t_span[-1] - t_span[0]) / n_steps

        for i in range(n_steps):
            # Compute Magnus terms
            Omega_1 = self._magnus_order1(H_protocol[i], H_protocol[i+1], dt)
            Omega_2 = self._magnus_order2(H_protocol[i], H_protocol[i+1], dt)

            if self.order >= 4:
                Omega_3 = self._magnus_order3(H_protocol[i], H_protocol[i+1], dt)
                Omega_4 = self._magnus_order4(H_protocol[i], H_protocol[i+1], dt)
                Omega = Omega_1 + Omega_2 + Omega_3 + Omega_4
            else:
                Omega = Omega_1 + Omega_2

            # Propagator: U = exp(Œ©)
            U = expm(Omega)

            # Apply to density matrix: œÅ(t+dt) = U œÅ(t) U‚Ä†
            rho_new = U @ rho_evolution[-1] @ U.conj().T

            # Add dissipation (split-operator method)
            rho_new = self._apply_dissipation(rho_new, L_ops, gammas, dt)

            rho_evolution.append(rho_new)

        return np.array(rho_evolution)

    def _magnus_order1(self, H_i, H_f, dt):
        """First order Magnus: Œ©‚ÇÅ = ‚à´ H(t) dt ‚âà (H_i + H_f)/2 * dt"""
        return 0.5 * (H_i + H_f) * dt

    def _magnus_order2(self, H_i, H_f, dt):
        """Second order: Œ©‚ÇÇ = ¬Ω ‚à´‚à´ [H(t), H(t')] dt' dt"""
        commutator = H_f @ H_i - H_i @ H_f
        return (dt**2 / 12) * commutator

    # ... order 3 and 4 implementations
```

**Integration into Quantum Agent**:
```python
# In nonequilibrium_quantum_agent.py
def _lindblad_master_equation(self, input_data):
    # ...
    solver = input_data.get('parameters', {}).get('solver', 'RK45')

    if solver == 'magnus':
        from solvers.magnus_expansion import MagnusExpansionSolver
        magnus = MagnusExpansionSolver(order=4)
        rho_evolution = magnus.solve(rho0, H_protocol, L_ops, gammas, t_eval, n_steps)
    elif solver == 'RK45':
        # Existing implementation
    # ...
```

#### 2.2 Pontryagin Maximum Principle (PMP) Solver

**Theory**: Solve optimal control problems via necessary conditions for optimality.

**Problem Formulation**:
```
Minimize: J = ‚à´‚ÇÄ·µÄ L(x, u, t) dt
Subject to: dx/dt = f(x, u, t)
            x(0) = x‚ÇÄ (given)
            x(T) = x_T (optional)
```

**PMP Necessary Conditions**:
```
1. Hamiltonian: H(x, u, p, t) = L(x, u, t) + p ¬∑ f(x, u, t)
2. Costate: dp/dt = -‚àÇH/‚àÇx
3. Optimality: ‚àÇH/‚àÇu = 0 ‚Üí u*(t) = argmin_u H(x, u, p, t)
4. Boundary conditions: p(T) = -‚àÇœÜ/‚àÇx|_(x(T))
```

**Implementation**:

```python
# solvers/pontryagin_solver.py
import numpy as np
from scipy.integrate import solve_bvp
from scipy.optimize import minimize

class PontryaginSolver:
    """Solve optimal control via Pontryagin Maximum Principle.

    Uses shooting method with BVP solver for costate equations.
    """

    def __init__(self, state_cost_fn, control_cost_fn, dynamics_fn):
        """
        Args:
            state_cost_fn: L_state(x) - cost on state
            control_cost_fn: L_control(u) - cost on control
            dynamics_fn: f(x, u) - state dynamics dx/dt = f(x, u)
        """
        self.L_state = state_cost_fn
        self.L_control = control_cost_fn
        self.f = dynamics_fn

    def solve(self, x_initial, x_target, duration, n_steps=100):
        """Solve for optimal control trajectory.

        Returns:
            x_optimal: Optimal state trajectory
            u_optimal: Optimal control trajectory
            cost: Total cost J
        """
        t_grid = np.linspace(0, duration, n_steps)

        # Define augmented system: [x, p] dynamics
        def augmented_dynamics(t, y):
            n = len(y) // 2
            x = y[:n]
            p = y[n:]

            # Optimal control from Hamiltonian minimization
            u_opt = self._compute_optimal_control(x, p)

            # State dynamics: dx/dt = f(x, u)
            dx_dt = self.f(x, u_opt)

            # Costate dynamics: dp/dt = -‚àÇH/‚àÇx
            dp_dt = -self._hamiltonian_gradient_x(x, u_opt, p)

            return np.concatenate([dx_dt, dp_dt])

        # Boundary conditions
        def boundary_conditions(y_left, y_right):
            n = len(y_left) // 2
            # x(0) = x_initial
            bc_initial = y_left[:n] - x_initial
            # x(T) = x_target
            bc_final = y_right[:n] - x_target
            return np.concatenate([bc_initial, bc_final])

        # Initial guess (linear interpolation)
        n_state = len(x_initial)
        y_guess = np.zeros((2 * n_state, n_steps))
        y_guess[:n_state] = np.linspace(x_initial, x_target, n_steps).T

        # Solve BVP
        sol = solve_bvp(
            augmented_dynamics,
            boundary_conditions,
            t_grid,
            y_guess
        )

        # Extract optimal trajectory
        x_optimal = sol.y[:n_state].T
        p_optimal = sol.y[n_state:].T
        u_optimal = np.array([
            self._compute_optimal_control(x, p)
            for x, p in zip(x_optimal, p_optimal)
        ])

        # Compute total cost
        cost = self._compute_cost(x_optimal, u_optimal, t_grid)

        return {
            'x_optimal': x_optimal,
            'u_optimal': u_optimal,
            'p_costate': p_optimal,
            'cost': cost,
            'time_grid': t_grid
        }

    def _compute_optimal_control(self, x, p):
        """Solve ‚àÇH/‚àÇu = 0 for optimal control.

        For quadratic control cost: u* = -R‚Åª¬π B^T p
        For general case: numerical optimization
        """
        # Simplified: assume LQR structure
        # u* = argmin_u [L_control(u) + p ¬∑ f(x, u)]

        def objective(u):
            return self.L_control(u) + np.dot(p, self.f(x, u))

        result = minimize(objective, x0=np.zeros_like(x), method='BFGS')
        return result.x

    def _hamiltonian_gradient_x(self, x, u, p):
        """Compute ‚àÇH/‚àÇx using finite differences."""
        eps = 1e-8
        grad = np.zeros_like(x)

        for i in range(len(x)):
            x_plus = x.copy()
            x_plus[i] += eps

            H_plus = self.L_state(x_plus) + self.L_control(u) + np.dot(p, self.f(x_plus, u))
            H_minus = self.L_state(x) + self.L_control(u) + np.dot(p, self.f(x, u))

            grad[i] = (H_plus - H_minus) / eps

        return grad

    def _compute_cost(self, x_trajectory, u_trajectory, t_grid):
        """Compute total cost via trapezoidal integration."""
        dt = t_grid[1] - t_grid[0]
        integrand = np.array([
            self.L_state(x) + self.L_control(u)
            for x, u in zip(x_trajectory, u_trajectory)
        ])
        return np.trapz(integrand, dx=dt)
```

**Integration into Optimal Control Agent**:
```python
# In optimal_control_agent.py
def _stochastic_optimal_control(self, input_data):
    # ...
    solver = input_data.get('parameters', {}).get('solver', 'LQR')

    if solver == 'pontryagin':
        from solvers.pontryagin_solver import PontryaginSolver

        # Define cost functions
        Q = params.get('state_cost', 1.0)
        R = params.get('control_cost', 0.1)
        L_state = lambda x: 0.5 * Q * np.sum(x**2)
        L_control = lambda u: 0.5 * R * np.sum(u**2)
        dynamics = lambda x, u: u  # dx/dt = u (simple integrator)

        pmp = PontryaginSolver(L_state, L_control, dynamics)
        result = pmp.solve(x_initial, x_target, duration)

        return {
            'protocol_type': 'pontryagin_optimal_control',
            'x_optimal': result['x_optimal'].tolist(),
            'u_optimal': result['u_optimal'].tolist(),
            'p_costate': result['p_costate'].tolist(),
            'total_cost': float(result['cost']),
            # ...
        }
```

### Files to Create/Modify

**New Files**:
- `solvers/__init__.py`
- `solvers/magnus_expansion.py`
- `solvers/pontryagin_solver.py`
- `solvers/collocation_methods.py` (alternative to shooting)
- `tests/solvers/test_magnus.py`
- `tests/solvers/test_pontryagin.py`

**Modified Files**:
- `nonequilibrium_quantum_agent.py`: Add Magnus solver option
- `optimal_control_agent.py`: Add PMP solver
- `requirements.txt`: Update scipy>=1.11.0 (for improved BVP solver)

### Success Metrics
- ‚úÖ Magnus expansion: 10x better energy conservation than RK4
- ‚úÖ PMP solver: 5x reduction in optimal cost vs LQR
- ‚úÖ Benchmark suite: All canonical problems solved correctly
- ‚úÖ Performance: Magnus comparable speed to RK4
- ‚úÖ Convergence: PMP BVP solver converges in < 20 iterations

---

## 3. Machine Learning Integration ü§ñ

### Objective
Integrate deep reinforcement learning for adaptive optimal control that learns from experience and generalizes across similar thermodynamic systems.

### Components

#### 3.1 Neural Network Optimal Control Policies

**Architecture Overview**:
```
Environment: Thermodynamic System
    State: s = [x, temperature, Œª(t)]
    Action: a = u(t) (control)
    Reward: r = -(ŒîS + Œª¬∑control_cost)

Agent: Actor-Critic (A2C/PPO)
    Actor: œÄ_Œ∏(a|s) - policy network
    Critic: V_œÜ(s) - value network
```

**Implementation**:

```python
# ml_optimal_control/__init__.py
from .neural_policies import NeuralOptimalController
from .pinn_solver import PhysicsInformedNN
from .rl_environment import ThermodynamicEnvironment

__all__ = [
    'NeuralOptimalController',
    'PhysicsInformedNN',
    'ThermodynamicEnvironment'
]
```

```python
# ml_optimal_control/neural_policies.py
import jax
import jax.numpy as jnp
import flax.linen as nn
import optax
from typing import Tuple

class ActorNetwork(nn.Module):
    """Policy network œÄ_Œ∏(a|s)."""
    action_dim: int

    @nn.compact
    def __call__(self, state):
        x = nn.Dense(128)(state)
        x = nn.relu(x)
        x = nn.Dense(128)(x)
        x = nn.relu(x)

        # Mean of action distribution
        mean = nn.Dense(self.action_dim)(x)
        # Log std (learned)
        log_std = self.param('log_std', nn.initializers.zeros, (self.action_dim,))

        return mean, jnp.exp(log_std)

class CriticNetwork(nn.Module):
    """Value network V_œÜ(s)."""

    @nn.compact
    def __call__(self, state):
        x = nn.Dense(128)(state)
        x = nn.relu(x)
        x = nn.Dense(128)(x)
        x = nn.relu(x)
        value = nn.Dense(1)(x)
        return value.squeeze()

class NeuralOptimalController:
    """Deep RL optimal control with PPO algorithm."""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        learning_rate: float = 3e-4,
        gamma: float = 0.99,
        clip_epsilon: float = 0.2
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.clip_epsilon = clip_epsilon

        # Initialize networks
        self.actor = ActorNetwork(action_dim=action_dim)
        self.critic = CriticNetwork()

        # Optimizers
        self.actor_optimizer = optax.adam(learning_rate)
        self.critic_optimizer = optax.adam(learning_rate)

        # Initialize parameters
        key = jax.random.PRNGKey(0)
        dummy_state = jnp.zeros((state_dim,))
        self.actor_params = self.actor.init(key, dummy_state)
        self.critic_params = self.critic.init(key, dummy_state)

        self.actor_opt_state = self.actor_optimizer.init(self.actor_params)
        self.critic_opt_state = self.critic_optimizer.init(self.critic_params)

    def select_action(self, state: jnp.ndarray, key: jax.random.PRNGKey) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """Sample action from policy."""
        mean, std = self.actor.apply(self.actor_params, state)
        action = mean + std * jax.random.normal(key, shape=mean.shape)
        log_prob = -0.5 * jnp.sum(
            ((action - mean) / std) ** 2 + 2 * jnp.log(std) + jnp.log(2 * jnp.pi)
        )
        return action, log_prob

    def train(self, environment, n_episodes: int = 10000, steps_per_episode: int = 200):
        """Train using PPO algorithm.

        Args:
            environment: ThermodynamicEnvironment instance
            n_episodes: Number of training episodes
            steps_per_episode: Max steps per episode

        Returns:
            training_history: Dict with metrics
        """
        key = jax.random.PRNGKey(42)
        episode_rewards = []

        for episode in range(n_episodes):
            key, subkey = jax.random.split(key)

            # Collect trajectory
            states, actions, rewards, log_probs = self._collect_trajectory(
                environment, subkey, steps_per_episode
            )

            # Compute returns and advantages
            returns = self._compute_returns(rewards)
            values = jax.vmap(lambda s: self.critic.apply(self.critic_params, s))(states)
            advantages = returns - values

            # PPO update
            self._ppo_update(states, actions, log_probs, advantages, returns)

            # Log progress
            episode_rewards.append(jnp.sum(rewards))

            if episode % 100 == 0:
                print(f"Episode {episode}: Mean reward = {jnp.mean(jnp.array(episode_rewards[-100:])):.2f}")

        return {
            'episode_rewards': episode_rewards,
            'final_policy_params': self.actor_params,
            'final_value_params': self.critic_params
        }

    def _collect_trajectory(self, environment, key, max_steps):
        """Collect one episode of experience."""
        states, actions, rewards, log_probs = [], [], [], []

        state = environment.reset()

        for step in range(max_steps):
            key, subkey = jax.random.split(key)

            action, log_prob = self.select_action(state, subkey)
            next_state, reward, done = environment.step(action)

            states.append(state)
            actions.append(action)
            rewards.append(reward)
            log_probs.append(log_prob)

            state = next_state

            if done:
                break

        return (
            jnp.array(states),
            jnp.array(actions),
            jnp.array(rewards),
            jnp.array(log_probs)
        )

    def _compute_returns(self, rewards):
        """Compute discounted returns."""
        returns = []
        G = 0

        for r in reversed(rewards):
            G = r + self.gamma * G
            returns.insert(0, G)

        return jnp.array(returns)

    def _ppo_update(self, states, actions, old_log_probs, advantages, returns):
        """PPO policy and value updates."""
        # Update actor (policy)
        def actor_loss_fn(params):
            means, stds = jax.vmap(lambda s: self.actor.apply(params, s))(states)
            new_log_probs = -0.5 * jnp.sum(
                ((actions - means) / stds) ** 2 + 2 * jnp.log(stds) + jnp.log(2 * jnp.pi),
                axis=-1
            )

            ratio = jnp.exp(new_log_probs - old_log_probs)
            clipped_ratio = jnp.clip(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)

            loss = -jnp.mean(jnp.minimum(ratio * advantages, clipped_ratio * advantages))
            return loss

        # Update critic (value)
        def critic_loss_fn(params):
            values = jax.vmap(lambda s: self.critic.apply(params, s))(states)
            loss = jnp.mean((returns - values) ** 2)
            return loss

        # Gradient descent
        actor_loss, actor_grads = jax.value_and_grad(actor_loss_fn)(self.actor_params)
        updates, self.actor_opt_state = self.actor_optimizer.update(
            actor_grads, self.actor_opt_state
        )
        self.actor_params = optax.apply_updates(self.actor_params, updates)

        critic_loss, critic_grads = jax.value_and_grad(critic_loss_fn)(self.critic_params)
        updates, self.critic_opt_state = self.critic_optimizer.update(
            critic_grads, self.critic_opt_state
        )
        self.critic_params = optax.apply_updates(self.critic_params, updates)
```

```python
# ml_optimal_control/rl_environment.py
import jax.numpy as jnp

class ThermodynamicEnvironment:
    """Thermodynamic system environment for RL training.

    State: [position, velocity, temperature, control_parameter]
    Action: Control force u(t)
    Reward: -[entropy_production + Œª¬∑control_cost]
    """

    def __init__(
        self,
        state_dim: int = 4,
        action_dim: int = 1,
        temperature: float = 300.0,
        kB: float = 1.380649e-23
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.temperature = temperature
        self.kB = kB
        self.dt = 0.01  # Time step

        self.state = None

    def reset(self):
        """Reset environment to initial state."""
        # Initial state: random around equilibrium
        self.state = jnp.array([0.0, 0.0, self.temperature, 0.0])
        return self.state

    def step(self, action):
        """Take one step in the environment.

        Args:
            action: Control force u(t)

        Returns:
            next_state: New state after action
            reward: Immediate reward
            done: Episode termination flag
        """
        # Unpack state
        x, v, T, lambda_param = self.state
        u = action[0]

        # Dynamics: overdamped Langevin equation
        # dx/dt = u + Œ∑(t)
        # Simplified: x_{t+1} = x_t + u * dt
        x_new = x + u * self.dt
        v_new = u  # Velocity = control
        T_new = T  # Constant temperature
        lambda_new = lambda_param + 0.01  # Protocol evolution

        # Compute entropy production (simplified)
        entropy_production = (u ** 2) / (2 * self.kB * T)

        # Reward: minimize entropy production + control cost
        control_cost = 0.1 * (u ** 2)
        reward = -(entropy_production + control_cost)

        # Done if reached target or max steps
        done = (abs(x_new - 1.0) < 0.01) or (lambda_new > 1.0)

        self.state = jnp.array([x_new, v_new, T_new, lambda_new])

        return self.state, reward, done

    def get_state(self):
        """Return current state."""
        return self.state
```

#### 3.2 Physics-Informed Neural Networks (PINNs)

**Theory**: Embed physical laws (fluctuation theorems, thermodynamic bounds) as loss terms.

```python
# ml_optimal_control/pinn_solver.py
import jax
import jax.numpy as jnp
import flax.linen as nn

class PhysicsInformedNN(nn.Module):
    """Learn solutions to HJB equation with thermodynamic constraints."""

    hidden_dim: int = 64

    @nn.compact
    def __call__(self, x, t):
        """Value function V(x, t)."""
        # Concatenate state and time
        input_features = jnp.concatenate([x, jnp.array([t])])

        # Neural network
        z = nn.Dense(self.hidden_dim)(input_features)
        z = nn.tanh(z)
        z = nn.Dense(self.hidden_dim)(z)
        z = nn.tanh(z)
        z = nn.Dense(self.hidden_dim)(z)
        z = nn.tanh(z)
        value = nn.Dense(1)(z)

        return value.squeeze()

    def hjb_residual(self, params, x, t, dynamics_fn, cost_fn):
        """Compute HJB equation residual for loss.

        HJB: -‚àÇV/‚àÇt = min_u [L(x,u) + ‚àáV¬∑f(x,u)]
        """
        # Autodiff to get gradients
        def value_fn(x_input, t_input):
            return self.apply(params, x_input, t_input)

        # ‚àÇV/‚àÇt
        dV_dt = jax.grad(value_fn, argnums=1)(x, t)

        # ‚àáV
        grad_V = jax.grad(value_fn, argnums=0)(x, t)

        # Optimal control from Hamiltonian minimization
        def hamiltonian(u):
            return cost_fn(x, u) + jnp.dot(grad_V, dynamics_fn(x, u))

        u_opt = jax.scipy.optimize.minimize(hamiltonian, x0=jnp.zeros_like(x)).x

        # HJB residual
        residual = -dV_dt - (cost_fn(x, u_opt) + jnp.dot(grad_V, dynamics_fn(x, u_opt)))

        return residual ** 2

    def train(self, training_data, dynamics_fn, cost_fn, n_iterations=10000):
        """Train PINN with physics loss."""
        # Training loop with HJB residual loss + data loss
        # ...
```

### Files to Create/Modify

**New Files**:
- `ml_optimal_control/__init__.py`
- `ml_optimal_control/neural_policies.py` (PPO implementation)
- `ml_optimal_control/rl_environment.py` (Thermodynamic env)
- `ml_optimal_control/pinn_solver.py` (Physics-informed NN)
- `ml_optimal_control/pretrained_models/` (Saved model directory)
- `tests/ml/test_neural_policies.py`
- `tests/ml/test_pinn.py`
- `examples/ml_optimal_control_demo.py`

**Modified Files**:
- `optimal_control_agent.py`: Add neural network policy option
- `requirements.txt`: Add `jax>=0.4.0`, `flax>=0.7.0`, `optax>=0.1.4`

### Success Metrics
- ‚úÖ PPO converges on simple LQR benchmark
- ‚úÖ Neural policy matches or exceeds PMP solution
- ‚úÖ Transfer learning: Train on one system, generalize to similar systems
- ‚úÖ PINN solver: HJB residual < 1e-4
- ‚úÖ Training time: < 4 hours on single GPU

---

## 4. Interactive Visualization üìä

### Objective
Create web-based interactive dashboards for real-time monitoring and analysis of agent execution results.

### Components

#### 4.1 Dashboard Architecture

**Technology Stack**:
- **Frontend**: Plotly Dash (Python-based)
- **Backend**: WebSocket for real-time updates
- **Visualization**: Plotly.js, D3.js for custom plots
- **Deployment**: Docker container, accessible via browser

**Dashboard Modules**:

1. **Quantum Evolution Viewer**
   - Real-time density matrix visualization (Hinton diagrams)
   - Entropy/purity time series plots
   - Interactive Bloch sphere animation (for qubits)
   - Export animations as GIF/MP4

2. **Optimal Control Designer**
   - Interactive protocol editor (drag control points)
   - Real-time dissipation calculation
   - Speed limit bounds overlay
   - Compare multiple protocols

3. **Large Deviation Explorer**
   - Rate function I(x) surface plots
   - Committor probability heatmaps
   - Rare trajectory playback
   - s-ensemble phase diagram

**Implementation**:

```python
# visualization/dashboard/app.py
import dash
from dash import dcc, html, Input, Output, State
import plotly.graph_objs as go
import numpy as np

# Import agent wrappers
from nonequilibrium_quantum_agent import NonequilibriumQuantumAgent
from optimal_control_agent import OptimalControlAgent
from large_deviation_agent import LargeDeviationAgent

# Initialize Dash app
app = dash.Dash(__name__, suppress_callback_exceptions=True)

# Layout
app.layout = html.Div([
    html.H1("Nonequilibrium Physics Agent Dashboard"),

    dcc.Tabs(id='tabs', value='quantum', children=[
        dcc.Tab(label='Quantum Evolution', value='quantum'),
        dcc.Tab(label='Optimal Control', value='control'),
        dcc.Tab(label='Large Deviation', value='deviation')
    ]),

    html.Div(id='tab-content')
])

# Quantum tab layout
def quantum_layout():
    return html.Div([
        html.H2("Lindblad Master Equation Evolution"),

        html.Div([
            html.Label("Hilbert Space Dimension:"),
            dcc.Slider(id='n-dim-slider', min=2, max=10, step=1, value=2,
                      marks={i: str(i) for i in range(2, 11)}),

            html.Label("Decay Rate (Œ≥):"),
            dcc.Input(id='decay-rate', type='number', value=0.1, step=0.01),

            html.Label("Evolution Time:"),
            dcc.Input(id='time', type='number', value=10.0, step=0.5),

            html.Button('Run Simulation', id='run-quantum-btn', n_clicks=0)
        ], style={'width': '30%', 'display': 'inline-block', 'vertical-align': 'top'}),

        html.Div([
            dcc.Graph(id='density-matrix-plot'),
            dcc.Graph(id='entropy-plot'),
            dcc.Graph(id='populations-plot')
        ], style={'width': '65%', 'display': 'inline-block'})
    ])

# Optimal control tab layout
def control_layout():
    return html.Div([
        html.H2("Optimal Protocol Designer"),

        html.Div([
            html.Label("Protocol Type:"),
            dcc.Dropdown(
                id='protocol-type',
                options=[
                    {'label': 'Minimal Dissipation', 'value': 'minimal'},
                    {'label': 'Shortcut to Adiabaticity', 'value': 'shortcut'},
                    {'label': 'Pontryagin (PMP)', 'value': 'pmp'},
                    {'label': 'Neural Network', 'value': 'neural'}
                ],
                value='minimal'
            ),

            html.Label("Duration (œÑ):"),
            dcc.Input(id='duration', type='number', value=10.0),

            html.Label("Temperature (K):"),
            dcc.Input(id='temperature', type='number', value=300.0),

            html.Button('Optimize Protocol', id='run-control-btn', n_clicks=0)
        ], style={'width': '30%', 'display': 'inline-block', 'vertical-align': 'top'}),

        html.Div([
            dcc.Graph(id='protocol-plot'),
            dcc.Graph(id='dissipation-plot'),
            dcc.Graph(id='speedlimit-plot')
        ], style={'width': '65%', 'display': 'inline-block'})
    ])

# Callbacks
@app.callback(
    Output('tab-content', 'children'),
    Input('tabs', 'value')
)
def render_tab_content(tab):
    if tab == 'quantum':
        return quantum_layout()
    elif tab == 'control':
        return control_layout()
    # ... more tabs

@app.callback(
    [Output('density-matrix-plot', 'figure'),
     Output('entropy-plot', 'figure'),
     Output('populations-plot', 'figure')],
    Input('run-quantum-btn', 'n_clicks'),
    [State('n-dim-slider', 'value'),
     State('decay-rate', 'value'),
     State('time', 'value')]
)
def run_quantum_simulation(n_clicks, n_dim, decay_rate, time):
    if n_clicks == 0:
        # Return empty plots
        return go.Figure(), go.Figure(), go.Figure()

    # Run quantum agent
    agent = NonequilibriumQuantumAgent()
    result = agent.execute({
        'method': 'lindblad_master_equation',
        'data': {'n_dim': n_dim},
        'parameters': {'time': time, 'decay_rate': decay_rate},
        'analysis': ['evolution', 'entropy']
    })

    # Extract data
    data = result.data
    rho_final = np.array(data['rho_final'])
    entropy = data['entropy']
    populations = np.array(data['populations'])
    time_grid = data['time_grid']

    # Create plots
    # 1. Density matrix (Hinton diagram)
    fig_rho = go.Figure(data=go.Heatmap(
        z=np.abs(rho_final),
        colorscale='Viridis',
        text=np.round(rho_final, 3),
        texttemplate='%{text}',
        textfont={"size": 10}
    ))
    fig_rho.update_layout(title='Final Density Matrix |œÅ(T)|',
                          xaxis_title='Column', yaxis_title='Row')

    # 2. Entropy evolution
    fig_entropy = go.Figure()
    fig_entropy.add_trace(go.Scatter(x=time_grid, y=entropy, mode='lines',
                                     name='Von Neumann Entropy'))
    fig_entropy.update_layout(title='Entropy S(t)', xaxis_title='Time',
                              yaxis_title='S (nats)')

    # 3. Population dynamics
    fig_pop = go.Figure()
    for i in range(n_dim):
        fig_pop.add_trace(go.Scatter(x=time_grid, y=populations[:, i],
                                     mode='lines', name=f'Level {i}'))
    fig_pop.update_layout(title='Population Dynamics', xaxis_title='Time',
                          yaxis_title='Population')

    return fig_rho, fig_entropy, fig_pop

# Run server
if __name__ == '__main__':
    app.run_server(debug=True, host='0.0.0.0', port=8050)
```

#### 4.2 Deployment

**Docker Container**:
```dockerfile
# visualization/Dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy dashboard code
COPY dashboard/ ./dashboard/
COPY ../nonequilibrium_quantum_agent.py .
COPY ../optimal_control_agent.py .
COPY ../large_deviation_agent.py .
COPY ../base_agent.py .

# Expose port
EXPOSE 8050

# Run dashboard
CMD ["python", "dashboard/app.py"]
```

**Launch Script**:
```bash
# visualization/launch_dashboard.sh
#!/bin/bash

echo "Building dashboard Docker image..."
docker build -t nep-dashboard:latest .

echo "Starting dashboard server..."
docker run -p 8050:8050 nep-dashboard:latest

echo "Dashboard available at http://localhost:8050"
```

### Files to Create/Modify

**New Files**:
- `visualization/dashboard/app.py` (Main Dash app)
- `visualization/dashboard/layouts/quantum.py`
- `visualization/dashboard/layouts/control.py`
- `visualization/dashboard/layouts/deviation.py`
- `visualization/dashboard/callbacks/quantum_callbacks.py`
- `visualization/dashboard/callbacks/control_callbacks.py`
- `visualization/plotters/quantum_viz.py` (Plotting utilities)
- `visualization/plotters/control_viz.py`
- `visualization/plotters/deviation_viz.py`
- `visualization/Dockerfile`
- `visualization/requirements.txt`
- `visualization/launch_dashboard.sh`

**Modified Files**:
- `requirements.txt`: Add `dash>=2.14.0`, `plotly>=5.17.0`

### Success Metrics
- ‚úÖ Dashboard loads in < 2 seconds
- ‚úÖ Real-time updates at 10 Hz
- ‚úÖ All agent integrations functional
- ‚úÖ Interactive protocol editor responsive
- ‚úÖ Export animations (GIF/MP4) working

---

## 5. HPC Integration ‚ö°

### Objective
Enable seamless execution on HPC clusters (SLURM, PBS, LSF) with distributed computing via Dask.

### Components

#### 5.1 Cluster Schedulers

**Supported Systems**:
- SLURM (Slurm Workload Manager)
- PBS/Torque (Portable Batch System)
- LSF (IBM Spectrum LSF)
- SGE (Sun Grid Engine)

**Implementation**:

```python
# hpc/schedulers.py
import subprocess
import os
from pathlib import Path
from typing import Dict, Any, Optional

class ClusterScheduler:
    """Base class for HPC cluster schedulers."""

    def submit_job(self, agent_name: str, input_data: Dict[str, Any],
                   resources: Dict[str, Any]) -> str:
        """Submit agent execution as cluster job.

        Args:
            agent_name: Name of agent to run
            input_data: Agent input data
            resources: Resource requirements (nodes, gpus, time, memory)

        Returns:
            job_id: Cluster job identifier
        """
        raise NotImplementedError

    def check_status(self, job_id: str) -> str:
        """Check job status (PENDING, RUNNING, COMPLETED, FAILED)."""
        raise NotImplementedError

    def cancel_job(self, job_id: str):
        """Cancel running job."""
        raise NotImplementedError

    def get_output(self, job_id: str) -> Dict[str, Any]:
        """Retrieve job output once completed."""
        raise NotImplementedError

class SLURMScheduler(ClusterScheduler):
    """SLURM cluster scheduler."""

    def __init__(self, partition: str = 'gpu', account: Optional[str] = None):
        self.partition = partition
        self.account = account

    def submit_job(self, agent_name: str, input_data: Dict[str, Any],
                   resources: Dict[str, Any]) -> str:
        """Submit job to SLURM."""
        # Generate job script
        job_script = self._generate_slurm_script(agent_name, input_data, resources)

        # Write to file
        script_path = f"/tmp/{agent_name}_{os.getpid()}.sh"
        with open(script_path, 'w') as f:
            f.write(job_script)

        # Submit via sbatch
        result = subprocess.run(
            ['sbatch', script_path],
            capture_output=True,
            text=True
        )

        # Extract job ID
        # Output format: "Submitted batch job 12345"
        job_id = result.stdout.strip().split()[-1]

        return job_id

    def _generate_slurm_script(self, agent_name: str, input_data: Dict[str, Any],
                                resources: Dict[str, Any]) -> str:
        """Generate SLURM batch script."""
        nodes = resources.get('nodes', 1)
        gpus = resources.get('gpus', 0)
        cpus = resources.get('cpus', 4)
        memory_gb = resources.get('memory_gb', 16)
        time_hours = resources.get('time_hours', 24)

        script = f"""#!/bin/bash
#SBATCH --job-name={agent_name}
#SBATCH --partition={self.partition}
#SBATCH --nodes={nodes}
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task={cpus}
#SBATCH --mem={memory_gb}G
#SBATCH --time={time_hours}:00:00
"""

        if gpus > 0:
            script += f"#SBATCH --gpus={gpus}\n"

        if self.account:
            script += f"#SBATCH --account={self.account}\n"

        script += f"""
#SBATCH --output={agent_name}_%j.out
#SBATCH --error={agent_name}_%j.err

# Environment setup
module load python/3.10
module load cuda/12.0

# Activate conda environment
source activate neph-agents

# Run agent
python -c "
import json
from {agent_name} import {agent_name.title().replace('_', '')}Agent

# Load input data
with open('/tmp/input_{agent_name}_{os.getpid()}.json', 'r') as f:
    input_data = json.load(f)

# Execute agent
agent = {agent_name.title().replace('_', '')}Agent()
result = agent.execute(input_data)

# Save output
with open('/tmp/output_{agent_name}_{os.getpid()}.json', 'w') as f:
    json.dump(result.data, f)

print('Job completed successfully')
"
"""

        return script

    def check_status(self, job_id: str) -> str:
        """Check SLURM job status."""
        result = subprocess.run(
            ['squeue', '-j', job_id, '-h', '-o', '%T'],
            capture_output=True,
            text=True
        )

        status = result.stdout.strip()

        # Map SLURM status to standard
        status_map = {
            'PENDING': 'PENDING',
            'RUNNING': 'RUNNING',
            'COMPLETED': 'COMPLETED',
            'FAILED': 'FAILED',
            'CANCELLED': 'CANCELLED'
        }

        return status_map.get(status, 'UNKNOWN')

    def cancel_job(self, job_id: str):
        """Cancel SLURM job."""
        subprocess.run(['scancel', job_id])

    def get_output(self, job_id: str) -> Dict[str, Any]:
        """Retrieve job output."""
        output_file = f"/tmp/output_*_{job_id}.json"

        import json
        import glob

        matches = glob.glob(output_file)
        if not matches:
            raise FileNotFoundError(f"Output file not found for job {job_id}")

        with open(matches[0], 'r') as f:
            return json.load(f)

class PBSScheduler(ClusterScheduler):
    """PBS/Torque scheduler (similar structure to SLURM)."""
    # Implementation similar to SLURM with PBS-specific commands
    pass

class LSFScheduler(ClusterScheduler):
    """IBM Spectrum LSF scheduler."""
    # Implementation with bsub, bjobs, bkill commands
    pass
```

#### 5.2 Distributed Execution with Dask

**Architecture**:
```
Dask Scheduler (central coordinator)
    |
    +-- Worker 1 (Node 1, 4 GPUs)
    +-- Worker 2 (Node 2, 4 GPUs)
    +-- Worker 3 (Node 3, 4 GPUs)
    ...
    +-- Worker N (Node N, 4 GPUs)
```

**Implementation**:

```python
# hpc/distributed_agent.py
import dask
import dask.distributed as distributed
from typing import List, Dict, Any
import numpy as np

class DistributedAgentExecutor:
    """Execute agents across distributed Dask cluster."""

    def __init__(self, scheduler_address: str = 'tcp://cluster:8786'):
        """
        Args:
            scheduler_address: Dask scheduler address
                - Local: 'localhost:8786'
                - Cluster: 'tcp://head-node:8786'
        """
        self.client = distributed.Client(scheduler_address)
        print(f"Connected to Dask cluster: {self.client}")
        print(f"Workers: {len(self.client.scheduler_info()['workers'])}")

    def execute_batch(self, agent_class, input_batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Execute agent on batch of inputs in parallel.

        Args:
            agent_class: Agent class (e.g., NonequilibriumQuantumAgent)
            input_batch: List of input dictionaries

        Returns:
            results: List of output dictionaries

        Example:
            >>> executor = DistributedAgentExecutor('tcp://cluster:8786')
            >>> inputs = [{'method': 'lindblad', 'data': {...}} for _ in range(1000)]
            >>> results = executor.execute_batch(NonequilibriumQuantumAgent, inputs)
            >>> # Results computed across 100 nodes in parallel
        """
        # Create agent instance on each worker
        agent_futures = self.client.scatter(agent_class())

        # Submit tasks
        futures = self.client.map(
            lambda agent, inp: agent.execute(inp),
            [agent_futures] * len(input_batch),
            input_batch
        )

        # Gather results
        results = self.client.gather(futures)

        return [r.data for r in results]

    def execute_parameter_sweep(
        self,
        agent_class,
        base_input: Dict[str, Any],
        parameter_grid: Dict[str, List[Any]]
    ) -> Dict[str, List[Any]]:
        """Execute parameter sweep across cluster.

        Args:
            agent_class: Agent class
            base_input: Base input configuration
            parameter_grid: Parameters to sweep
                Example: {'temperature': [100, 200, 300], 'time': [1, 5, 10]}

        Returns:
            results: Dictionary mapping parameter combinations to outputs

        Example:
            >>> grid = {'temperature': np.linspace(100, 500, 50),
            ...         'decay_rate': np.logspace(-3, 0, 30)}
            >>> results = executor.execute_parameter_sweep(
            ...     NonequilibriumQuantumAgent,
            ...     {'method': 'lindblad', 'data': {...}},
            ...     grid
            ... )
            >>> # 1500 simulations completed in parallel
        """
        from itertools import product

        # Generate all parameter combinations
        param_names = list(parameter_grid.keys())
        param_values = list(parameter_grid.values())
        combinations = list(product(*param_values))

        # Create input batch
        input_batch = []
        for combo in combinations:
            input_data = base_input.copy()
            input_data['parameters'] = input_data.get('parameters', {}).copy()

            for name, value in zip(param_names, combo):
                input_data['parameters'][name] = value

            input_batch.append(input_data)

        print(f"Executing {len(input_batch)} parameter combinations...")

        # Execute in parallel
        results_data = self.execute_batch(agent_class, input_batch)

        # Organize results by parameter combination
        results = {
            'parameter_grid': parameter_grid,
            'combinations': combinations,
            'results': results_data
        }

        return results

    def close(self):
        """Close Dask client."""
        self.client.close()

# Convenience function for launching Dask cluster on SLURM
def launch_dask_slurm_cluster(
    n_workers: int = 10,
    cores_per_worker: int = 4,
    memory_per_worker: str = '16GB',
    gpus_per_worker: int = 1,
    partition: str = 'gpu',
    walltime: str = '24:00:00'
) -> distributed.Client:
    """Launch Dask cluster on SLURM.

    Uses dask-jobqueue to automatically submit SLURM jobs for workers.
    """
    from dask_jobqueue import SLURMCluster

    cluster = SLURMCluster(
        cores=cores_per_worker,
        memory=memory_per_worker,
        processes=1,
        walltime=walltime,
        queue=partition,
        extra=['--resources "GPU=1"'] if gpus_per_worker > 0 else []
    )

    cluster.scale(jobs=n_workers)

    client = distributed.Client(cluster)

    print(f"Dask cluster launched: {cluster.dashboard_link}")

    return client
```

**Usage Example**:

```python
# examples/distributed_execution_demo.py
from hpc.distributed_agent import DistributedAgentExecutor, launch_dask_slurm_cluster
from nonequilibrium_quantum_agent import NonequilibriumQuantumAgent
import numpy as np

# Launch Dask cluster on SLURM (100 workers, 4 GPUs each = 400 GPUs total)
client = launch_dask_slurm_cluster(
    n_workers=100,
    cores_per_worker=16,
    memory_per_worker='64GB',
    gpus_per_worker=4,
    partition='gpu',
    walltime='24:00:00'
)

# Create distributed executor
executor = DistributedAgentExecutor(client.scheduler.address)

# Parameter sweep: 1000 quantum fluctuation theorem trajectories
parameter_grid = {
    'temperature': np.linspace(100, 500, 50),
    'n_realizations': [1000] * 50,
    'time': [10.0] * 50
}

results = executor.execute_parameter_sweep(
    NonequilibriumQuantumAgent,
    {
        'method': 'quantum_fluctuation_theorem',
        'data': {'n_dim': 4},
        'parameters': {}
    },
    parameter_grid
)

print(f"Completed {len(results['results'])} simulations")
print(f"Total trajectories: {50 * 1000} = 50,000")

# Analyze results
jarzynski_ratios = [r['jarzynski_ratio'] for r in results['results']]
print(f"Mean Jarzynski ratio: {np.mean(jarzynski_ratios):.4f} ¬± {np.std(jarzynski_ratios):.4f}")

# Close
executor.close()
```

### Files to Create/Modify

**New Files**:
- `hpc/__init__.py`
- `hpc/schedulers.py` (SLURM, PBS, LSF)
- `hpc/distributed_agent.py` (Dask integration)
- `hpc/job_templates/` (Job script templates)
- `examples/distributed_execution_demo.py`
- `examples/slurm_submission_demo.py`
- `tests/hpc/test_schedulers.py`
- `tests/hpc/test_distributed.py`

**Modified Files**:
- `base_agent.py`: Add `submit_to_cluster()` method
- `requirements.txt`: Add `dask>=2023.5.0`, `distributed>=2023.5.0`, `dask-jobqueue>=0.8.0`

### Success Metrics
- ‚úÖ SLURM job submission working
- ‚úÖ Dask cluster scales to 100+ nodes
- ‚úÖ 1000 parallel tasks complete in < 10 minutes
- ‚úÖ GPU utilization > 75% across cluster
- ‚úÖ Fault tolerance: Auto-restart failed tasks

---

## 6. Higher Test Coverage üß™

### Objective
Increase Phase 3 test pass rate from 77.6% (173/223) to 95%+ (210+/223) by fixing edge cases, stochastic test issues, and integration mismatches.

### Current Failure Analysis

**Failure Categories** (50 failing tests):
1. **Resource Estimation Edge Cases** (~15 tests)
   - `estimate_resources()` fails for extreme n_dim values
   - Environment routing logic errors
   - Mock execution environment mismatches

2. **Stochastic Test Variation** (~20 tests)
   - Random number generation causes occasional failures
   - Statistical tests fail due to insufficient samples
   - Rare event sampling has high variance

3. **Integration Data Structure Mismatches** (~15 tests)
   - Agent-to-agent data passing format inconsistent
   - JSON serialization issues with numpy arrays
   - Legacy test expectations outdated

### Fix Strategy

#### 6.1 Resource Estimation Fixes (Week 1-2)

**Problem**: Edge cases in `estimate_resources()` cause failures

**Solution**:
```python
# In nonequilibrium_quantum_agent.py
def estimate_resources(self, input_data: Dict[str, Any]) -> ResourceRequirement:
    """Estimate computational resources needed."""
    method = input_data.get('method', 'lindblad_master_equation')
    params = input_data.get('parameters', {})
    data = input_data.get('data', {})

    # Robust extraction with defaults
    n_dim = data.get('n_dim', 2)
    n_steps = params.get('n_steps', 100)

    # Validate inputs
    if n_dim < 2:
        n_dim = 2
    if n_dim > 100:  # Cap at reasonable value
        n_dim = 100

    if n_steps < 10:
        n_steps = 10
    if n_steps > 100000:
        n_steps = 100000

    # Compute resources with edge case handling
    if method == 'lindblad_master_equation':
        # Scaling: O(n_dim^2) for density matrix
        cpu_cores = min(max(4, n_dim // 2), 64)  # Clamp to [4, 64]
        memory_gb = max(2.0, 2.0 * (n_dim**2 / 4.0))
        duration_est = max(60, 60 * (n_dim**2 / 4.0))

        # Environment selection
        if n_dim > 10 or n_steps > 5000:
            env = ExecutionEnvironment.HPC
        elif n_dim > 4:
            env = ExecutionEnvironment.CLOUD
        else:
            env = ExecutionEnvironment.LOCAL

    # ... similar fixes for other methods

    return ResourceRequirement(
        cpu_cores=cpu_cores,
        memory_gb=memory_gb,
        gpu_required=False,
        estimated_duration_seconds=duration_est,
        environment=env
    )
```

**Tests to Fix** (15 tests):
- `test_quantum_estimate_resources_edge_cases`
- `test_optimal_control_resource_estimation`
- `test_large_deviation_hpc_routing`
- ... (all resource estimation tests)

#### 6.2 Stochastic Test Fixes (Week 3-4)

**Problem**: Tests with randomness fail occasionally

**Solution Strategies**:

1. **Fixed Random Seeds**:
```python
# In tests/test_nonequilibrium_quantum_agent.py
def test_quantum_fluctuation_theorem(quantum_agent):
    """Test quantum Jarzynski equality."""
    np.random.seed(42)  # FIX: Add fixed seed

    input_data = {
        'method': 'quantum_fluctuation_theorem',
        'data': {'n_dim': 2},
        'parameters': {
            'temperature': 300.0,
            'n_realizations': 2000  # FIX: Increase from 1000
        }
    }

    result = quantum_agent.execute(input_data)

    # FIX: Add statistical tolerance
    jarzynski_ratio = result.data['jarzynski_ratio']
    assert 0.9 <= jarzynski_ratio <= 1.1, \
        f"Jarzynski ratio {jarzynski_ratio:.3f} outside [0.9, 1.1]"
```

2. **Statistical Tolerances**:
```python
# tests/utils/statistical_assertions.py
import numpy as np
from scipy import stats

def assert_jarzynski_equality(work_samples, delta_F, temperature, tolerance=0.1):
    """Statistical test for Jarzynski equality with tolerance.

    Tests: ‚ü®e^(-Œ≤W)‚ü© ‚âà e^(-Œ≤ŒîF) within tolerance
    """
    kB = 1.380649e-23
    beta = 1.0 / (kB * temperature)

    lhs = np.mean(np.exp(-beta * work_samples))
    rhs = np.exp(-beta * delta_F)
    ratio = lhs / rhs

    # Bootstrap confidence interval
    n_bootstrap = 1000
    bootstrap_ratios = []

    for _ in range(n_bootstrap):
        sample = np.random.choice(work_samples, size=len(work_samples), replace=True)
        lhs_boot = np.mean(np.exp(-beta * sample))
        bootstrap_ratios.append(lhs_boot / rhs)

    ci_lower, ci_upper = np.percentile(bootstrap_ratios, [2.5, 97.5])

    # Test: 1.0 should be in 95% CI
    assert ci_lower <= 1.0 <= ci_upper, \
        f"Jarzynski equality failed: ratio={ratio:.3f}, CI=[{ci_lower:.3f}, {ci_upper:.3f}]"
```

3. **Increase Sample Sizes**:
```python
# tests/test_large_deviation_agent.py
def test_rare_event_sampling(ld_agent):
    # OLD: n_samples = 1000
    # NEW: n_samples = 5000 (5x more)
    input_data = {
        'method': 'rare_event_sampling',
        'data': {'observable': work_samples.tolist()},
        'parameters': {'n_samples': 5000}  # FIX: Increased
    }
```

**Tests to Fix** (20 tests):
- All `test_*_fluctuation_theorem` tests
- All `test_rare_event_*` tests
- All stochastic dynamics tests with random trajectories

#### 6.3 Integration Data Structure Fixes (Week 5-6)

**Problem**: Agent-to-agent data passing inconsistent

**Solution**: Standardize data schemas with JSON Schema validation

```python
# agents/data_schemas.py
import jsonschema

# Define schemas for agent outputs
QUANTUM_OUTPUT_SCHEMA = {
    "type": "object",
    "properties": {
        "method_type": {"type": "string"},
        "rho_evolution": {"type": "array"},
        "entropy": {"type": "array"},
        "purity": {"type": "array"},
        "n_dim": {"type": "integer"},
        "n_steps": {"type": "integer"}
    },
    "required": ["method_type", "rho_evolution", "entropy", "n_dim"]
}

OPTIMAL_CONTROL_OUTPUT_SCHEMA = {
    "type": "object",
    "properties": {
        "protocol_type": {"type": "string"},
        "lambda_optimal": {"type": "array"},
        "dissipation": {"type": "number"},
        "efficiency_metric": {"type": "number"}
    },
    "required": ["protocol_type", "lambda_optimal", "dissipation"]
}

def validate_output(data: Dict[str, Any], schema: Dict[str, Any]):
    """Validate agent output against schema."""
    try:
        jsonschema.validate(instance=data, schema=schema)
    except jsonschema.ValidationError as e:
        raise ValueError(f"Output validation failed: {e.message}")

# In each agent
def execute(self, input_data):
    # ... computation

    # Validate output before returning
    validate_output(result_data, QUANTUM_OUTPUT_SCHEMA)

    return AgentResult(...)
```

**Tests to Fix** (15 tests):
- `test_ld_fluctuation_integration_method`
- `test_oc_driven_integration_method`
- `test_quantum_transport_coefficients`
- All Phase 3 integration tests

#### 6.4 New Test Coverage (Week 7-8)

**Add Tests for New Features**:

1. **GPU Kernel Tests**:
```python
# tests/gpu/test_quantum_gpu.py
import pytest
import jax.numpy as jnp
from gpu_kernels.quantum_evolution import solve_lindblad_jax

@pytest.mark.gpu
def test_lindblad_gpu_correctness():
    """Verify GPU implementation matches CPU."""
    # Setup
    n_dim = 4
    H = jnp.eye(n_dim)
    L_ops = [jnp.array([[0, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])]
    gammas = [0.1]
    t_span = jnp.linspace(0, 10, 100)
    rho0 = jnp.eye(n_dim) / n_dim

    # Compute on GPU
    result_gpu = solve_lindblad_jax(rho0, H, L_ops, gammas, t_span)

    # Compute on CPU (reference)
    from scipy.integrate import solve_ivp
    # ... CPU reference implementation

    # Compare
    assert jnp.allclose(result_gpu, result_cpu, atol=1e-10)

@pytest.mark.gpu
def test_lindblad_gpu_performance():
    """Verify GPU speedup."""
    import time

    n_dim = 10
    # ... setup

    # GPU timing
    start = time.time()
    result_gpu = solve_lindblad_jax(rho0, H, L_ops, gammas, t_span)
    gpu_time = time.time() - start

    # CPU timing
    start = time.time()
    result_cpu = solve_lindblad_cpu(rho0, H, L_ops, gammas, t_span)
    cpu_time = time.time() - start

    speedup = cpu_time / gpu_time
    assert speedup > 10, f"Expected >10x speedup, got {speedup:.1f}x"
```

2. **Magnus Expansion Tests**:
```python
# tests/solvers/test_magnus.py
def test_magnus_energy_conservation():
    """Magnus should conserve energy better than RK4."""
    from solvers.magnus_expansion import MagnusExpansionSolver

    # Time-dependent Hamiltonian
    H_protocol = [...]

    # Solve with Magnus
    magnus = MagnusExpansionSolver(order=4)
    rho_magnus = magnus.solve(rho0, H_protocol, L_ops, gammas, t_span, n_steps)

    # Solve with RK4 (reference)
    rho_rk4 = solve_rk4(...)

    # Compute energy drift
    energy_magnus = [compute_energy(rho, H) for rho, H in zip(rho_magnus, H_protocol)]
    energy_rk4 = [compute_energy(rho, H) for rho, H in zip(rho_rk4, H_protocol)]

    drift_magnus = np.std(energy_magnus)
    drift_rk4 = np.std(energy_rk4)

    assert drift_magnus < drift_rk4 / 10, "Magnus should have 10x better conservation"
```

3. **Neural Network Policy Tests**:
```python
# tests/ml/test_neural_policies.py
def test_ppo_convergence():
    """PPO should converge on LQR benchmark."""
    from ml_optimal_control.neural_policies import NeuralOptimalController
    from ml_optimal_control.rl_environment import ThermodynamicEnvironment

    controller = NeuralOptimalController(state_dim=4, action_dim=1)
    env = ThermodynamicEnvironment()

    history = controller.train(env, n_episodes=5000)

    # Check convergence
    final_rewards = history['episode_rewards'][-100:]
    assert np.mean(final_rewards) > -10, "Should achieve reasonable performance"

    # Check policy quality
    # ... test on evaluation environment
```

### Files to Modify

**Fix Existing Tests**:
- `tests/test_nonequilibrium_quantum_agent.py` (15 fixes)
- `tests/test_optimal_control_agent.py` (10 fixes)
- `tests/test_large_deviation_agent.py` (10 fixes)
- `tests/test_phase3_integration.py` (15 fixes)

**New Test Files**:
- `tests/gpu/test_quantum_gpu.py`
- `tests/gpu/test_md_gpu.py`
- `tests/solvers/test_magnus.py`
- `tests/solvers/test_pontryagin.py`
- `tests/ml/test_neural_policies.py`
- `tests/ml/test_pinn.py`
- `tests/hpc/test_distributed.py`
- `tests/visualization/test_dashboard.py`

### Success Metrics
- ‚úÖ Test pass rate: 95%+ (210+/223 tests)
- ‚úÖ All edge cases handled
- ‚úÖ Stochastic tests robust (no flakiness)
- ‚úÖ Integration tests standardized
- ‚úÖ New features fully tested
- ‚úÖ CI/CD pipeline passing consistently

---

## Implementation Timeline

### Phase 4.1: Foundation (Weeks 1-16) - **CORE PERFORMANCE**

**Weeks 1-4: GPU Acceleration Foundation**
- Week 1: JAX backend infrastructure
- Week 2: Quantum evolution GPU kernels
- Week 3: CUDA optimizations
- Week 4: MD GPU integration (HOOMD-blue)

**Weeks 5-8: Advanced Solvers**
- Week 5-6: Magnus expansion implementation
- Week 7-8: Pontryagin Maximum Principle solver

**Weeks 9-12: Test Infrastructure**
- Week 9-10: Fix resource estimation edge cases
- Week 11-12: Fix stochastic test issues
- Week 13-14: Standardize integration data formats
- Week 15-16: New test coverage for GPU/solvers

**Milestone 4.1**: 85% test pass rate, 50x GPU speedup achieved

---

### Phase 4.2: Intelligence Layer (Weeks 17-28) - **ML & VISUALIZATION**

**Weeks 17-22: Machine Learning Integration**
- Week 17-18: PPO neural network policies
- Week 19-20: Physics-informed neural networks (PINNs)
- Week 21-22: Transfer learning experiments

**Weeks 23-28: Visualization Dashboard**
- Week 23-24: Dash app infrastructure
- Week 25-26: Real-time monitoring components
- Week 27-28: Interactive protocol designer

**Milestone 4.2**: ML-enhanced control operational, dashboard deployed

---

### Phase 4.3: Scale & Deploy (Weeks 29-40) - **HPC & PRODUCTION**

**Weeks 29-34: HPC Integration**
- Week 29-30: SLURM/PBS schedulers
- Week 31-32: Dask distributed execution
- Week 33-34: Parameter sweep infrastructure

**Weeks 35-40: Production Hardening**
- Week 35-36: Final test coverage push (95%+ goal)
- Week 37-38: Performance benchmarking
- Week 39-40: Documentation and deployment guides

**Milestone 4.3**: 95%+ test pass rate, cluster-ready, production deployment

---

## Risk Assessment & Mitigation

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| **GPU Memory Limits** | Medium | High | Implement sparse storage, batch size tuning, multi-GPU support |
| **Numerical Instability** | Medium | High | Extensive validation vs analytical solutions, adaptive time stepping |
| **ML Training Time** | High | Medium | Start with simple problems, use transfer learning, leverage GPUs |
| **Cluster Compatibility** | Medium | Medium | Abstract scheduler interface, test on local Dask first |
| **Test Fragility** | Low | Medium | Prioritize deterministic tests, statistical tests as sanity checks |
| **Resource Constraints** | Medium | High | Phased implementation, prioritize P0 features first |
| **Integration Complexity** | High | Medium | Incremental integration, maintain backward compatibility |

---

## Dependencies

### Software Requirements

**Core**:
- Python >= 3.10
- NumPy >= 1.24.0
- SciPy >= 1.11.0

**GPU Acceleration** (P0):
- JAX >= 0.4.0
- jaxlib >= 0.4.0
- diffrax >= 0.4.0
- CuPy >= 12.0.0 (optional, CUDA)
- HOOMD-blue >= 4.0.0

**Machine Learning** (P1):
- Flax >= 0.7.0
- Optax >= 0.1.4
- Equinox >= 0.11.0 (optional)

**Visualization** (P1):
- Dash >= 2.14.0
- Plotly >= 5.17.0
- Pandas >= 2.0.0

**HPC** (P1):
- Dask >= 2023.5.0
- Dask-jobqueue >= 0.8.0
- Distributed >= 2023.5.0

**Testing**:
- pytest >= 7.3.0
- pytest-cov >= 4.1.0
- pytest-gpu >= 0.1.0 (new)

### Hardware Requirements

**Development**:
- CPU: 8+ cores
- RAM: 32 GB
- GPU: NVIDIA RTX 3090 / A100 (24 GB VRAM)
- Storage: 500 GB SSD

**Production (HPC Cluster)**:
- Nodes: 10-100+
- GPUs: 4-8 per node (NVIDIA A100 recommended)
- RAM: 256+ GB per node
- Network: InfiniBand (100+ Gbps)
- Scheduler: SLURM / PBS / LSF

---

## Success Criteria

### Performance
- ‚úÖ **GPU Speedup**: 50-100x for quantum evolution (n_dim=10)
- ‚úÖ **Scalability**: 1000 parallel trajectories in < 10 min
- ‚úÖ **Memory Efficiency**: n_dim=20 quantum systems feasible

### Accuracy
- ‚úÖ **Magnus Expansion**: 10x better energy conservation
- ‚úÖ **PMP Solver**: 5x cost reduction vs LQR
- ‚úÖ **ML Policies**: Match analytical solutions on benchmarks

### Robustness
- ‚úÖ **Test Pass Rate**: 95%+ (210+/223 tests)
- ‚úÖ **Edge Cases**: All handled gracefully
- ‚úÖ **Stochastic Tests**: No flakiness

### Usability
- ‚úÖ **Dashboard**: Real-time monitoring at 10 Hz
- ‚úÖ **HPC Integration**: One-command cluster submission
- ‚úÖ **Documentation**: Complete API reference and tutorials

### Production Readiness
- ‚úÖ **CI/CD**: Automated testing and deployment
- ‚úÖ **Monitoring**: Prometheus metrics integration
- ‚úÖ **Fault Tolerance**: Auto-restart failed tasks
- ‚úÖ **Backward Compatibility**: Phase 1-3 features preserved

---

## Documentation & Training

### Developer Documentation
- API reference (Sphinx)
- Architecture diagrams
- Performance benchmarks
- Troubleshooting guide

### User Documentation
- Getting started guide
- Tutorials for each enhancement
- Example notebooks
- FAQ

### Training Materials
- GPU acceleration workshop
- ML optimal control tutorial
- HPC deployment guide
- Dashboard customization guide

---

## Maintenance Plan

### Post-Release
- Monthly dependency updates
- Quarterly performance benchmarks
- Continuous test suite monitoring
- User feedback integration

### Long-term Roadmap
- Phase 5: Quantum chemistry integration
- Phase 6: Experimental data integration (LAMMPS, GROMACS)
- Phase 7: Cloud deployment (AWS, GCP, Azure)

---

**Phase 4 Status**: üöß In Progress
**Next Milestone**: GPU Acceleration Foundation (Week 4)
**Estimated Completion**: 2026-Q2
