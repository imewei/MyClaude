# allow-torch
# Calculus and Backpropagation - Complete Reference

Comprehensive reference for calculus concepts and backpropagation derivations essential to neural network training.

---

## Table of Contents

1. [Differential Calculus Basics](#differential-calculus-basics)
2. [Multivariable Calculus](#multivariable-calculus)
3. [Chain Rule](#chain-rule)
4. [Backpropagation Algorithm](#backpropagation-algorithm)
5. [Automatic Differentiation](#automatic-differentiation)
6. [Common Layer Gradients](#common-layer-gradients)
7. [Numerical Stability](#numerical-stability)

---

## Differential Calculus Basics

### Derivatives

**Definition:**
```
f'(x) = lim_{hâ†’0} [f(x + h) - f(x)] / h
```

**Interpretation:**
- Rate of change
- Slope of tangent line
- Sensitivity of output to input

### Common Derivatives

```
d/dx (xâ¿) = nÂ·xâ¿â»Â¹
d/dx (eË£) = eË£
d/dx (ln x) = 1/x
d/dx (sin x) = cos x
d/dx (cos x) = -sin x
```

### Activation Function Derivatives

#### ReLU
```python
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

# Alternative:
def relu_derivative_alt(x):
    return np.where(x > 0, 1, 0)
```

#### Sigmoid
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# If you already have sigmoid output
def sigmoid_derivative_from_output(output):
    return output * (1 - output)
```

#### Tanh
```python
def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x) ** 2

# From output
def tanh_derivative_from_output(output):
    return 1 - output ** 2
```

#### Leaky ReLU
```python
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def leaky_relu_derivative(x, alpha=0.01):
    return np.where(x > 0, 1, alpha)
```

#### Softmax
```python
def softmax(x):
    # Numerically stable version
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

def softmax_derivative(x):
    """
    Jacobian of softmax: âˆ‚sáµ¢/âˆ‚xâ±¼
    For a single sample:
    J[i,j] = sáµ¢(Î´áµ¢â±¼ - sâ±¼)
    where Î´áµ¢â±¼ is Kronecker delta
    """
    s = softmax(x)
    # Jacobian: s[i] * (I - s[j])
    return np.diag(s) - np.outer(s, s)
```

---

## Multivariable Calculus

### Partial Derivatives

**Definition:**
```
âˆ‚f/âˆ‚xáµ¢ = lim_{hâ†’0} [f(xâ‚,...,xáµ¢+h,...,xâ‚™) - f(xâ‚,...,xáµ¢,...,xâ‚™)] / h
```

Hold all other variables constant, differentiate with respect to one variable.

### Gradient

**Definition:**
```
âˆ‡f(x) = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€
```

**Properties:**
- Points in direction of steepest ascent
- Perpendicular to level curves
- Magnitude = rate of steepest increase

**Example:**
```python
# f(x, y) = xÂ²y + yÂ³
# âˆ‡f = [2xy, xÂ² + 3yÂ²]

def f(x, y):
    return x**2 * y + y**3

def gradient_f(x, y):
    df_dx = 2 * x * y
    df_dy = x**2 + 3 * y**2
    return np.array([df_dx, df_dy])
```

### Jacobian Matrix

**For vector function f: â„â¿ â†’ â„áµ:**
```
J = [âˆ‚fáµ¢/âˆ‚xâ±¼]  (mÃ—n matrix)

J[i,j] = âˆ‚fáµ¢/âˆ‚xâ±¼
```

**Example: Linear Layer**
```python
# f(x) = Wx + b
# Jacobian âˆ‚f/âˆ‚x = W

def linear_jacobian(W):
    return W  # Already the Jacobian!
```

### Hessian Matrix

**For scalar function f: â„â¿ â†’ â„:**
```
H = [âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼]  (nÃ—n symmetric matrix)

H[i,j] = âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼
```

**Properties:**
- Symmetric: H = Háµ€
- Describes curvature of loss landscape
- Eigenvalues determine convexity

**Application:**
```python
# Second-order optimization (Newton's method)
# x_{t+1} = x_t - Hâ»Â¹âˆ‡f

# In practice, approximate Hessian (BFGS, L-BFGS)
```

---

## Chain Rule

### Single Variable

```
If y = f(g(x)), then:
dy/dx = df/dg Â· dg/dx
```

**Example:**
```python
# y = exp(xÂ²)
# Let u = xÂ²
# y = exp(u)

# dy/dx = dy/du Â· du/dx = exp(u) Â· 2x = exp(xÂ²) Â· 2x

def f(x):
    return np.exp(x**2)

def f_derivative(x):
    return np.exp(x**2) * 2 * x
```

### Multivariable Chain Rule

```
If z = f(yâ‚, ..., yâ‚˜) and each yáµ¢ = gáµ¢(xâ‚, ..., xâ‚™):

âˆ‚z/âˆ‚xâ±¼ = Î£áµ¢ (âˆ‚z/âˆ‚yáµ¢ Â· âˆ‚yáµ¢/âˆ‚xâ±¼)
```

**Neural Network Application:**
```
Loss L = f(y)
Output y = g(z)
Pre-activation z = Wx + b

âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚z Â· âˆ‚z/âˆ‚W
```

### Vector Chain Rule

```
If y = f(x) where f: â„â¿ â†’ â„áµ and x = g(t) where g: â„ â†’ â„â¿:

dy/dt = (âˆ‚f/âˆ‚x) Â· (dx/dt)
       = Jacobian(f) Â· (dx/dt)
```

---

## Backpropagation Algorithm

### Forward Pass

**Layer by layer computation:**
```
Input: xâ½â°â¾
Layer 1: zâ½Â¹â¾ = Wâ½Â¹â¾xâ½â°â¾ + bâ½Â¹â¾
         aâ½Â¹â¾ = Ïƒ(zâ½Â¹â¾)
Layer 2: zâ½Â²â¾ = Wâ½Â²â¾aâ½Â¹â¾ + bâ½Â²â¾
         aâ½Â²â¾ = Ïƒ(zâ½Â²â¾)
...
Loss: L = loss(aâ½á´¸â¾, y)
```

### Backward Pass

**Start with loss gradient, propagate backward:**

```
âˆ‚L/âˆ‚aâ½á´¸â¾ = âˆ‚loss/âˆ‚aâ½á´¸â¾

For layer l (from L to 1):
    âˆ‚L/âˆ‚zâ½Ë¡â¾ = âˆ‚L/âˆ‚aâ½Ë¡â¾ âŠ™ Ïƒ'(zâ½Ë¡â¾)          (element-wise)
    âˆ‚L/âˆ‚Wâ½Ë¡â¾ = âˆ‚L/âˆ‚zâ½Ë¡â¾ Â· (aâ½Ë¡â»Â¹â¾)áµ€        (outer product)
    âˆ‚L/âˆ‚bâ½Ë¡â¾ = âˆ‚L/âˆ‚zâ½Ë¡â¾                   (sum over batch)
    âˆ‚L/âˆ‚aâ½Ë¡â»Â¹â¾ = (Wâ½Ë¡â¾)áµ€ Â· âˆ‚L/âˆ‚zâ½Ë¡â¾       (matrix-vector product)
```

### Derivation: Single Layer

**Setup:**
```
Input: x âˆˆ â„â¿
Weight: W âˆˆ â„áµË£â¿
Bias: b âˆˆ â„áµ
Pre-activation: z = Wx + b âˆˆ â„áµ
Activation: a = Ïƒ(z) âˆˆ â„áµ
Loss: L âˆˆ â„
```

**Given:** âˆ‚L/âˆ‚a (gradient from next layer)

**Find:** âˆ‚L/âˆ‚W, âˆ‚L/âˆ‚b, âˆ‚L/âˆ‚x

**Step 1: âˆ‚L/âˆ‚z**
```
Chain rule: âˆ‚L/âˆ‚záµ¢ = Î£â±¼ (âˆ‚L/âˆ‚aâ±¼ Â· âˆ‚aâ±¼/âˆ‚záµ¢)

Since aâ±¼ = Ïƒ(zâ±¼) depends only on zâ±¼:
âˆ‚L/âˆ‚záµ¢ = âˆ‚L/âˆ‚aáµ¢ Â· âˆ‚aáµ¢/âˆ‚záµ¢ = âˆ‚L/âˆ‚aáµ¢ Â· Ïƒ'(záµ¢)

Vectorized: âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚a âŠ™ Ïƒ'(z)  (element-wise product)
```

**Step 2: âˆ‚L/âˆ‚W**
```
záµ¢ = Î£â±¼ Wáµ¢â±¼xâ±¼ + báµ¢

âˆ‚L/âˆ‚Wáµ¢â±¼ = âˆ‚L/âˆ‚záµ¢ Â· âˆ‚záµ¢/âˆ‚Wáµ¢â±¼ = âˆ‚L/âˆ‚záµ¢ Â· xâ±¼

Vectorized: âˆ‚L/âˆ‚W = (âˆ‚L/âˆ‚z) Â· xáµ€  (outer product)
```

**Step 3: âˆ‚L/âˆ‚b**
```
âˆ‚L/âˆ‚báµ¢ = âˆ‚L/âˆ‚záµ¢ Â· âˆ‚záµ¢/âˆ‚báµ¢ = âˆ‚L/âˆ‚záµ¢ Â· 1 = âˆ‚L/âˆ‚záµ¢

Vectorized: âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚z
```

**Step 4: âˆ‚L/âˆ‚x** (for previous layer)
```
záµ¢ = Î£â±¼ Wáµ¢â±¼xâ±¼ + báµ¢

âˆ‚L/âˆ‚xâ±¼ = Î£áµ¢ (âˆ‚L/âˆ‚záµ¢ Â· âˆ‚záµ¢/âˆ‚xâ±¼) = Î£áµ¢ (âˆ‚L/âˆ‚záµ¢ Â· Wáµ¢â±¼)

Vectorized: âˆ‚L/âˆ‚x = Wáµ€(âˆ‚L/âˆ‚z)  (matrix-vector product)
```

### Complete Implementation

```python
class LinearLayer:
    def __init__(self, input_dim, output_dim):
        # He initialization
        self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2.0 / input_dim)
        self.b = np.zeros((output_dim, 1))

    def forward(self, x):
        """Forward pass: z = Wx + b"""
        self.x = x  # Cache for backward pass
        self.z = self.W @ x + self.b
        return self.z

    def backward(self, grad_output):
        """
        Backward pass.

        Args:
            grad_output: âˆ‚L/âˆ‚z (gradient from next layer)

        Returns:
            grad_input: âˆ‚L/âˆ‚x (gradient to pass to previous layer)
        """
        batch_size = self.x.shape[1]

        # Gradient w.r.t weights: âˆ‚L/âˆ‚W = (âˆ‚L/âˆ‚z) @ xáµ€
        self.grad_W = grad_output @ self.x.T / batch_size

        # Gradient w.r.t bias: âˆ‚L/âˆ‚b = mean(âˆ‚L/âˆ‚z) over batch
        self.grad_b = np.mean(grad_output, axis=1, keepdims=True)

        # Gradient w.r.t input: âˆ‚L/âˆ‚x = Wáµ€ @ (âˆ‚L/âˆ‚z)
        grad_input = self.W.T @ grad_output

        return grad_input


class ReLU:
    def forward(self, x):
        """Forward pass: a = max(0, x)"""
        self.x = x  # Cache for backward pass
        return np.maximum(0, x)

    def backward(self, grad_output):
        """
        Backward pass.

        Args:
            grad_output: âˆ‚L/âˆ‚a

        Returns:
            grad_input: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚a âŠ™ (x > 0)
        """
        return grad_output * (self.x > 0)


class MSELoss:
    def forward(self, predictions, targets):
        """Forward pass: L = 1/2 ||pred - target||Â²"""
        self.diff = predictions - targets
        return 0.5 * np.mean(self.diff ** 2)

    def backward(self):
        """
        Backward pass.

        Returns:
            âˆ‚L/âˆ‚pred = (pred - target) / batch_size
        """
        batch_size = self.diff.shape[1]
        return self.diff / batch_size


# Full network
class TwoLayerNetwork:
    def __init__(self, input_dim, hidden_dim, output_dim):
        self.layer1 = LinearLayer(input_dim, hidden_dim)
        self.relu = ReLU()
        self.layer2 = LinearLayer(hidden_dim, output_dim)
        self.loss_fn = MSELoss()

    def forward(self, x, targets=None):
        """Forward pass through network."""
        # Layer 1
        z1 = self.layer1.forward(x)
        a1 = self.relu.forward(z1)

        # Layer 2
        z2 = self.layer2.forward(a1)

        if targets is not None:
            loss = self.loss_fn.forward(z2, targets)
            return z2, loss
        return z2

    def backward(self):
        """Backward pass through network."""
        # Loss gradient
        grad_z2 = self.loss_fn.backward()

        # Layer 2 backward
        grad_a1 = self.layer2.backward(grad_z2)

        # ReLU backward
        grad_z1 = self.relu.backward(grad_a1)

        # Layer 1 backward
        grad_x = self.layer1.backward(grad_z1)

        return grad_x

    def get_gradients(self):
        """Return all parameter gradients."""
        return {
            'W1': self.layer1.grad_W,
            'b1': self.layer1.grad_b,
            'W2': self.layer2.grad_W,
            'b2': self.layer2.grad_b
        }


# Usage
network = TwoLayerNetwork(input_dim=784, hidden_dim=128, output_dim=10)

# Forward pass
x = np.random.randn(784, 32)  # Batch of 32
targets = np.random.randn(10, 32)
predictions, loss = network.forward(x, targets)

# Backward pass
network.backward()

# Get gradients
gradients = network.get_gradients()
```

---

## Automatic Differentiation

### Forward Mode vs Reverse Mode

**Forward Mode (builds graph from inputs):**
- Efficient for functions with few inputs, many outputs
- Computes one directional derivative at a time
- âˆ‚y/âˆ‚xáµ¢ for each input xáµ¢

**Reverse Mode (backpropagation):**
- Efficient for functions with many inputs, few outputs (like neural networks!)
- Computes âˆ‚L/âˆ‚xáµ¢ for all inputs in one pass
- This is what PyTorch/TensorFlow use

### Computational Graph

**Example: y = (xâ‚ + xâ‚‚) Â· xâ‚ƒ**

```
Forward pass (build graph):
    vâ‚ = xâ‚ + xâ‚‚
    vâ‚‚ = vâ‚ Â· xâ‚ƒ
    y = vâ‚‚

Backward pass (compute gradients):
    âˆ‚y/âˆ‚vâ‚‚ = 1
    âˆ‚y/âˆ‚vâ‚ = âˆ‚y/âˆ‚vâ‚‚ Â· âˆ‚vâ‚‚/âˆ‚vâ‚ = 1 Â· xâ‚ƒ = xâ‚ƒ
    âˆ‚y/âˆ‚xâ‚ƒ = âˆ‚y/âˆ‚vâ‚‚ Â· âˆ‚vâ‚‚/âˆ‚xâ‚ƒ = 1 Â· vâ‚ = (xâ‚ + xâ‚‚)
    âˆ‚y/âˆ‚xâ‚‚ = âˆ‚y/âˆ‚vâ‚ Â· âˆ‚vâ‚/âˆ‚xâ‚‚ = xâ‚ƒ Â· 1 = xâ‚ƒ
    âˆ‚y/âˆ‚xâ‚ = âˆ‚y/âˆ‚vâ‚ Â· âˆ‚vâ‚/âˆ‚xâ‚ = xâ‚ƒ Â· 1 = xâ‚ƒ
```

### PyTorch Autograd

```python
import torch

# Enable gradient tracking
x = torch.randn(3, requires_grad=True)
W = torch.randn(5, 3, requires_grad=True)
b = torch.randn(5, requires_grad=True)

# Forward pass
z = W @ x + b
a = torch.relu(z)
loss = (a ** 2).sum()

# Backward pass (automatic!)
loss.backward()

# Gradients computed
print(x.grad)  # âˆ‚loss/âˆ‚x
print(W.grad)  # âˆ‚loss/âˆ‚W
print(b.grad)  # âˆ‚loss/âˆ‚b
```

---

## Common Layer Gradients

### Convolutional Layer

**Forward:**
```
output[b, c_out, h_out, w_out] = Î£_{c_in,kh,kw}
    input[b, c_in, h_out+kh, w_out+kw] Â· kernel[c_out, c_in, kh, kw]
```

**Backward:**
```python
def conv2d_backward(grad_output, input, kernel):
    """
    grad_output: âˆ‚L/âˆ‚output
    input: cached from forward pass
    kernel: weight tensor

    Returns:
        grad_input: âˆ‚L/âˆ‚input
        grad_kernel: âˆ‚L/âˆ‚kernel
    """
    # Gradient w.r.t input: convolve grad_output with flipped kernel
    grad_input = F.conv_transpose2d(grad_output, kernel)

    # Gradient w.r.t kernel: convolve input with grad_output
    grad_kernel = F.conv2d(input.transpose(0, 1), grad_output.transpose(0, 1))

    return grad_input, grad_kernel
```

### Batch Normalization

**Forward:**
```
Î¼ = mean(x)
ÏƒÂ² = var(x)
x_hat = (x - Î¼) / âˆš(ÏƒÂ² + Îµ)
y = Î³Â·x_hat + Î²
```

**Backward:**
```python
def batchnorm_backward(grad_output, x, mean, var, gamma, eps=1e-5):
    """
    Backward pass for batch normalization.

    grad_output: âˆ‚L/âˆ‚y
    """
    N = x.shape[0]

    # Normalized input
    x_centered = x - mean
    std = np.sqrt(var + eps)
    x_hat = x_centered / std

    # Gradients
    grad_gamma = np.sum(grad_output * x_hat, axis=0)
    grad_beta = np.sum(grad_output, axis=0)

    # Gradient w.r.t x_hat
    grad_x_hat = grad_output * gamma

    # Gradient w.r.t variance
    grad_var = np.sum(grad_x_hat * x_centered * -0.5 * (var + eps)**(-1.5), axis=0)

    # Gradient w.r.t mean
    grad_mean = np.sum(grad_x_hat * -1.0 / std, axis=0) + \
                grad_var * np.mean(-2.0 * x_centered, axis=0)

    # Gradient w.r.t input
    grad_x = grad_x_hat / std + \
             grad_var * 2.0 * x_centered / N + \
             grad_mean / N

    return grad_x, grad_gamma, grad_beta
```

### Dropout

**Forward:**
```python
def dropout_forward(x, dropout_rate=0.5, training=True):
    if not training:
        return x

    # Binary mask
    mask = np.random.binomial(1, 1 - dropout_rate, size=x.shape)

    # Scale to maintain expected value
    return x * mask / (1 - dropout_rate), mask
```

**Backward:**
```python
def dropout_backward(grad_output, mask, dropout_rate=0.5):
    """
    Gradient is masked and scaled by same factor.
    """
    return grad_output * mask / (1 - dropout_rate)
```

### Attention (Simplified)

**Forward:**
```python
def attention_forward(Q, K, V):
    """
    Q: queries (batch, seq_len_q, d_k)
    K: keys (batch, seq_len_k, d_k)
    V: values (batch, seq_len_v, d_v)
    """
    # Scores
    scores = Q @ K.transpose(-2, -1) / np.sqrt(K.shape[-1])

    # Attention weights
    weights = softmax(scores, axis=-1)

    # Output
    output = weights @ V

    return output, weights
```

**Backward (conceptual):**
```python
def attention_backward(grad_output, Q, K, V, weights):
    """
    grad_output: âˆ‚L/âˆ‚output

    Returns: grad_Q, grad_K, grad_V
    """
    d_k = K.shape[-1]

    # âˆ‚L/âˆ‚V = weightsáµ€ @ grad_output
    grad_V = weights.transpose(-2, -1) @ grad_output

    # âˆ‚L/âˆ‚weights = grad_output @ Váµ€
    grad_weights = grad_output @ V.transpose(-2, -1)

    # âˆ‚L/âˆ‚scores (softmax backward)
    grad_scores = softmax_backward(grad_weights, weights)

    # âˆ‚L/âˆ‚Q = grad_scores @ K / âˆšd_k
    grad_Q = grad_scores @ K / np.sqrt(d_k)

    # âˆ‚L/âˆ‚K = grad_scoresáµ€ @ Q / âˆšd_k
    grad_K = grad_scores.transpose(-2, -1) @ Q / np.sqrt(d_k)

    return grad_Q, grad_K, grad_V
```

---

## Numerical Stability

### Gradient Checking

```python
def gradient_check(f, x, analytical_grad, eps=1e-7):
    """
    Verify analytical gradient using finite differences.

    f: function to compute
    x: input point
    analytical_grad: computed gradient
    eps: finite difference step size
    """
    numerical_grad = np.zeros_like(x)

    # Compute numerical gradient
    for i in range(x.size):
        x_plus = x.copy()
        x_plus.flat[i] += eps

        x_minus = x.copy()
        x_minus.flat[i] -= eps

        numerical_grad.flat[i] = (f(x_plus) - f(x_minus)) / (2 * eps)

    # Compare
    diff = np.linalg.norm(numerical_grad - analytical_grad)
    norm = np.linalg.norm(numerical_grad) + np.linalg.norm(analytical_grad)
    relative_error = diff / (norm + 1e-8)

    return relative_error, numerical_grad


# Example usage
def test_layer_gradient():
    layer = LinearLayer(10, 5)
    x = np.random.randn(10, 1)

    # Forward
    z = layer.forward(x)
    loss = np.sum(z ** 2)

    # Backward
    grad_z = 2 * z
    layer.backward(grad_z)

    # Gradient check for W
    def f_W(W):
        layer.W = W.reshape(layer.W.shape)
        z = layer.forward(x)
        return np.sum(z ** 2)

    error, _ = gradient_check(f_W, layer.W.flatten(), layer.grad_W.flatten())
    print(f"Gradient check error: {error:.2e}")
    assert error < 1e-5, "Gradient check failed!"
```

### Numerical Stability Tips

1. **Softmax:**
```python
# Bad: overflow risk
def softmax_unstable(x):
    return np.exp(x) / np.sum(np.exp(x))

# Good: subtract max for numerical stability
def softmax_stable(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
```

2. **Log-Sum-Exp:**
```python
# Compute log(Î£ exp(xáµ¢)) stably
def logsumexp(x, axis=-1):
    max_x = np.max(x, axis=axis, keepdims=True)
    return max_x + np.log(np.sum(np.exp(x - max_x), axis=axis, keepdims=True))
```

3. **Cross-Entropy:**
```python
# Bad: compute softmax then log (numerical issues)
def cross_entropy_unstable(logits, targets):
    probs = softmax(logits)
    return -np.sum(targets * np.log(probs + 1e-10))

# Good: log-softmax in one stable operation
def cross_entropy_stable(logits, targets):
    log_probs = logits - logsumexp(logits, axis=-1, keepdims=True)
    return -np.sum(targets * log_probs)
```

---

## Quick Reference

### Derivative Rules
| Function | Derivative |
|----------|------------|
| xâ¿ | nÂ·xâ¿â»Â¹ |
| eË£ | eË£ |
| ln(x) | 1/x |
| sigmoid(x) | Ïƒ(x)Â·(1 - Ïƒ(x)) |
| tanh(x) | 1 - tanhÂ²(x) |
| ReLU(x) | ğŸ™(x > 0) |

### Chain Rule
| Context | Formula |
|---------|---------|
| Composition | (fâˆ˜g)'(x) = f'(g(x))Â·g'(x) |
| Neural layer | âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚W |
| Backprop | âˆ‚L/âˆ‚xâ‚— = âˆ‚L/âˆ‚xâ‚—â‚Šâ‚ Â· âˆ‚xâ‚—â‚Šâ‚/âˆ‚xâ‚— |

### Gradient Formulas
| Layer | Gradient |
|-------|----------|
| Linear: z = Wx + b | âˆ‚L/âˆ‚W = (âˆ‚L/âˆ‚z)Â·xáµ€ |
| | âˆ‚L/âˆ‚x = Wáµ€Â·(âˆ‚L/âˆ‚z) |
| Activation: a = Ïƒ(z) | âˆ‚L/âˆ‚z = (âˆ‚L/âˆ‚a) âŠ™ Ïƒ'(z) |
| Loss: L = Â½||y - Å·||Â² | âˆ‚L/âˆ‚Å· = Å· - y |

---

## References

1. "Deep Learning" - Goodfellow, Bengio, Courville (Chapter 6)
2. "Neural Networks and Deep Learning" - Michael Nielsen
3. "Automatic Differentiation in Machine Learning: a Survey" - Baydin et al., 2018
4. "Calculus on Computational Graphs: Backpropagation" - Chris Olah
5. "Yes you should understand backprop" - Andrej Karpathy
